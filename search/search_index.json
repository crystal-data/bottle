{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction # Num.cr is the core shard needed for scientific computing with Crystal Website: https://crystal-data.github.io/num.cr API Documentation: https://crystal-data.github.io/num.cr/ Source code: https://github.com/crystal-data/num.cr Bug reports: https://github.com/crystal-data/num.cr/issues It provides: An n-dimensional Tensor data structure Efficient map , reduce and accumulate routines GPU accelerated routines backed by OpenCL Linear algebra routines backed by LAPACK and BLAS Prerequisites # Num.cr aims to be a scientific computing library written in pure Crystal. All standard operations and data structures are written in Crystal. Certain routines, primarily linear algebra routines, are instead provided by a BLAS or LAPACK implementation. Several implementations can be used, including Cblas , Openblas , and the Accelerate framework on Darwin systems. For GPU accelerated BLAS routines, the ClBlast library is required. Num.cr also supports Tensor s stored on a GPU . This is currently limited to OpenCL , and a valid OpenCL installation and device(s) are required. Installation # Add this to your applications shard.yml dependencies: num: github: crystal-data/num.cr Several third-party libraries are required to use certain features of Num.cr . They are: BLAS LAPACK OpenCL ClBlast NNPACK While not at all required, they provide additional functionality than is provided by the basic library. Just show me the code # The core data structure implemented by Num.cr is the Tensor , an N-dimensional data structure. A Tensor supports slicing, mutation, permutation, reduction, and accumulation. A Tensor can be a view of another Tensor , and can support either C-style or Fortran-style storage. Creation # There are many ways to initialize a Tensor . Most creation methods can allocate a Tensor backed by either CPU or GPU based storage. [ 1 , 2 , 3 ]. to_tensor Tensor . from_array [ 1 , 2 , 3 ] Tensor ( UInt8 , CPU ( UInt8 )) . zeros ( [ 3 , 3 , 2 ] ) Tensor . random ( 0.0 ... 1.0 , [ 2 , 2 , 2 ] ) Tensor ( Float32 , OCL ( Float32 )) . zeros ( [ 3 , 2 , 2 ] ) Tensor ( Float64 , OCL ( Float64 )) . full ( [ 3 , 4 , 5 ] , 3.8 ) Operations # A Tensor supports a wide variety of numerical operations. Many of these operations are provided by Num.cr , but any operation can be mapped across one or more Tensor s using sophisticated broadcasted mapping routines. a = [ 1 , 2 , 3 , 4 ]. to_tensor b = [[ 3 , 4 , 5 , 6 ] , [ 5 , 6 , 7 , 8 ]]. to_tensor puts a + b # a is broadcast to b's shape # [[ 4, 6, 8, 10], # [ 6, 8, 10, 12]] When operating on more than two Tensor s, it is recommended to use map rather than builtin functions to avoid the allocation of intermediate results. All map operations support broadcasting. a = [ 1 , 2 , 3 , 4 ]. to_tensor b = [[ 3 , 4 , 5 , 6 ] , [ 5 , 6 , 7 , 8 ]]. to_tensor c = [ 3 , 5 , 7 , 9 ]. to_tensor a . map ( b , c ) do | i , j , k | i + 2 / j + k * 3.5 end # [[12.1667, 20 , 27.9 , 35.8333], # [11.9 , 19.8333, 27.7857, 35.75 ]] Mutation # Tensor s support flexible slicing and mutation operations. Many of these operations return views, not copies, so any changes made to the results might also be reflected in the parent. a = Tensor . new ( [ 3 , 2 , 2 ] ) { | i | i } puts a . transpose # [[[ 0, 4, 8], # [ 2, 6, 10]], # # [[ 1, 5, 9], # [ 3, 7, 11]]] puts a . reshape ( 6 , 2 ) # [[ 0, 1], # [ 2, 3], # [ 4, 5], # [ 6, 7], # [ 8, 9], # [10, 11]] puts a [... , 1 ] # [[ 2, 3], # [ 6, 7], # [10, 11]] puts a [ 1 ... , { ... , - 1 } ] # [[[ 6, 7], # [ 4, 5]], # # [[10, 11], # [ 8, 9]]] puts a [ 0 , 1 , 1 ]. value # 3 Linear Algebra # Tensor s provide easy access to power Linear Algebra routines backed by LAPACK and BLAS implementations, and ClBlast for GPU backed Tensor s. a = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor . map &. to_f32 puts a . inv # [[-2 , 1 ], # [1.5 , -0.5]] puts a . eigvals # [-0.372281, 5.37228 ] puts a . matmul ( a ) # [[7 , 10], # [15, 22]] Einstein Notation # For representing certain complex contractions of Tensor s, Einstein notation can be used to simplify the operation. For example, the following matrix multiplication + summation operation: a = Tensor . new ( [ 30 , 40 , 50 ] ) { | i | i * 1_f32 } b = Tensor . new ( [ 40 , 30 , 20 ] ) { | i | i * 1_f32 } result = Float32Tensor . zeros ( [ 50 , 20 ] ) ny , nx = result . shape b2 = b . swap_axes ( 0 , 1 ) ny . times do | k | nx . times do | l | result [ k , l ] = ( a [... , ... , k ] * b2 [... , ... , l ] ) . sum end end Can instead be represented in Einstein notiation as the following: Num :: Einsum . einsum ( \"ijk,jil->kl\" , a , b ) This can lead to performance improvements due to optimized contractions on Tensor s. einsum 2.22k (450.41\u00b5s) (\u00b1 0.86%) 350kB/op fastest manual 117.52 ( 8.51ms) (\u00b1 0.98%) 5.66MB/op 18.89\u00d7 slower Machine Learning # Num::Grad provides a pure-crystal approach to find derivatives of mathematical functions. Use a Num::Grad::Variable with a Num::Grad::Context to easily compute these derivatives. ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 3.0 ]. to_tensor ) y = ctx . variable ( [ 2.0 ]. to_tensor ) # f(x) = x ** y f = x ** y puts f # => [9] f . backprop # df/dx = y * x = 6.0 puts x . grad # => [6.0] Num::NN contains an extension to Num::Grad that provides an easy-to-use interface to assist in creating neural networks. Designing and creating a network is simple using Crystal's block syntax. ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x_train = [[ 0.0 , 0.0 ] , [ 1.0 , 0.0 ] , [ 0.0 , 1.0 ] , [ 1.0 , 1.0 ]]. to_tensor y_train = [[ 0.0 ] , [ 1.0 ] , [ 1.0 ] , [ 0.0 ]]. to_tensor x = ctx . variable ( x_train ) net = Num :: NN :: Network . new ( ctx ) do input [ 2 ] # A basic network with a single hidden layer using # a ReLU activation function linear 3 relu linear 1 # SGD Optimizer sgd 0.7 # Sigmoid Cross Entropy to calculate loss sigmoid_cross_entropy_loss end 500 . times do | epoch | y_pred = net . forward ( x ) loss = net . loss ( y_pred , y_train ) puts \"Epoch: #{ epoch } - Loss #{ loss } \" loss . backprop net . optimizer . update end # Clip results to make a prediction puts net . forward ( x ) . value . map { | el | el > 0 ? 1 : 0 } # [[0], # [1], # [1], # [0]] Review the documentation for full implementation details, and if something is missing, open an issue to add it! Numpy Comparison # This contains a table of manipulation + creation routines. All operators, trigonometric, binary, equality, and mathematical functions have been implemented as well. These follow the naming convention of the Crystal Math module. They have also all been implemented for both CPU and OpenCL backends. Method Description CPU OpenCL empty In Crystal, same as Tensor.zeros empty_like In Crystal, same as Tensor.zeros_like eye Tensor.eye Yes Yes identity Tensor.identity Yes Yes ones Tensor.ones Yes Yes ones_like Tensor.ones_like Yes Yes zeros Tensor.zeros Yes Yes zeros_like Tensor.zeros_like Yes Yes full Tensor.full Yes Yes full_like Tensor.full_like Yes Yes array .to_tensor Yes Yes from_npy Tensor.from_npy Yes Yes arange Tensor.range Yes Yes linspace Tensor.linear_space Yes Yes geomspace Tensor.geometric_space Yes Yes logspace Tensor.logarithmic_space Yes Yes meshgrid mgrid ogrid diag diagflat tri tril Tensor.tril Yes No triu Tensor.triu Yes No vander Tensor.vandermonde Yes No reshape Tensor.reshape Yes Yes ravel Tensor.flat Yes Yes ndarray.flat Tensor.flat.each Yes No moveaxis Tensor.move_axis Yes No rollaxis swapaxes Tensor.swap_axes Yes No transpose Tensor.transpose Yes Only full transpose broadcast broadcast_to Tensor.broadcast_to Yes Yes broadcast_arrays Tensor.broadcast , only 2 or 3 Yes Yes expand_dims Tensor.expand_dims Yes No squeeze concatenate Num.concatenate Yes Yes stack block vstack Num.vstack Yes Yes hstack Num.hstack Yes Yes column_stack row_stack split array_split dsplit hsplit vsplit tile Num.tile Yes No repeat Num.repeat Yes Yes unique flip Tensor.flip Yes Yes","title":"Introduction"},{"location":"#introduction","text":"Num.cr is the core shard needed for scientific computing with Crystal Website: https://crystal-data.github.io/num.cr API Documentation: https://crystal-data.github.io/num.cr/ Source code: https://github.com/crystal-data/num.cr Bug reports: https://github.com/crystal-data/num.cr/issues It provides: An n-dimensional Tensor data structure Efficient map , reduce and accumulate routines GPU accelerated routines backed by OpenCL Linear algebra routines backed by LAPACK and BLAS","title":"Introduction"},{"location":"#prerequisites","text":"Num.cr aims to be a scientific computing library written in pure Crystal. All standard operations and data structures are written in Crystal. Certain routines, primarily linear algebra routines, are instead provided by a BLAS or LAPACK implementation. Several implementations can be used, including Cblas , Openblas , and the Accelerate framework on Darwin systems. For GPU accelerated BLAS routines, the ClBlast library is required. Num.cr also supports Tensor s stored on a GPU . This is currently limited to OpenCL , and a valid OpenCL installation and device(s) are required.","title":"Prerequisites"},{"location":"#installation","text":"Add this to your applications shard.yml dependencies: num: github: crystal-data/num.cr Several third-party libraries are required to use certain features of Num.cr . They are: BLAS LAPACK OpenCL ClBlast NNPACK While not at all required, they provide additional functionality than is provided by the basic library.","title":"Installation"},{"location":"#just-show-me-the-code","text":"The core data structure implemented by Num.cr is the Tensor , an N-dimensional data structure. A Tensor supports slicing, mutation, permutation, reduction, and accumulation. A Tensor can be a view of another Tensor , and can support either C-style or Fortran-style storage.","title":"Just show me the code"},{"location":"#creation","text":"There are many ways to initialize a Tensor . Most creation methods can allocate a Tensor backed by either CPU or GPU based storage. [ 1 , 2 , 3 ]. to_tensor Tensor . from_array [ 1 , 2 , 3 ] Tensor ( UInt8 , CPU ( UInt8 )) . zeros ( [ 3 , 3 , 2 ] ) Tensor . random ( 0.0 ... 1.0 , [ 2 , 2 , 2 ] ) Tensor ( Float32 , OCL ( Float32 )) . zeros ( [ 3 , 2 , 2 ] ) Tensor ( Float64 , OCL ( Float64 )) . full ( [ 3 , 4 , 5 ] , 3.8 )","title":"Creation"},{"location":"#operations","text":"A Tensor supports a wide variety of numerical operations. Many of these operations are provided by Num.cr , but any operation can be mapped across one or more Tensor s using sophisticated broadcasted mapping routines. a = [ 1 , 2 , 3 , 4 ]. to_tensor b = [[ 3 , 4 , 5 , 6 ] , [ 5 , 6 , 7 , 8 ]]. to_tensor puts a + b # a is broadcast to b's shape # [[ 4, 6, 8, 10], # [ 6, 8, 10, 12]] When operating on more than two Tensor s, it is recommended to use map rather than builtin functions to avoid the allocation of intermediate results. All map operations support broadcasting. a = [ 1 , 2 , 3 , 4 ]. to_tensor b = [[ 3 , 4 , 5 , 6 ] , [ 5 , 6 , 7 , 8 ]]. to_tensor c = [ 3 , 5 , 7 , 9 ]. to_tensor a . map ( b , c ) do | i , j , k | i + 2 / j + k * 3.5 end # [[12.1667, 20 , 27.9 , 35.8333], # [11.9 , 19.8333, 27.7857, 35.75 ]]","title":"Operations"},{"location":"#mutation","text":"Tensor s support flexible slicing and mutation operations. Many of these operations return views, not copies, so any changes made to the results might also be reflected in the parent. a = Tensor . new ( [ 3 , 2 , 2 ] ) { | i | i } puts a . transpose # [[[ 0, 4, 8], # [ 2, 6, 10]], # # [[ 1, 5, 9], # [ 3, 7, 11]]] puts a . reshape ( 6 , 2 ) # [[ 0, 1], # [ 2, 3], # [ 4, 5], # [ 6, 7], # [ 8, 9], # [10, 11]] puts a [... , 1 ] # [[ 2, 3], # [ 6, 7], # [10, 11]] puts a [ 1 ... , { ... , - 1 } ] # [[[ 6, 7], # [ 4, 5]], # # [[10, 11], # [ 8, 9]]] puts a [ 0 , 1 , 1 ]. value # 3","title":"Mutation"},{"location":"#linear-algebra","text":"Tensor s provide easy access to power Linear Algebra routines backed by LAPACK and BLAS implementations, and ClBlast for GPU backed Tensor s. a = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor . map &. to_f32 puts a . inv # [[-2 , 1 ], # [1.5 , -0.5]] puts a . eigvals # [-0.372281, 5.37228 ] puts a . matmul ( a ) # [[7 , 10], # [15, 22]]","title":"Linear Algebra"},{"location":"#einstein-notation","text":"For representing certain complex contractions of Tensor s, Einstein notation can be used to simplify the operation. For example, the following matrix multiplication + summation operation: a = Tensor . new ( [ 30 , 40 , 50 ] ) { | i | i * 1_f32 } b = Tensor . new ( [ 40 , 30 , 20 ] ) { | i | i * 1_f32 } result = Float32Tensor . zeros ( [ 50 , 20 ] ) ny , nx = result . shape b2 = b . swap_axes ( 0 , 1 ) ny . times do | k | nx . times do | l | result [ k , l ] = ( a [... , ... , k ] * b2 [... , ... , l ] ) . sum end end Can instead be represented in Einstein notiation as the following: Num :: Einsum . einsum ( \"ijk,jil->kl\" , a , b ) This can lead to performance improvements due to optimized contractions on Tensor s. einsum 2.22k (450.41\u00b5s) (\u00b1 0.86%) 350kB/op fastest manual 117.52 ( 8.51ms) (\u00b1 0.98%) 5.66MB/op 18.89\u00d7 slower","title":"Einstein Notation"},{"location":"#machine-learning","text":"Num::Grad provides a pure-crystal approach to find derivatives of mathematical functions. Use a Num::Grad::Variable with a Num::Grad::Context to easily compute these derivatives. ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 3.0 ]. to_tensor ) y = ctx . variable ( [ 2.0 ]. to_tensor ) # f(x) = x ** y f = x ** y puts f # => [9] f . backprop # df/dx = y * x = 6.0 puts x . grad # => [6.0] Num::NN contains an extension to Num::Grad that provides an easy-to-use interface to assist in creating neural networks. Designing and creating a network is simple using Crystal's block syntax. ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x_train = [[ 0.0 , 0.0 ] , [ 1.0 , 0.0 ] , [ 0.0 , 1.0 ] , [ 1.0 , 1.0 ]]. to_tensor y_train = [[ 0.0 ] , [ 1.0 ] , [ 1.0 ] , [ 0.0 ]]. to_tensor x = ctx . variable ( x_train ) net = Num :: NN :: Network . new ( ctx ) do input [ 2 ] # A basic network with a single hidden layer using # a ReLU activation function linear 3 relu linear 1 # SGD Optimizer sgd 0.7 # Sigmoid Cross Entropy to calculate loss sigmoid_cross_entropy_loss end 500 . times do | epoch | y_pred = net . forward ( x ) loss = net . loss ( y_pred , y_train ) puts \"Epoch: #{ epoch } - Loss #{ loss } \" loss . backprop net . optimizer . update end # Clip results to make a prediction puts net . forward ( x ) . value . map { | el | el > 0 ? 1 : 0 } # [[0], # [1], # [1], # [0]] Review the documentation for full implementation details, and if something is missing, open an issue to add it!","title":"Machine Learning"},{"location":"#numpy-comparison","text":"This contains a table of manipulation + creation routines. All operators, trigonometric, binary, equality, and mathematical functions have been implemented as well. These follow the naming convention of the Crystal Math module. They have also all been implemented for both CPU and OpenCL backends. Method Description CPU OpenCL empty In Crystal, same as Tensor.zeros empty_like In Crystal, same as Tensor.zeros_like eye Tensor.eye Yes Yes identity Tensor.identity Yes Yes ones Tensor.ones Yes Yes ones_like Tensor.ones_like Yes Yes zeros Tensor.zeros Yes Yes zeros_like Tensor.zeros_like Yes Yes full Tensor.full Yes Yes full_like Tensor.full_like Yes Yes array .to_tensor Yes Yes from_npy Tensor.from_npy Yes Yes arange Tensor.range Yes Yes linspace Tensor.linear_space Yes Yes geomspace Tensor.geometric_space Yes Yes logspace Tensor.logarithmic_space Yes Yes meshgrid mgrid ogrid diag diagflat tri tril Tensor.tril Yes No triu Tensor.triu Yes No vander Tensor.vandermonde Yes No reshape Tensor.reshape Yes Yes ravel Tensor.flat Yes Yes ndarray.flat Tensor.flat.each Yes No moveaxis Tensor.move_axis Yes No rollaxis swapaxes Tensor.swap_axes Yes No transpose Tensor.transpose Yes Only full transpose broadcast broadcast_to Tensor.broadcast_to Yes Yes broadcast_arrays Tensor.broadcast , only 2 or 3 Yes Yes expand_dims Tensor.expand_dims Yes No squeeze concatenate Num.concatenate Yes Yes stack block vstack Num.vstack Yes Yes hstack Num.hstack Yes Yes column_stack row_stack split array_split dsplit hsplit vsplit tile Num.tile Yes No repeat Num.repeat Yes Yes unique flip Tensor.flip Yes Yes","title":"Numpy Comparison"},{"location":"Array/","text":"class Array(T) inherits Reference # An Array is an ordered, integer-indexed collection of objects of type T. Array indexing starts at 0. A negative index is assumed to be relative to the end of the array: -1 indicates the last element, -2 is the next to last element, and so on. An Array can be created using the usual new method (several are provided), or with an array literal: Array ( Int32 ) . new # => [] [ 1 , 2 , 3 ] # Array(Int32) [ 1 , \"hello\" , 'x' ] # Array(Int32 | String | Char) See Array literals in the language reference. An Array can have mixed types, meaning T will be a union of types, but these are determined when the array is created, either by specifying T or by using an array literal. In the latter case, T will be set to the union of the array literal elements' types. When creating an empty array you must always specify T: [] of Int32 # same as Array(Int32) [] # syntax error An Array is implemented using an internal buffer of some capacity and is reallocated when elements are pushed to it when more capacity is needed. This is normally known as a dynamic array . You can use a special array literal syntax with other types too, as long as they define an argless new method and a << method. Set is one such type: set = Set { 1 , 2 , 3 } # => Set{1, 2, 3} set . class # => Set(Int32) The above is the same as this: set = Set ( typeof ( 1 , 2 , 3 )) . new set << 1 set << 2 set << 3 Included modules Comparable Indexable::Mutable Methods # #to_tensor ( device = CPU ) # Converts a standard library array of any shape to a Tensor . The array, if nested, must have a uniform shape in all dimensions, and each innermost element must have the same data type. Arguments # device : Num::Storage - The storage backend on which to place the Tensor Examples # a = [ 1 , 2 , 3 ] typeof ( a . to_tensor ) # => Tensor(Int32, CPU(Float32)) View source","title":"Array"},{"location":"Array/#Array","text":"An Array is an ordered, integer-indexed collection of objects of type T. Array indexing starts at 0. A negative index is assumed to be relative to the end of the array: -1 indicates the last element, -2 is the next to last element, and so on. An Array can be created using the usual new method (several are provided), or with an array literal: Array ( Int32 ) . new # => [] [ 1 , 2 , 3 ] # Array(Int32) [ 1 , \"hello\" , 'x' ] # Array(Int32 | String | Char) See Array literals in the language reference. An Array can have mixed types, meaning T will be a union of types, but these are determined when the array is created, either by specifying T or by using an array literal. In the latter case, T will be set to the union of the array literal elements' types. When creating an empty array you must always specify T: [] of Int32 # same as Array(Int32) [] # syntax error An Array is implemented using an internal buffer of some capacity and is reallocated when elements are pushed to it when more capacity is needed. This is normally known as a dynamic array . You can use a special array literal syntax with other types too, as long as they define an argless new method and a << method. Set is one such type: set = Set { 1 , 2 , 3 } # => Set{1, 2, 3} set . class # => Set(Int32) The above is the same as this: set = Set ( typeof ( 1 , 2 , 3 )) . new set << 1 set << 2 set << 3","title":"Array"},{"location":"Array/#Array-methods","text":"","title":"Methods"},{"location":"Array/#Array#to_tensor(device)","text":"Converts a standard library array of any shape to a Tensor . The array, if nested, must have a uniform shape in all dimensions, and each innermost element must have the same data type.","title":"#to_tensor"},{"location":"CPU/","text":"class CPU(T) inherits Num::Backend::Storage # Constructors # .new ( shape : Array ( Int ), order : Num::OrderType , value : T ) # Initialize a CPU storage from an initial capacity and an initial value, which will fill the buffer Arguments # shape : Array(Int) - Shape of the parent Tensor order : Array(Int) - Memory layout of the parent Tensor value : T - Initial value to populate the buffer Examples # CPU . new ( [ 10 , 10 ] , 3.4 ) View source .new ( shape : Array ( Int ), strides : Array ( Int ), value : T ) # Initialize a CPU storage from an initial capacity and an initial value, which will fill the buffer Arguments # shape : Array(Int) - Shape of the parent Tensor strides : Array(Int) - Strides of the parent Tensor value : T - Initial value to populate the buffer Examples # CPU . new ( [ 10 , 10 ] , 3.4 ) View source .new ( data : Pointer ( T ), shape : Array ( Int ), strides : Array ( Int )) # Initialize a CPU storage from a hostptr and initial shape. The shape is not required for this storage type, but is needed by other implementations to ensure copy requirements have the right pointer size. Arguments # data : Pointer(T) - Existing databuffer for a Tensor shape : Array(Int) - Shape of the parent Tensor strides : Array(Int) - Strides of the parent Tensor Examples # a = Pointer ( Int32 ) . malloc ( 10 ) s = CPU . new ( a , [ 5 , 2 ] ) View source .new ( shape : Array ( Int ), order : Num::OrderType ) # Initialize a CPU storage from an initial capacity. The data will be filled with zeros Arguments # shape : Array(Int) - Shape of the parent Tensor order : Array(Int) - Memory layout of the parent Tensor Examples # CPU . new ( [ 2 , 3 , 4 ] ) View source .new ( shape : Array ( Int ), strides : Array ( Int )) # Initialize a CPU storage from an initial capacity. The data will be filled with zeros Arguments # shape : Array(Int) - Shape of the parent Tensor strides : Array(Int) - Strides of the parent Tensor Examples # CPU . new ( [ 2 , 3 , 4 ] ) View source Class methods # .base ( dtype : U . class ) : CPU ( U ) . class forall U # Return a generic class of a specific generic type, to allow for explicit return types in functions that return a different storage type than the parent Tensor Examples # a = CPU ( Float32 ) . new ( [ 10 ] ) # Cannot do # a.class.new ... a . class . base ( Float64 ) . new ( [ 10 ] ) View source Methods # #data : Pointer ( T ) # Raw Crystal pointer that holds a CPU(T) s data View source #to_hostptr : Pointer ( T ) # Converts a CPU storage to a crystal pointer Examples # a = CPU ( Int32 ) . new ( [ 3 , 3 , 2 ] ) a . to_hostptr View source #to_unsafe # Returns the raw Crystal pointer associated with a CPU(T) View source","title":"CPU"},{"location":"CPU/#CPU","text":"","title":"CPU"},{"location":"CPU/#CPU-constructors","text":"","title":"Constructors"},{"location":"CPU/#CPU.new(shape,order,value)","text":"Initialize a CPU storage from an initial capacity and an initial value, which will fill the buffer","title":".new"},{"location":"CPU/#CPU-class-methods","text":"","title":"Class methods"},{"location":"CPU/#CPU.base(dtype)","text":"Return a generic class of a specific generic type, to allow for explicit return types in functions that return a different storage type than the parent Tensor","title":".base"},{"location":"CPU/#CPU-methods","text":"","title":"Methods"},{"location":"CPU/#CPU#data","text":"Raw Crystal pointer that holds a CPU(T) s data View source","title":"#data"},{"location":"CPU/#CPU#to_hostptr","text":"Converts a CPU storage to a crystal pointer","title":"#to_hostptr"},{"location":"CPU/#CPU#to_unsafe","text":"Returns the raw Crystal pointer associated with a CPU(T) View source","title":"#to_unsafe"},{"location":"Num/","text":"module Num # Extended modules Num Constants # ColMajor = OrderType :: ColMajor # RowMajor = OrderType :: RowMajor # VERSION = \"0.5.0\" # Methods # #acos ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method acos on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . acos ( a ) View source #acos ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function acos for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acos ( a ) View source #acos! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method acos on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . acos ( a ) View source #acos! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function acos for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acos! ( a ) View source #acosh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method acosh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . acosh ( a ) View source #acosh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function acosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acosh ( a ) View source #acosh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method acosh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . acosh ( a ) View source #acosh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function acosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acosh! ( a ) View source #acospi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function acospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acospi ( a ) View source #acospi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function acospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . acospi! ( a ) View source #add ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :+ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . add ( a , b ) View source #add ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :+ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . add ( a , b ) View source #add ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :+ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . add ( b , a ) View source #add ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Add two Tensor s elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to add b : Tensor(U, OCL(U)) - RHS argument to add Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . add ( a , a ) View source #add ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Add a Tensor and a Number elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to add b : U - RHS argument to add Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . add ( a , 3.5 ) View source #add ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Add a Number and a Tensor elementwise Arguments # a : U - LHS argument to add b : Tensor(U, OCL(U)) - RHS argument to add Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . add ( a , 3.5 ) View source #add! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :+ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . add! ( a , b ) # a is modified View source #add! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :+ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . add! ( a , b ) View source #add! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # \"Add\" two Tensor s elementwise, storing the result in the first Tensor Arguments # a : Tensor(U, OCL(U)) - LHS argument to add, will be modified inplace b : Tensor(U, OCL(U)) - RHS argument to add Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . add! ( a , a ) View source #add! ( a : Tensor ( U , OCL ( U )), b : U ) : Nil forall U # Add a Tensor and a Number elementwise, modifying the Tensor inplace. Arguments # a : Tensor(U, OCL(U)) - LHS argument to add b : U - RHS argument to add Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . add ( a , 3.5 ) View source #all ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, asserting the truthiness of all values in each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . all ( a , 0 ) # => [false, true] Num . all ( a , 1 , dims : true ) # [[false], # [ true]] View source #all ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a boolean by asserting the truthiness of all elements Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 0 , 2 , 3 ] Num . all ( a ) # => false View source #all_close ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V )), epsilon = 1e-6 ) : Bool forall U , V # Asserts that two Tensor s are equal, allowing for small margins of errors with floating point values using an EPSILON value. Arguments # a : Tensor(U, CPU(U)) - LHS argument to compare b : Tensor(V, CPU(V)) - RHS argument to compare epsilon : Number - Allowed variance between numbers Examples # a = [ 0.0 , 0.0 , 0.0000000001 ]. to_tensor b = [ 0.0 , 0.0 , 0.0 ]. to_tensor Num . all_close ( a , b ) # => true View source #any ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, asserting the truthiness of any values in each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . any ( a , 0 ) # => [true, true] Num . any ( a , 1 , dims : true ) # [[true], # [ true]] View source #any ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a boolean by asserting the truthiness of any element Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 0 , 2 , 3 ] Num . any ( a ) # => true View source #argmax ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Find the maximum index value of a Tensor along an axis Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 2 , 1 ] , [ 1 , 2 ]]. to_tensor puts a . argmax ( 1 ) # => [0, 1] View source #argmax ( a : Tensor ( U , CPU ( U ))) : Int32 forall U # Find the maximum index value of a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 10 , 1 ]. to_tensor a . argmax # => 1 View source #argmin ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Find the minimum index value of a Tensor along an axis Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 2 , 1 ] , [ 1 , 2 ]]. to_tensor puts a . argmin ( 1 ) # => [1, 0] View source #argmin ( a : Tensor ( U , CPU ( U ))) : Int32 forall U # Find the minimum index value of a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 10 , 1 , 10 ]. to_tensor a . argmin # => 1 View source #array_split ( a : Tensor ( U , CPU ( U )), ind : Int , axis : Int = 0 ) : Array ( Tensor ( U , CPU ( U ))) forall U # Split a Tensor into multiple sub- Tensor s Arguments # a : Tensor(U, CPU(U)) - Tensor to split` ind : Int - Number of sections of resulting Array axis : Int - Axis along which to split Examples # a = Tensor . range ( 9 ) puts Num . array_split ( a , 2 ) # => [[0, 1, 2, 3, 4], [5, 6, 7, 8]] View source #array_split ( a : Tensor ( U , CPU ( U )), ind : Array ( Int ), axis : Int = 0 ) : Array ( Tensor ( U , CPU ( U ))) forall U # Split a Tensor into multiple sub- Tensor s, using an explicit mapping of indices to split the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to split` ind : Int - Array of indices to use when splitting the Tensor axis : Int - Axis along which to split Examples # a = Tensor . range ( 9 ) puts Num . array_split ( a , [ 1 , 3 , 5 ] ) # => [[0], [1, 2], [3, 4], [5, 6, 7, 8]] View source #as_strided ( arr : Tensor ( U , V ), shape : Array ( Int ), strides : Array ( Int )) : Tensor ( U , V ) forall U , V # as_strided creates a view into the Tensor given the exact strides and shape. This means it manipulates the internal data structure of a Tensor and, if done incorrectly, the array elements can point to invalid memory and can corrupt results or crash your program. It is advisable to always use the original strides when calculating new strides to avoid reliance on a contiguous memory layout. Furthermore, Tensor s created with this function often contain self overlapping memory, so that two elements are identical. Vectorized write operations on such Tensor s will typically be unpredictable. They may even give different results for small, large, or transposed Tensor s. Arguments # arr : Tensor - Tensor to broadcast shape : Array(Int) - Shape of broadcasted Tensor strides : Array(Int) - Strides of broadcasted Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . as_strided ( [ 3 , 3 ] , [ 0 , 1 ] ) # [[1, 2, 3], # [1, 2, 3], # [1, 2, 3]] View source #as_type ( arr : Tensor ( U , CPU ( U )), dtype : V . class ) forall U , V # Casts a Tensor to a new dtype, by making a copy. Information may be lost when converting between data types, for example Float to Int or Int to Bool. Arguments # u : U.class - Data type the Tensor will be cast to Examples # a = Tensor . from_array [ 1.5 , 2.5 , 3.5 ] a . astype ( Int32 ) # => [1, 2, 3] a . astype ( Bool ) # => [true, true, true] a . astype ( Float32 ) # => [1.5, 2.5, 3.5] View source #asin ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method asin on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . asin ( a ) View source #asin ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function asin for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asin ( a ) View source #asin! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method asin on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . asin ( a ) View source #asin! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function asin for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asin! ( a ) View source #asinh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method asinh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . asinh ( a ) View source #asinh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function asinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asinh ( a ) View source #asinh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method asinh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . asinh ( a ) View source #asinh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function asinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asinh! ( a ) View source #asinpi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function asinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asinpi ( a ) View source #asinpi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function asinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . asinpi! ( a ) View source #atan ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method atan on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . atan ( a ) View source #atan ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atan for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atan ( a ) View source #atan! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method atan on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . atan ( a ) View source #atan! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function atan for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atan! ( a ) View source #atan2 ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method atan2 on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . atan2 ( a , b ) View source #atan2 ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method atan2 on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . atan2 ( a , b ) View source #atan2 ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method atan2 on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . atan2 ( a , b ) View source #atan2 ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atan2 between two Tensor s Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . atan2 ( a , b ) View source #atan2 ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atan2 between two a Tensor and a Number Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : U - RHS Number for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atan2 ( a , 3_f64 ) View source #atan2 ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atan2 between two a Tensor and a Number Arguments # a : U - LHS Number for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atan2 ( 3_f64 , a ) View source #atan2! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method atan2 on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . atan2! ( a , b ) View source #atan2! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method atan2 on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . atan2! ( a , b ) View source #atan2! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function atan2 between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . atan2! ( a , b ) View source #atanh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method atanh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . atanh ( a ) View source #atanh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atanh ( a ) View source #atanh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method atanh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . atanh ( a ) View source #atanh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function atanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atanh! ( a ) View source #atanpi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function atanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atanpi ( a ) View source #atanpi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function atanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . atanpi! ( a ) View source #besselj ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method besselj on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . besselj ( a , b ) View source #besselj ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method besselj on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . besselj ( a , b ) View source #besselj ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method besselj on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . besselj ( a , b ) View source #besselj! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method besselj on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . besselj! ( a , b ) View source #besselj! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method besselj on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . besselj! ( a , b ) View source #besselj0 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method besselj0 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . besselj0 ( a ) View source #besselj0! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method besselj0 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . besselj0 ( a ) View source #besselj1 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method besselj1 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . besselj1 ( a ) View source #besselj1! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method besselj1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . besselj1 ( a ) View source #bessely ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method bessely on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . bessely ( a , b ) View source #bessely ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method bessely on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . bessely ( a , b ) View source #bessely ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method bessely on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . bessely ( a , b ) View source #bessely! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method bessely on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . bessely! ( a , b ) View source #bessely! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method bessely on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . bessely! ( a , b ) View source #bessely0 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method bessely0 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . bessely0 ( a ) View source #bessely0! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method bessely0 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . bessely0 ( a ) View source #bessely1 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method bessely1 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . bessely1 ( a ) View source #bessely1! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method bessely1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . bessely1 ( a ) View source #bitwise_and ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :& operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_and ( a , b ) View source #bitwise_and ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :& operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_and ( a , b ) View source #bitwise_and ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :& operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_and ( b , a ) View source #bitwise_and ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"&\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"&\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"&\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . bitwise_and ( a , a ) View source #bitwise_and ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"&\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"&\" operator b : U - RHS argument to the \"&\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_and ( a , 3 ) View source #bitwise_and ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"&\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : U - LHS argument to the \"&\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"&\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_and ( 3 , a ) View source #bitwise_and! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :& operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_and! ( a , b ) # a is modified View source #bitwise_and! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :& operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_and! ( a , b ) View source #bitwise_or ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :| operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_or ( a , b ) View source #bitwise_or ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :| operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_or ( a , b ) View source #bitwise_or ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :| operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_or ( b , a ) View source #bitwise_or ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"|\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"|\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"|\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . bitwise_or ( a , a ) View source #bitwise_or ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"|\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"|\" operator b : U - RHS argument to the \"|\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_or ( a , 3 ) View source #bitwise_or ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"|\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : U - LHS argument to the \"|\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"|\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_or ( 3 , a ) View source #bitwise_or! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :| operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_or! ( a , b ) # a is modified View source #bitwise_or! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :| operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_or! ( a , b ) View source #bitwise_xor ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :^ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_xor ( a , b ) View source #bitwise_xor ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :^ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_xor ( a , b ) View source #bitwise_xor ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :^ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_xor ( b , a ) View source #bitwise_xor ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"^\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"^\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"^\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . bitwise_xor ( a , a ) View source #bitwise_xor ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"^\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"^\" operator b : U - RHS argument to the \"^\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_xor ( a , 3 ) View source #bitwise_xor ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"^\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : U - LHS argument to the \"^\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"^\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . bitwise_xor ( 3 , a ) View source #bitwise_xor! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :^ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . bitwise_xor! ( a , b ) # a is modified View source #bitwise_xor! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :^ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . bitwise_xor! ( a , b ) View source #broadcast ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V )), c : Tensor ( W , CPU ( W ))) forall U , V , W # Broadcasts two Tensor 's' to a new shape. This allows for elementwise operations between the two Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error. Arguments # a : Tensor(U, CPU(U)) - First Tensor to broadcast b : Tensor(V, CPU(V)) - Second Tensor to broadcast c : Tensor(W, CPU(W)) - Third Tensor to broadcast Examples # a = Tensor . from_array [ 1 , 2 , 3 ] b = Tensor . new ( [ 3 , 3 ] ) { | i | i } c = Tensor . new ( [ 3 , 3 , 3 , 3 ] ) { | i | i } x , y , z = a . broadcast ( b , c ) x . shape # => [3, 3, 3, 3] View source #broadcast ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) forall U , V # Broadcasts two Tensor 's' to a new shape. This allows for elementwise operations between the two Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error. Arguments # a : Tensor(U, CPU(U)) - First Tensor to broadcast b : Tensor(V, CPU(V)) - Second Tensor to broadcast Examples # a = Tensor . from_array [ 1 , 2 , 3 ] b = Tensor . new ( [ 3 , 3 ] ) { | i | i } x , y = a . broadcast ( b ) x . shape # => [3, 3] View source #broadcast ( a : Tensor ( U , OCL ( U )), b : Tensor ( V , OCL ( V ))) forall U , V # Broadcasts two Tensor 's' to a new shape. This allows for elementwise operations between the two Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error. Examples a = Tensor . from_array [ 1 , 2 , 3 ] b = Tensor . new ( [ 3 , 3 ] ) { | i | i } x , y = a . broadcast ( b ) x . shape # => [3, 3] View source #broadcast_to ( arr : Tensor ( U , V ), shape : Array ( Int )) forall U , V # Broadcasts a Tensor to a new shape. Returns a read-only view of the original Tensor . Many elements in the Tensor will refer to the same memory location, and the result is rarely contiguous. Shapes must be broadcastable, and an error will be raised if broadcasting fails. Arguments # arr : Tensor - Tensor to broadcast shape : Array(Int) - The shape of the desired output Tensor Examples # a = [ 1 , 2 , 3 ]. to_tensor a . broadcast_to ( [ 3 , 3 ] ) # [[1, 2, 3], # [1, 2, 3], # [1, 2, 3]] View source #cbrt ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method cbrt on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cbrt ( a ) View source #cbrt ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function cbrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cbrt ( a ) View source #cbrt! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method cbrt on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cbrt ( a ) View source #cbrt! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function cbrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cbrt! ( a ) View source #ceil ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function ceil for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . ceil ( a ) View source #ceil! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function ceil for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . ceil! ( a ) View source #concatenate ( arrs : Array ( Tensor ( U , V )), axis : Int ) forall U , V # Join a sequence of Tensor s along an existing axis. The Tensor s must have the same shape for all axes other than the axis of concatenation Arguments # arrs : Array(Tensor) - Tensor s to concatenate axis : Int - Axis of concatenation, negative axes are allowed Examples # a = [ 1 , 2 , 3 ]. to_tensor b = Tensor . from_array [ 4 , 5 , 6 ] Num . concat ( [ a , b ] , 0 ) # => [1, 2, 3, 4, 5, 6] c = Tensor . new ( [ 3 , 2 , 2 ] ) { | i | i / 2 } Num . concat ( [ c , c , c ] , - 1 ) # [[[0 , 0.5, 0 , 0.5, 0 , 0.5], # [1 , 1.5, 1 , 1.5, 1 , 1.5]], # # [[2 , 2.5, 2 , 2.5, 2 , 2.5], # [3 , 3.5, 3 , 3.5, 3 , 3.5]], # # [[4 , 4.5, 4 , 4.5, 4 , 4.5], # [5 , 5.5, 5 , 5.5, 5 , 5.5]]] View source #concatenate ( * arrs : Tensor ( U , V ), axis : Int ) forall U , V # Join a sequence of Tensor s along an existing axis. The Tensor s must have the same shape for all axes other than the axis of concatenation Arguments # arrs : Tuple(Tensor) - Tensor s to concatenate axis : Int - Axis of concatenation, negative axes are allowed Examples # a = [ 1 , 2 , 3 ]. to_tensor b = Tensor . from_array [ 4 , 5 , 6 ] Num . concat ( [ a , b ] , 0 ) # => [1, 2, 3, 4, 5, 6] c = Tensor . new ( [ 3 , 2 , 2 ] ) { | i | i / 2 } Num . concat ( [ c , c , c ] , - 1 ) # [[[0 , 0.5, 0 , 0.5, 0 , 0.5], # [1 , 1.5, 1 , 1.5, 1 , 1.5]], # # [[2 , 2.5, 2 , 2.5, 2 , 2.5], # [3 , 3.5, 3 , 3.5, 3 , 3.5]], # # [[4 , 4.5, 4 , 4.5, 4 , 4.5], # [5 , 5.5, 5 , 5.5, 5 , 5.5]]] View source #copysign ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method copysign on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . copysign ( a , b ) View source #copysign ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method copysign on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . copysign ( a , b ) View source #copysign ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method copysign on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . copysign ( a , b ) View source #copysign! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method copysign on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . copysign! ( a , b ) View source #copysign! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method copysign on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . copysign! ( a , b ) View source #cos ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method cos on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cos ( a ) View source #cos ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function cos for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cos ( a ) View source #cos! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method cos on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cos ( a ) View source #cos! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function cos for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cos! ( a ) View source #cosh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method cosh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cosh ( a ) View source #cosh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function cosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cosh ( a ) View source #cosh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method cosh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . cosh ( a ) View source #cosh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function cosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cosh! ( a ) View source #cospi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function cospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cospi ( a ) View source #cospi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function cospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . cospi! ( a ) View source #cpu ( arr : Tensor ( U , CPU ( U ))) forall U # Converts a CPU Tensor to CPU. Returns the input array, no copy is performed. Arguments # arr : Tensor(U, CPU(U)) - Tensor to return Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . opencl # => \"<4> on OpenCL Backend\" View source #cpu ( arr : Tensor ( U , OCL ( U ))) forall U # Converts a Tensor stored on an OpenCL device to a Tensor stored on a CPU. Arguments # arr : Tensor(U, OCL(U)) - Tensor to place on a CPU Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } a . cpu # => [[0, 1], [2, 3]] View source #diagonal ( arr : Tensor ( U , CPU ( U ))) forall U # Returns a view of the diagonal of a Tensor . This method only works for two-dimensional arrays. Todo Implement views for offset diagonals Arguments # arr : Tensor(U, CPU(U)) - Tensor to view along the diagonal Examples # a = Tensor . new ( 3 , 3 ) { | i , _ | i } a . diagonal # => [0, 1, 2] View source #divide ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :/ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . divide ( a , b ) View source #divide ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :/ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . divide ( a , b ) View source #divide ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :/ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . divide ( b , a ) View source #divide ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Divide two Tensor s elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to divide b : Tensor(U, OCL(U)) - RHS argument to divide Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . divide ( a , a ) View source #divide ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Divide a Tensor and a Number elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to divide b : U - RHS argument to divide Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . divide ( a , 3.5 ) View source #divide ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Divide a Number and a Tensor elementwise Arguments # a : U - LHS argument to divide b : Tensor(U, OCL(U)) - RHS argument to divide Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . divide ( a , 3.5 ) View source #divide! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :/ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . divide! ( a , b ) # a is modified View source #divide! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :/ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . divide! ( a , b ) View source #divide! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # \"Divide\" two Tensor s elementwise, storing the result in the first Tensor Arguments # a : Tensor(U, OCL(U)) - LHS argument to divide, will be modified inplace b : Tensor(U, OCL(U)) - RHS argument to divide Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . divide! ( a , a ) View source #divide! ( a : Tensor ( U , OCL ( U )), b : U ) : Nil forall U # Divide a Tensor and a Number elementwise, modifying the Tensor inplace. Arguments # a : Tensor(U, OCL(U)) - LHS argument to divide b : U - RHS argument to divide Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . divide ( a , 3.5 ) View source #dup ( t : Tensor ( U , CPU ( U )), order : Num::OrderType = Num :: RowMajor ) forall U # Deep-copies a Tensor . If an order is provided, the returned Tensor 's memory layout will respect that order. If no order is provided, the Tensor will retain it's same memory layout. Arguments # t : Tensor(U, CPU(U)) - Tensor to duplicate order : Num::OrderType - Memory layout to use for the returned Tensor Examples # - a = Tensor . from_array [ 1 , 2 , 3 ] a . dup # => [1, 2, 3] View source #each ( arr : Tensor ( U , CPU ( U )), & block : U -> _ ) forall U # Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Arguments # arr : Tensor(U, CPU(U)) - Tensor from which values will be yielded Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each do | el | puts el end # 0 # 1 # 2 # 3 View source #each ( arr : Tensor ( U , CPU ( U ))) forall U # Yields the elements of a Tensor lazily, always in RowMajor order, as if the Tensor was flat. Arguments # arr : Tensor(U, CPU(U)) - Tensor from which to create an iterator Examples # a = Tensor . new ( 2 , 2 ) { | i | i } iter = a . each a . size . times do puts iter . next . value end # 0 # 1 # 2 # 3 View source #each_axis ( a0 : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false , & block : Tensor ( U , CPU ( U )) -> _ ) forall U # Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor Arguments # a0 : Tensor(U, CPU(U)) - Tensor to iterate along axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] ) { | i | i } a . each_axis ( 1 ) do | ax | puts ax end # [0, 3, 6] # [1, 4, 7] # [2, 5, 8] View source #each_axis ( arr : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Returns an iterator along each element of an axis . Each element returned by the iterator is a view, not a copy Arguments # arr : Tensor(U, CPU(U)) - Tensor to iterate along axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] ) { | i | i } a . each_axis ( 1 ) . next # => [0, 3, 6] View source #each_axis ( a0 : Tensor ( U , OCL ( U )), axis : Int , dims : Bool = false , & block : Tensor ( U , OCL ( U )) -> _ ) forall U # Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor Arguments # a0 : Tensor(U, OCL(U)) - Tensor to iterate along axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] , device : OCL ) { | i | i } a . each_axis ( 1 ) do | ax | puts ax . cpu end # [0, 3, 6] # [1, 4, 7] # [2, 5, 8] View source #each_pointer ( arr : Tensor ( U , CPU ( U )), & block : Pointer ( U ) -> _ ) forall U # Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor Arguments # arr : Tensor(U, CPU(U)) - Tensor from which values will be yielded Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_pointer do | el | puts el . value end # 0 # 1 # 2 # 3 View source #each_pointer_with_index ( arr : Tensor ( U , CPU ( U )), & block : Pointer ( U ), Int32 -> _ ) forall U # Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor Arguments # arr : Tensor(U, CPU(U)) - Tensor from which values will be yielded Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_pointer do | el | puts el . value end # 0 # 1 # 2 # 3 View source #each_with_index ( arr : Tensor ( U , CPU ( U )), & block : U , Int32 -> _ ) forall U # Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Also yields the flat index of each element. Arguments # arr : Tensor(U, CPU(U)) - Tensor from which values will be yielded Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_with_index do | el , i | puts \" #{ el } _ #{ i } \" end # 0_0 # 1_1 # 2_2 # 3_3 View source #equal ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :== operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . equal ( a , b ) View source #equal ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :== operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . equal ( a , b ) View source #equal ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :== operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . equal ( b , a ) View source #equal ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"==\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"==\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"==\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . equal ( a , a ) View source #equal ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"==\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"==\" operator b : U - RHS argument to the \"==\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . equal ( a , 3 ) View source #equal ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"==\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \"==\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"==\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . equal ( 3 , a ) View source #equal! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :== operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . equal! ( a , b ) # a is modified View source #equal! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :== operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . equal! ( a , b ) View source #erf ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method erf on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . erf ( a ) View source #erf ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function erf for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . erf ( a ) View source #erf! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method erf on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . erf ( a ) View source #erf! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function erf for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . erf! ( a ) View source #erfc ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method erfc on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . erfc ( a ) View source #erfc ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function erfc for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . erfc ( a ) View source #erfc! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method erfc on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . erfc ( a ) View source #erfc! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function erfc for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . erfc! ( a ) View source #exp ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method exp on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . exp ( a ) View source #exp ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function exp for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp ( a ) View source #exp! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method exp on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . exp ( a ) View source #exp! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function exp for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp! ( a ) View source #exp10 ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function exp10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp10 ( a ) View source #exp10! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function exp10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp10! ( a ) View source #exp2 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method exp2 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . exp2 ( a ) View source #exp2 ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function exp2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp2 ( a ) View source #exp2! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method exp2 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . exp2 ( a ) View source #exp2! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function exp2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . exp2! ( a ) View source #expand_dims ( arr : Tensor ( U , V ), axis : Int ) : Tensor ( U , V ) forall U , V # Expands a Tensor along an axis Arguments # arr : Tensor - Tensor to expand axis : Int - Axis along which to expand Examples # a = [ 1 , 2 , 3 ]. to_tensor a . expand_dims ( 0 ) # => [[1, 2, 3]] View source #expm1 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method expm1 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . expm1 ( a ) View source #expm1 ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function expm1 for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . expm1 ( a ) View source #expm1! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method expm1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . expm1 ( a ) View source #expm1! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function expm1 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . expm1! ( a ) View source #fabs ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fabs for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fabs ( a ) View source #fabs! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function fabs for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fabs! ( a ) View source #flat ( arr : Tensor ( U , CPU ( U ))) forall U # Flattens a Tensor to a single dimension. If a view can be created, the reshape operation will not copy data. Arguments # arr : Tensor(U, CPU(U)) - Tensor to flatten Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . flat # => [0, 1, 2, 3] View source #flip ( a : Tensor ( U , CPU ( U )), axis : Int ) forall U # Flips a Tensor along an axis, returning a view Arguments # a : Tensor(U, CPU(U)) - Tensor to flip axis : Int - Axis along which to flip Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts Num . flip ( a , 1 ) # [[3, 2, 1], # [6, 5, 4]] View source #flip ( a : Tensor ( U , CPU ( U ))) forall U # Flips a Tensor along all axes, returning a view Arguments # a : Tensor(U, CPU(U)) - Tensor to flip Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts Num . flip ( a ) # [[6, 5, 4], # [3, 2, 1]] View source #floor ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function floor for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . floor ( a ) View source #floor! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function floor for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . floor! ( a ) View source #floordiv ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :// operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . floordiv ( a , b ) View source #floordiv ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :// operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . floordiv ( a , b ) View source #floordiv ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :// operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . floordiv ( b , a ) View source #floordiv! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :// operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . floordiv! ( a , b ) # a is modified View source #floordiv! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :// operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . floordiv! ( a , b ) View source #gamma ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method gamma on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . gamma ( a ) View source #gamma! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method gamma on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . gamma ( a ) View source #greater ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . greater ( a , b ) View source #greater ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater ( a , b ) View source #greater ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :> operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater ( b , a ) View source #greater ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . greater ( a , a ) View source #greater ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">\" operator b : U - RHS argument to the \">\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . greater ( a , 3 ) View source #greater ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \">\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . greater ( 3 , a ) View source #greater! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . greater! ( a , b ) # a is modified View source #greater! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater! ( a , b ) View source #greater_equal ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :>= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . greater_equal ( a , b ) View source #greater_equal ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :>= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater_equal ( a , b ) View source #greater_equal ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :>= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater_equal ( b , a ) View source #greater_equal ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . greater_equal ( a , a ) View source #greater_equal ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">=\" operator b : U - RHS argument to the \">=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . greater_equal ( a , 3 ) View source #greater_equal ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \">=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \">=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . greater_equal ( 3 , a ) View source #greater_equal! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :>= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . greater_equal! ( a , b ) # a is modified View source #greater_equal! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :>= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . greater_equal! ( a , b ) View source #hstack ( arrs : Array ( Tensor ( U , V ))) forall U , V # Stack an array of Tensor s in sequence column-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3 For one dimensional Tensor s, this will still stack along the first axis Arguments # arrs : Array(Tensor) - Tensor s to concatenate Examples # a = [ 1 , 2 , 3 ]. to_tensor Num . h_concat ( [ a , a ] ) # => [1, 2, 3, 1, 2, 3] b = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor Num . h_concat ( [ b , b ] ) # [[1, 2, 1, 2], # [3, 4, 3, 4]] View source #hstack ( * arrs : Tensor ( U , V )) forall U , V # Stack an array of Tensor s in sequence column-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3 For one dimensional Tensor s, this will still stack along the first axis Arguments # arrs : Tuple(Tensor) - Tensor s to concatenate Examples # a = [ 1 , 2 , 3 ]. to_tensor Num . h_concat ( [ a , a ] ) # => [1, 2, 3, 1, 2, 3] b = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor Num . h_concat ( [ b , b ] ) # [[1, 2, 1, 2], # [3, 4, 3, 4]] View source #hypot ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method hypot on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . hypot ( a , b ) View source #hypot ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method hypot on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . hypot ( a , b ) View source #hypot ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method hypot on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . hypot ( a , b ) View source #hypot! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method hypot on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . hypot! ( a , b ) View source #hypot! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method hypot on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . hypot! ( a , b ) View source #ilogb ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method ilogb on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . ilogb ( a ) View source #ilogb! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method ilogb on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . ilogb ( a ) View source #ldexp ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method ldexp on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . ldexp ( a , b ) View source #ldexp ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method ldexp on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . ldexp ( a , b ) View source #ldexp ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method ldexp on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . ldexp ( a , b ) View source #ldexp! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method ldexp on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . ldexp! ( a , b ) View source #ldexp! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method ldexp on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . ldexp! ( a , b ) View source #left_shift ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :<< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . left_shift ( a , b ) View source #left_shift ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :<< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . left_shift ( a , b ) View source #left_shift ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :<< operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . left_shift ( b , a ) View source #left_shift ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"<<\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<<\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . left_shift ( a , a ) View source #left_shift ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"<<\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<<\" operator b : U - RHS argument to the \"<<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . left_shift ( a , 3 ) View source #left_shift ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \"<<\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : U - LHS argument to the \"<<\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . left_shift ( 3 , a ) View source #left_shift! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :<< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . left_shift! ( a , b ) # a is modified View source #left_shift! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :<< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . left_shift! ( a , b ) View source #less ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . less ( a , b ) View source #less ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less ( a , b ) View source #less ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :< operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less ( b , a ) View source #less ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . less ( a , a ) View source #less ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<\" operator b : U - RHS argument to the \"<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . less ( a , 3 ) View source #less ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \"<\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . less ( 3 , a ) View source #less! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . less! ( a , b ) # a is modified View source #less! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less! ( a , b ) View source #less_equal ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :<= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . less_equal ( a , b ) View source #less_equal ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :<= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less_equal ( a , b ) View source #less_equal ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :<= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less_equal ( b , a ) View source #less_equal ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . less_equal ( a , a ) View source #less_equal ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"<=\" operator b : U - RHS argument to the \"<=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . less_equal ( a , 3 ) View source #less_equal ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"<=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \"<=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"<=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . less_equal ( 3 , a ) View source #less_equal! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :<= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . less_equal! ( a , b ) # a is modified View source #less_equal! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :<= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . less_equal! ( a , b ) View source #lgamma ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method lgamma on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . lgamma ( a ) View source #lgamma ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function lgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . lgamma ( a ) View source #lgamma! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method lgamma on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . lgamma ( a ) View source #lgamma! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function lgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . lgamma! ( a ) View source #log ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method log on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log ( a ) View source #log ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function log for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log ( a ) View source #log! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method log on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log ( a ) View source #log! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function log for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log! ( a ) View source #log10 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method log10 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log10 ( a ) View source #log10 ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function log10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log10 ( a ) View source #log10! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method log10 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log10 ( a ) View source #log10! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function log10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log10! ( a ) View source #log1p ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method log1p on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log1p ( a ) View source #log1p ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function log1p for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log1p ( a ) View source #log1p! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method log1p on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log1p ( a ) View source #log1p! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function log1p for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log1p! ( a ) View source #log2 ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method log2 on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log2 ( a ) View source #log2 ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function log2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log2 ( a ) View source #log2! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method log2 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . log2 ( a ) View source #log2! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function log2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . log2! ( a ) View source #logb ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method logb on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . logb ( a ) View source #logb ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function logb for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . logb ( a ) View source #logb! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method logb on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . logb ( a ) View source #logb! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function logb for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . logb! ( a ) View source #map ( a0 : Tensor ( U , CPU ( U )), a1 : Tensor ( V , CPU ( V )), a2 : Tensor ( W , CPU ( W )), & block : U , V , W -> X ) : Tensor ( X , CPU ( X )) forall U , V , W , X # Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block Arguments # a0 : Tensor(U, CPU(U)) - First Tensor for iteration, must be broadcastable against the shape of a1 and a2 a1 : Tensor(V, CPU(V)) - Second Tensor for iteration, must be broadcastable against the shape of a0 and a2 a2 : Tensor(W, CPU(W)) - Third Tensor for iteration, must be broadcastable against the shape of a0 and a1 block : Proc(T, U, V) - The block to map across both Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } c = Tensor . new ( [ 3 ] ) { | i | i } a . map ( b , c ) { | i , j , k | i + j + k } # => [0, 3, 6] View source #map ( a0 : Tensor ( U , CPU ( U )), a1 : Tensor ( V , CPU ( V )), & block : U , V -> W ) forall U , V , W # Maps a block across two Tensors . This is more efficient than zipping iterators since it iterates both Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block Arguments # a0 : Tensor(U, CPU(U)) - First Tensor for iteration, must be broadcastable against the shape of a1 a1 : Tensor(V, CPU(V)) - Second Tensor for iteration, must be broadcastable against the shape of a0 block : Proc(T, U, V) - The block to map across both Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } a . map ( b ) { | i , j | i + j } # => [0, 2, 4] View source #map ( arr : Tensor ( U , CPU ( U )), & block : U -> V ) : Tensor ( V , CPU ( V )) forall U , V # Maps a block across a Tensor . The Tensor is treated as flat during iteration, and iteration is always done in RowMajor order The generic type of the returned Tensor is inferred from the block Arguments # arr : Tensor(U, CPU(U)) - Tensor to map the Proc across block : Proc(T, U) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . map { | e | e + 5 } # => [5, 6, 7] View source #map! ( a0 : Tensor ( U , CPU ( U )), a1 : Tensor ( V , CPU ( V )), a2 : Tensor ( W , CPU ( W )), & ) forall U , V , W # Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor 's must broadcast to the shape of self Arguments # a0 : Tensor(U, CPU(U)) - First Tensor for iteration a1 : Tensor(V, CPU(V)) - Second Tensor for iteration, must be broadcastable against the shape of a0 a2 : Tensor(W, CPU(W)) - Third Tensor for iteration, must be broadcastable against the shape of a0 block : Proc(T, U, V) - The block to map across both Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } c = Tensor . new ( [ 3 ] ) { | i | i } a . map! ( b , c ) { | i , j , k | i + j + k } a # => [0, 3, 6] View source #map! ( a0 : Tensor ( U , CPU ( U )), a1 : Tensor ( V , CPU ( V )), & block : U , V -> _ ) forall U , V # Maps a block across two Tensors . This is more efficient than zipping iterators since it iterates both Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor must broadcast to the shape of self Arguments # a0 : Tensor(U, CPU(U)) - First Tensor for iteration a1 : Tensor(V, CPU(V)) - Second Tensor for iteration, must be broadcastable against the shape of a0 block : Proc(T, U, V) - The block to map across both Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } a . map! ( b ) { | i , j | i + j } a # => [0, 2, 4] View source #map! ( arr : Tensor ( U , CPU ( U )), & block : U -> _ ) forall U # Maps a block across a Tensor in place. The Tensor is treated as flat during iteration, and iteration is always done in RowMajor order Arguments # arr : Tensor(U, CPU(U)) - Tensor to map the Proc across block : Proc(T, U) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . map! { | e | e + 5 } a # => [5, 6, 7] View source #max ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method max on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . max ( a , b ) View source #max ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, finding the max of each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . max ( a , 0 ) # => [2, 3] Num . max ( a , 1 , dims : true ) # [[1], # [3]] View source #max ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method max on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . max ( a , b ) View source #max ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method max on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . max ( a , b ) View source #max ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmax between two Tensor s Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . fmax ( a , b ) View source #max ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmax between two a Tensor and a Number Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : U - RHS Number for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fmax ( a , 3_f64 ) View source #max ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmax between two a Tensor and a Number Arguments # a : U - LHS Number for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fmax ( 3_f64 , a ) View source #max ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by finding the maximum value Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . max ( a ) # => 3 View source #max! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method max on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . max! ( a , b ) View source #max! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method max on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . max! ( a , b ) View source #max! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function fmax between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . fmax! ( a , b ) View source #mean ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, finding the average of each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . mean ( a , 0 ) # => [1, 2] Num . mean ( a , 1 , dims : true ) # [[0], # [2]] View source #mean ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by finding the average Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . mean ( a ) # => 2.0 View source #min ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the stdlib Math method min on two Tensor s, broadcasting the Tensor s together. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . min ( a , b ) View source #min ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, finding the min of each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . min ( a , 0 ) # => [0, 1] Num . min ( a , 1 , dims : true ) # [[0], # [2]] View source #min ( a : Tensor ( U , CPU ( U )), b : Number ) : Tensor forall U # Implements the stdlib Math method min on a Tensor and a Number, broadcasting the method across all elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . min ( a , b ) View source #min ( a : Number , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method min on a Number and a Tensor , broadcasting the method across all elements of a Tensor Arguments # a : Number - RHS argument to the method b : Tensor(U, CPU(U)) - LHS argument to the method Examples # a = 1.5 b = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . min ( a , b ) View source #min ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmin between two Tensor s Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . fmin ( a , b ) View source #min ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmin between two a Tensor and a Number Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : U - RHS Number for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fmin ( a , 3_f64 ) View source #min ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function fmin between two a Tensor and a Number Arguments # a : U - LHS Number for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . fmin ( 3_f64 , a ) View source #min ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by finding the minimum value Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . min ( a ) # => 3 View source #min! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the stdlib Math method min on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Tensor(V, CPU(V)) - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = [ 1.45 , 3.2 , 1.18 ] Num . min! ( a , b ) View source #min! ( a : Tensor ( U , CPU ( U )), b : Number ) : Nil forall U # Implements the stdlib Math method min on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace Arguments # a : Tensor(U, CPU(U)) - LHS argument to the method b : Number - RHS argument to the method Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor b = 1.5 Num . min! ( a , b ) View source #min! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function fmin between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . fmin! ( a , b ) View source #modulo ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :% operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . modulo ( a , b ) View source #modulo ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :% operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . modulo ( a , b ) View source #modulo ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :% operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . modulo ( b , a ) View source #modulo! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :% operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . modulo! ( a , b ) # a is modified View source #modulo! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :% operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . modulo! ( a , b ) View source #move_axis ( arr : Tensor ( U , CPU ( U )), source : Array ( Int ), destination : Array ( Int )) forall U # Move axes of a Tensor to new positions, other axes remain in their original order Arguments # arr : Tensor(U, CPU(U)) - Tensor to permute source : Array(Int) - Original positions of axes destination : Array(Int) - Destination positions of axes Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 4 , 5 ] ) Num . moveaxis ( a , [ 0 ] , [- 1 ] ) . shape # => 4, 5, 3 View source #move_axis ( arr : Tensor ( U , CPU ( U )), source : Int , destination : Int ) forall U # Move axes of a Tensor to new positions, other axes remain in their original order Arguments # arr : Tensor(U, CPU(U)) - Tensor to permute source : Int - Original position of axis destination : Int - Destination position of axis Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 4 , 5 ] ) Num . moveaxis ( a , 0 , 1 ) . shape # => 4, 5, 3 View source #multiply ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :* operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . multiply ( a , b ) View source #multiply ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :* operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . multiply ( a , b ) View source #multiply ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :* operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . multiply ( b , a ) View source #multiply ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Multiply two Tensor s elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to multiply b : Tensor(U, OCL(U)) - RHS argument to multiply Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . multiply ( a , a ) View source #multiply ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Multiply a Tensor and a Number elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to multiply b : U - RHS argument to multiply Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . multiply ( a , 3.5 ) View source #multiply ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Multiply a Number and a Tensor elementwise Arguments # a : U - LHS argument to multiply b : Tensor(U, OCL(U)) - RHS argument to multiply Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . multiply ( a , 3.5 ) View source #multiply! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :* operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . multiply! ( a , b ) # a is modified View source #multiply! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :* operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . multiply! ( a , b ) View source #multiply! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # \"Multiply\" two Tensor s elementwise, storing the result in the first Tensor Arguments # a : Tensor(U, OCL(U)) - LHS argument to multiply, will be modified inplace b : Tensor(U, OCL(U)) - RHS argument to multiply Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . multiply! ( a , a ) View source #multiply! ( a : Tensor ( U , OCL ( U )), b : U ) : Nil forall U # Multiply a Tensor and a Number elementwise, modifying the Tensor inplace. Arguments # a : Tensor(U, OCL(U)) - LHS argument to multiply b : U - RHS argument to multiply Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . multiply ( a , 3.5 ) View source #negate ( a : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Implements the negation operator on a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to negate Examples # a = [ 1 , 2 , 3 ]. to_tensor Num . negate ( a ) # => [-1, -2, -3] View source #not_equal ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :!= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . not_equal ( a , b ) View source #not_equal ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :!= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . not_equal ( a , b ) View source #not_equal ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :!= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . not_equal ( b , a ) View source #not_equal ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"!=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) . Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"!=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"!=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . not_equal ( a , a ) View source #not_equal ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"!=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \"!=\" operator b : U - RHS argument to the \"!=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . not_equal ( a , 3 ) View source #not_equal ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( Int32 , OCL ( Int32 )) forall U # Implements the comparison operator \"!=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`. Arguments # a : U - LHS argument to the \"!=\" operator b : Tensor(U, OCL(U)) - RHS argument to the \"!=\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . not_equal ( 3 , a ) View source #not_equal! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :!= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . not_equal! ( a , b ) # a is modified View source #not_equal! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :!= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . not_equal! ( a , b ) View source #opencl ( arr : Tensor ( U , CPU ( U ))) : Tensor ( U , OCL ( U )) forall U # Places a Tensor stored on a CPU onto an OpenCL Device. Arguments # arr : Tensor(U, CPU(U)) - Tensor to place on OpenCL device. Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . opencl # => \"<4> on OpenCL Backend\" View source #power ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :** operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . power ( a , b ) View source #power ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :** operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . power ( a , b ) View source #power ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :** operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . power ( b , a ) View source #power ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function pow between two Tensor s Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . pow ( a , b ) View source #power ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function pow between two a Tensor and a Number Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : U - RHS Number for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . pow ( a , 3_f64 ) View source #power ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function pow between two a Tensor and a Number Arguments # a : U - LHS Number for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . pow ( 3_f64 , a ) View source #power! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :** operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . power! ( a , b ) # a is modified View source #power! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :** operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . power! ( a , b ) View source #power! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function pow between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation Arguments # a : Tensor(U, OCL(U)) - LHS Tensor for the operation b : Tensor(U, OCL(U)) - RHS Tensor for the operation Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) b = [ 0.2 , 0.5 , 0.1 ]. to_tensor ( OCL ) Num . pow! ( a , b ) View source #prod ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, multiplying each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . prod ( a , 0 ) # => [0, 3] Num . prod ( a , 1 , dims : true ) # [[0], # [6]] View source #prod ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by multiplying all of its elements Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . prod ( a ) # => 6 View source #ptp ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Finds the difference between the maximum and minimum elements of a Tensor along an axis Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 3 , 4 ] , [ 1 , 2 ] , [ 6 , 2 ]] Num . ptp ( a , 1 ) # [1, 1, 4] View source #ptp ( a : Tensor ( U , CPU ( U ))) forall U # Finds the difference between the maximum and minimum elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . ptp ( a ) # => 2 View source #reduce_axis ( a0 : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false , & block : U , U -> _ ) forall U # Reduces a Tensor along an axis. Returns a Tensor , with the axis of reduction either removed, or reduced to 1 if dims is True, which allows the result to broadcast against its previous shape Arguments # a0 : Tensor(U, CPU(U)) - Tensor to reduce along an axis axis : Int - Axis of reduction dims : Bool - Flag determining whether the axis of reduction should be kept in the result block : Proc(U, U, _) - Proc to apply to values along an axis Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . reduce_axis ( 0 ) { | i , j | i + j } # => [2, 4] View source #reduce_axis ( a0 : Tensor ( U , OCL ( U )), axis : Int , dims : Bool = false , & block : Tensor ( U , OCL ( U )), Tensor ( U , OCL ( U )) -> _ ) : Tensor ( U , OCL ( U )) forall U # Reduces a Tensor along an axis. Returns a Tensor , with the axis of reduction either removed, or reduced to 1 if dims is True, which allows the result to broadcast against its previous shape Arguments # a0 : Tensor(U, OCL(U)) - Tensor to reduce along an axis axis : Int - Axis of reduction dims : Bool - Flag determining whether the axis of reduction should be kept in the result block : Proc(Tensor(U, OCL(U)), Tensor(U, OCL(U)), _) - Proc to apply to values along an axis Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . reduce_axis ( 0 ) { | i , j | Num . add! ( i , j ) } # => \"<2> on OpenCL Backend\" View source #repeat ( a : Tensor ( U , CPU ( U )), n : Int , axis : Int ) forall U # Repeat elements of a Tensor along an axis Arguments # a : Tensor(U, CPU(U)) - Tensor to repeat n : Int - Number of times to repeat axis : Int - Axis along which to repeat Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] Num . repeat ( a , 2 , 1 ) # [[1, 1, 2, 2, 3, 3], # [4, 4, 5, 5, 6, 6]] View source #repeat ( a : Tensor ( U , CPU ( U )), n : Int ) forall U # Repeat elements of a Tensor , treating the Tensor as flat Arguments # a : Tensor(U, CPU(U)) - Tensor to repeat n : Int - Number of times to repeat Examples # a = [ 1 , 2 , 3 ] Num . repeat ( a , 2 ) # => [1, 1, 2, 2, 3, 3] View source #reshape ( arr : Tensor ( U , CPU ( U )), shape : Array ( Int )) forall U # Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same. Arguments # arr : Tensor(U, CPU(U)) - Tensor to reshape shape : Array(Int) - New shape for the Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 , 4 ] a . reshape ( [ 2 , 2 ] ) # [[1, 2], # [3, 4]] View source #reshape ( arr : Tensor ( U , CPU ( U )), * shape : Int ) forall U # Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same. Arguments # arr : Tensor(U, CPU(U)) - Tensor to reshape shape : Array(Int) - New shape for the Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 , 4 ] a . reshape ( [ 2 , 2 ] ) # [[1, 2], # [3, 4]] View source #right_shift ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :>> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . right_shift ( a , b ) View source #right_shift ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :>> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . right_shift ( a , b ) View source #right_shift ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :>> operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . right_shift ( b , a ) View source #right_shift ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \">>\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">>\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">>\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) b = [ 1 , 8 , 5 . to_tensor ( OCL ) Num . right_shift ( a , a ) View source #right_shift ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \">>\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : Tensor(U, OCL(U)) - LHS argument to the \">>\" operator b : U - RHS argument to the \">>\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . right_shift ( a , 3 ) View source #right_shift ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the bitwise operator \">>\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported Arguments # a : U - LHS argument to the \">>\" operator b : Tensor(U, OCL(U)) - RHS argument to the \">>\" operator Examples # a = [ 12 , 3 , 5 ]. to_tensor ( OCL ) Num . right_shift ( 3 , a ) View source #right_shift! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :>> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . right_shift! ( a , b ) # a is modified View source #right_shift! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :>> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . right_shift! ( a , b ) View source #rint ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function rint for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . rint ( a ) View source #rint! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function rint for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . rint! ( a ) View source #round ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function round for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . round ( a ) View source #round! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function round for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . round! ( a ) View source #rsqrt ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function rsqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . rsqrt ( a ) View source #rsqrt! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function rsqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . rsqrt! ( a ) View source #set ( arr : Tensor ( U , CPU ( U )), args : Array , t : Tensor ( V , CPU ( V ))) forall U , V # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, CPU(U)) - Tensor to which values will be assigned args : Array - Array of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : Tensor(V, CPU(V)) - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #set ( arr : Tensor ( U , CPU ( U )), args : Array , t : V ) forall U , V # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, CPU(U)) - Tensor to which values will be assigned args : Array - Tuple of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : V - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #set ( arr : Tensor ( U , OCL ( U )), args : Array , t : Tensor ( U , OCL ( U ))) forall U # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, OCL(U)) - Tensor to which values will be assigned args : Array - Array of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : Tensor(U, OCL(U)) - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #set ( arr : Tensor ( U , OCL ( U )), args : Array , t : U ) forall U # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, OCL(U)) - Tensor to which values will be assigned args : Array - Array of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : U - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #set ( arr : Tensor ( U , CPU ( U )), * args , value ) forall U # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, CPU(U)) - Tensor to which values will be assigned args : Tuple - Tuple of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : Tensor | Number - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #set ( arr : Tensor ( U , OCL ( U )), * args , value ) forall U # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # arr : Tensor(U, OCL(U)) - Tensor to which values will be assigned args : Tuple - Tuple of arguments. All arguments must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : Tensor | Number - Argument to assign to the Tensor Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } a [ 1 .. , 1 ..] = 99 a . cpu # [[ 0, 1], # [ 2, 99]] View source #sin ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method sin on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sin ( a ) View source #sin ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function sin for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sin ( a ) View source #sin! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method sin on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sin ( a ) View source #sin! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function sin for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sin! ( a ) View source #sinh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method sinh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sinh ( a ) View source #sinh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function sinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sinh ( a ) View source #sinh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method sinh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sinh ( a ) View source #sinh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function sinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sinh! ( a ) View source #sinpi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function sinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sinpi ( a ) View source #sinpi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function sinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sinpi! ( a ) View source #slice ( arr : Tensor ( U , V ), args : Array ) forall U , V # Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source #slice ( arr : Tensor ( U , V ), * args ) forall U , V # Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source #sort ( a : Tensor ( U , CPU ( U )), axis : Int ) forall U # Sorts a Tensor along an axis. Arguments # a : Tensor(U, CPU(U)) - Tensor to sort axis : Int - Axis along which to sort Examples # t = Tensor . random ( 0 ... 10 , [ 3 , 3 , 2 ] ) puts Num . sort ( t , axis : 1 ) # [[[1, 1], # [4, 5], # [5, 7]], # # [[0, 0], # [2, 3], # [8, 4]], # # [[2, 5], # [5, 7], # [5, 7]]] View source #sort ( a : Tensor ( U , CPU ( U ))) forall U # Sorts a Tensor , treating it's elements like the Tensor is flat. Arguments # a : Tensor(U, CPU(U)) - Tensor to sort Examples # a = [ 3 , 2 , 1 ]. to_tensor Num . sort ( a ) # => [1, 2, 3] View source #sort ( a : Tensor ( U , CPU ( U )), axis : Int , & block : U , U -> _ ) forall U # Sorts a Tensor along an axis. Arguments # a : Tensor(U, CPU(U)) - Tensor to sort axis : Int - Axis along which to sort block : Proc(U, U, _) - Proc to use to sort Examples # t = Tensor . random ( 0 ... 10 , [ 3 , 3 , 2 ] ) puts Num . sort ( t , axis : 1 ) { | i , j | i <=> j } # [[[3, 1], # [4, 3], # [6, 8]], # # [[4, 2], # [5, 3], # [9, 7]], # # [[0, 1], # [2, 2], # [4, 9]]] View source #sort ( a : Tensor ( U , CPU ( U )), & block : U , U -> _ ) forall U # Sorts a Tensor , treating it's elements like the Tensor is flat. Arguments # a : Tensor(U, CPU(U)) - Tensor to sort block : Proc(U, U, _) - Proc to use to compare values Examples # a = [ 3 , 2 , 1 ]. to_tensor Num . sort ( a ) # => [1, 2, 3] View source #split ( a : Tensor ( U , CPU ( U )), ind : Int , axis : Int = 0 ) : Array ( Tensor ( U , CPU ( U ))) forall U # Split a Tensor into multiple sub- Tensor s. The number of sections must divide the Tensor equally. Arguments # a : Tensor(U, CPU(U)) - Tensor to split` ind : Int - Number of sections of resulting Array axis : Int - Axis along which to split Examples # a = Tensor . range ( 1 , 9 ) puts Num . array_split ( a , 2 ) # => [[1, 2, 3, 4], [5, 6, 7, 8]] View source #split ( a : Tensor ( U , CPU ( U )), ind : Array ( Int ), axis : Int = 0 ) : Array ( Tensor ( U , CPU ( U ))) forall U # Split a Tensor into multiple sub- Tensor s, using an explicit mapping of indices to split the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to split` ind : Int - Array of indices to use when splitting the Tensor axis : Int - Axis along which to split Examples # a = Tensor . range ( 9 ) puts Num . array_split ( a , [ 1 , 3 , 5 ] ) # => [[0], [1, 2], [3, 4], [5, 6, 7, 8]] View source #sqrt ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method sqrt on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sqrt ( a ) View source #sqrt ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function sqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sqrt ( a ) View source #sqrt! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method sqrt on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . sqrt ( a ) View source #sqrt! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function sqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . sqrt! ( a ) View source #std ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, finding the std of each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . std ( a , 0 ) # => [1, 1] Num . std ( a , 1 , dims : true ) # [[0.707107], # [0.707107]] View source #std ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by finding the standard deviation Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . std ( a ) # => 0.816496580927726 View source #subtract ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Tensor forall U , V # Implements the :- operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . subtract ( a , b ) View source #subtract ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Tensor forall U # Implements the :- operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . subtract ( a , b ) View source #subtract ( a : Number | Complex , b : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the :- operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor Arguments # a : Number | Complex - RHS to the operation b : Tensor(U, CPU(U)) - LHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . subtract ( b , a ) View source #subtract ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Subtract two Tensor s elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to subtract b : Tensor(U, OCL(U)) - RHS argument to subtract Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . subtract ( a , a ) View source #subtract ( a : Tensor ( U , OCL ( U )), b : U ) : Tensor ( U , OCL ( U )) forall U # Subtract a Tensor and a Number elementwise Arguments # a : Tensor(U, OCL(U)) - LHS argument to subtract b : U - RHS argument to subtract Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . subtract ( a , 3.5 ) View source #subtract ( a : U , b : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Subtract a Number and a Tensor elementwise Arguments # a : U - LHS argument to subtract b : Tensor(U, OCL(U)) - RHS argument to subtract Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . subtract ( a , 3.5 ) View source #subtract! ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V ))) : Nil forall U , V # Implements the :- operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Tensor(U, CPU(U)) - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = [ 4 , 5 , 6 ]. to_tensor Num . subtract! ( a , b ) # a is modified View source #subtract! ( a : Tensor ( U , CPU ( U )), b : Number | Complex ) : Nil forall U # Implements the :- operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace. Arguments # a : Tensor(U, CPU(U)) - LHS to the operation b : Number | Complex - RHS to the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor b = 4 Num . subtract! ( a , b ) View source #subtract! ( a : Tensor ( U , OCL ( U )), b : Tensor ( U , OCL ( U ))) : Nil forall U # \"Subtract\" two Tensor s elementwise, storing the result in the first Tensor Arguments # a : Tensor(U, OCL(U)) - LHS argument to subtract, will be modified inplace b : Tensor(U, OCL(U)) - RHS argument to subtract Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . subtract! ( a , a ) View source #subtract! ( a : Tensor ( U , OCL ( U )), b : U ) : Nil forall U # Subtract a Tensor and a Number elementwise, modifying the Tensor inplace. Arguments # a : Tensor(U, OCL(U)) - LHS argument to subtract b : U - RHS argument to subtract Examples # a = [ 1.5 , 2.2 , 3.2 ]. to_tensor ( OCL ) Num . subtract ( a , 3.5 ) View source #sum ( a : Tensor ( U , CPU ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, summing each view into the Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } Num . sum ( a , 0 ) # => [2, 4] Num . sum ( a , 1 , dims : true ) # [[1], # [5]] View source #sum ( a : Tensor ( U , OCL ( U )), axis : Int , dims : Bool = false ) forall U # Reduces a Tensor along an axis, summing each view into the Tensor Arguments # a : Tensor(U, OCL(U)) - Tensor to reduce axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } Num . sum ( a , 0 ) . cpu # => [2, 4] Num . sum ( a , 1 , dims : true ) . cpu # [[1], # [5]] View source #sum ( a : Tensor ( U , CPU ( U ))) forall U # Reduces a Tensor to a scalar by summing all of its elements Arguments # a : Tensor(U, CPU(U)) - Tensor to reduce Examples # a = [ 1 , 2 , 3 ] Num . sum ( a ) # => 6 View source #swap_axes ( arr : Tensor ( U , CPU ( U )), a : Int , b : Int ) forall U # Permutes two axes of a Tensor . This will always create a view of the permuted Tensor Arguments # arr : Tensor(U, CPU(U)) - Tensor to permute source : Int - First axis to swap destination : Int - Second axis to swap Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . swap_axes ( 2 , 0 ) # [[[ 0, 6, 12, 18] # [ 2, 8, 14, 20] # [ 4, 10, 16, 22]] # # [[ 1, 7, 13, 19] # [ 3, 9, 15, 21] # [ 5, 11, 17, 23]]] View source #tan ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method tan on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . tan ( a ) View source #tan ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function tan for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tan ( a ) View source #tan! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method tan on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . tan ( a ) View source #tan! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function tan for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tan! ( a ) View source #tanh ( a : Tensor ( U , CPU ( U ))) : Tensor forall U # Implements the stdlib Math method tanh on a Tensor , broadcasting the operation across all elements of the Tensor Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . tanh ( a ) View source #tanh ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function tanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tanh ( a ) View source #tanh! ( a : Tensor ( U , CPU ( U ))) : Nil forall U # Implements the stdlib Math method tanh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result Arguments # a : Tensor(U, CPU(U)) - Argument to be operated upon Examples # a = [ 2.0 , 3.65 , 3.141 ]. to_tensor Num . tanh ( a ) View source #tanh! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function tanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tanh! ( a ) View source #tanpi ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function tanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tanpi ( a ) View source #tanpi! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function tanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tanpi! ( a ) View source #tgamma ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function tgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tgamma ( a ) View source #tgamma! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function tgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . tgamma! ( a ) View source #tile ( a : Tensor ( U , CPU ( U )), n : Int ) forall U # Tile elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to repeat n : Int - Number of times to tile Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts Num . tile ( a , 2 ) # [[1, 2, 3, 1, 2, 3], # [4, 5, 6, 4, 5, 6]] View source #tile ( a : Tensor ( U , CPU ( U )), n : Array ( Int )) forall U # Tile elements of a Tensor Arguments # a : Tensor(U, CPU(U)) - Tensor to repeat n : Array(Int) - Number of times to repeat Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts Num . tile ( a , 2 ) # [[1, 2, 3, 1, 2, 3], # [4, 5, 6, 4, 5, 6]] View source #to_a ( arr : Tensor ( U , CPU ( U ))) forall U # Converts a Tensor to a standard library array. The returned array will always be one-dimensional to avoid return type ambiguity Arguments # arr : Tensor(U, CPU(U)) - Tensor to convert to an Array Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . to_a # => [0, 1, 2, 3] View source #to_a ( arr : Tensor ( U , OCL ( U ))) forall U # Converts a Tensor to a standard library array. The returned array will always be one-dimensional to avoid return type ambiguity Arguments # arr : Tensor(U, OCL(U)) - Tensor to convert to a stdlib array Examples # a = Tensor . new ( [ 2 , 2 ] , device : OCL ) { | i | i } a . to_a # => [0, 1, 2, 3] View source #transpose ( arr : Tensor ( U , CPU ( U )), axes : Array ( Int ) = [] of Int32 ) forall U # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # Arguments # arr : Tensor(U, CPU(U)) - Tensor to permute axes : Array(Int) - Order of axes to permute Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #transpose ( arr : Tensor ( U , OCL ( U )), axes : Array ( Int ) = [] of Int32 ) forall U # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # axes : Array(Int) New ordering of axes for the permuted Tensor . If empty, a full transpose will occur Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #transpose ( arr : Tensor ( U , CPU ( U )), * args : Int ) forall U # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # Arguments # arr : Tensor(U, CPU(U)) - Tensor to permute axes : Array(Int) - Order of axes to permute Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #transpose ( arr : Tensor ( U , OCL ( U )), * args : Int ) forall U # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # axes : Array(Int) New ordering of axes for the permuted Tensor . If empty, a full transpose will occur Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #trunc ( a : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Implements the OpenCL builtin function trunc for a single Tensor . Only Float32 and Float64 Tensor s are supported. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . trunc ( a ) View source #trunc! ( a : Tensor ( U , OCL ( U ))) : Nil forall U # Implements the OpenCL builtin function trunc for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place. Arguments # a : Tensor(U, OCL(U)) - Tensor on which to operate Examples # a = [ 0.45 , 0.3 , 2.4 ]. to_tensor ( OCL ) Num . trunc! ( a ) View source #view ( arr : Tensor ( U , CPU ( U )), dtype : V . class ) forall U , V # Return a shallow copy of a Tensor with a new dtype. The underlying data buffer is shared, but the Tensor owns its other attributes. The size of the new dtype must be a multiple of the current dtype Arguments # arr : Tensor(U, CPU(U)) - Tensor to view as a different data type u : V.class - The data type used to reintepret the underlying data buffer of a Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . view ( Int8 ) # => [0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0] View source #vstack ( arrs : Array ( Tensor ( U , V ))) forall U , V # Stack an array of Tensor s in sequence row-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3 Arguments # arrs : Array(Tensor) - Tensor s to concatenate Examples # a = [ 1 , 2 , 3 ]. to_tensor Num . vstack ( [ a , a ] ) # [[1, 2, 3], # [1, 2, 3]] View source #vstack ( * arrs : Tensor ( U , V )) forall U , V # Stack an array of Tensor s in sequence row-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3 Arguments # arrs : Tuple(Tensor) - Tensor s to concatenate Examples # a = [ 1 , 2 , 3 ]. to_tensor Num . vstack ( [ a , a ] ) # [[1, 2, 3], # [1, 2, 3]] View source #with_broadcast ( arr : Tensor ( U , V ), n : Int ) : Tensor ( U , V ) forall U , V # Expands a Tensor s dimensions n times by broadcasting the shape and strides. No data is copied, and the result is a read-only view of the original Tensor Arguments # arr : Tensor - Tensor to broadcast n : Int - Number of dimensions to broadcast Examples # a = [ 1 , 2 , 3 ]. to_tensor a . with_broadcast ( 2 ) # [[[1]], # # [[2]], # # [[3]]] View source #yield_along_axis ( a0 : Tensor ( U , CPU ( U )), axis : Int , & ) forall U # Similar to each_axis , but instead of yielding slices of an axis, it yields slices along an axis, useful for methods that require an entire view of an axis slice for a reduction operation, such as std , rather than being able to incrementally reduce. Arguments # a0 : Tensor(U, CPU(U)) - Tensor along which to iterate axis : Int - Axis of reduction Examples # a = Tensor . new ( [ 3 , 3 , 3 ] ) { | i | i } a . yield_along_axis ( 0 ) do | ax | puts ax end # [ 0, 9, 18] # [ 1, 10, 19] # [ 2, 11, 20] # [ 3, 12, 21] # [ 4, 13, 22] # [ 5, 14, 23] # [ 6, 15, 24] # [ 7, 16, 25] # [ 8, 17, 26] View source #zip ( a : Tensor ( U , CPU ( U )), b : Tensor ( V , CPU ( V )), & block : U , V -> _ ) forall U , V # Yields the elements of two Tensor s, always in RowMajor order, as if the Tensor s were flat. Arguments # a : Tensor(U, CPU(U)) - First Tensor of iteration b : Tensor(U, CPU(U)) - Second Tensor of iteration Examples # a = Tensor . new ( 2 , 2 ) { | i | i } b = Tensor . new ( 2 , 2 ) { | i | i + 2 } a . zip ( b ) do | el | puts el end # { 0, 2} # { 1, 3} # { 2, 4} # { 3, 5} View source","title":"Num"},{"location":"Num/#Num","text":"","title":"Num"},{"location":"Num/#Num-constants","text":"","title":"Constants"},{"location":"Num/#Num::ColMajor","text":"","title":"ColMajor"},{"location":"Num/#Num::RowMajor","text":"","title":"RowMajor"},{"location":"Num/#Num::VERSION","text":"","title":"VERSION"},{"location":"Num/#Num-methods","text":"","title":"Methods"},{"location":"Num/#Num#acos(a)","text":"Implements the stdlib Math method acos on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#acos"},{"location":"Num/#Num#acos(a)","text":"Implements the OpenCL builtin function acos for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#acos"},{"location":"Num/#Num#acos!(a)","text":"Implements the stdlib Math method acos on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#acos!"},{"location":"Num/#Num#acos!(a)","text":"Implements the OpenCL builtin function acos for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#acos!"},{"location":"Num/#Num#acosh(a)","text":"Implements the stdlib Math method acosh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#acosh"},{"location":"Num/#Num#acosh(a)","text":"Implements the OpenCL builtin function acosh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#acosh"},{"location":"Num/#Num#acosh!(a)","text":"Implements the stdlib Math method acosh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#acosh!"},{"location":"Num/#Num#acosh!(a)","text":"Implements the OpenCL builtin function acosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#acosh!"},{"location":"Num/#Num#acospi(a)","text":"Implements the OpenCL builtin function acospi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#acospi"},{"location":"Num/#Num#acospi!(a)","text":"Implements the OpenCL builtin function acospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#acospi!"},{"location":"Num/#Num#add(a,b)","text":"Implements the :+ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#add"},{"location":"Num/#Num#add(a,b)","text":"Implements the :+ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#add"},{"location":"Num/#Num#add(a,b)","text":"Implements the :+ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#add"},{"location":"Num/#Num#add(a,b)","text":"Add two Tensor s elementwise","title":"#add"},{"location":"Num/#Num#add(a,b)","text":"Add a Tensor and a Number elementwise","title":"#add"},{"location":"Num/#Num#add(a,b)","text":"Add a Number and a Tensor elementwise","title":"#add"},{"location":"Num/#Num#add!(a,b)","text":"Implements the :+ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#add!"},{"location":"Num/#Num#add!(a,b)","text":"Implements the :+ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#add!"},{"location":"Num/#Num#add!(a,b)","text":"\"Add\" two Tensor s elementwise, storing the result in the first Tensor","title":"#add!"},{"location":"Num/#Num#add!(a,b)","text":"Add a Tensor and a Number elementwise, modifying the Tensor inplace.","title":"#add!"},{"location":"Num/#Num#all(a,axis,dims)","text":"Reduces a Tensor along an axis, asserting the truthiness of all values in each view into the Tensor","title":"#all"},{"location":"Num/#Num#all_close(a,b,epsilon)","text":"Asserts that two Tensor s are equal, allowing for small margins of errors with floating point values using an EPSILON value.","title":"#all_close"},{"location":"Num/#Num#any(a,axis,dims)","text":"Reduces a Tensor along an axis, asserting the truthiness of any values in each view into the Tensor","title":"#any"},{"location":"Num/#Num#argmax(a,axis,dims)","text":"Find the maximum index value of a Tensor along an axis","title":"#argmax"},{"location":"Num/#Num#argmin(a,axis,dims)","text":"Find the minimum index value of a Tensor along an axis","title":"#argmin"},{"location":"Num/#Num#array_split(a,ind,axis)","text":"Split a Tensor into multiple sub- Tensor s","title":"#array_split"},{"location":"Num/#Num#array_split(a,ind,axis)","text":"Split a Tensor into multiple sub- Tensor s, using an explicit mapping of indices to split the Tensor","title":"#array_split"},{"location":"Num/#Num#as_strided(arr,shape,strides)","text":"as_strided creates a view into the Tensor given the exact strides and shape. This means it manipulates the internal data structure of a Tensor and, if done incorrectly, the array elements can point to invalid memory and can corrupt results or crash your program. It is advisable to always use the original strides when calculating new strides to avoid reliance on a contiguous memory layout. Furthermore, Tensor s created with this function often contain self overlapping memory, so that two elements are identical. Vectorized write operations on such Tensor s will typically be unpredictable. They may even give different results for small, large, or transposed Tensor s.","title":"#as_strided"},{"location":"Num/#Num#as_type(arr,dtype)","text":"Casts a Tensor to a new dtype, by making a copy. Information may be lost when converting between data types, for example Float to Int or Int to Bool.","title":"#as_type"},{"location":"Num/#Num#asin(a)","text":"Implements the stdlib Math method asin on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#asin"},{"location":"Num/#Num#asin(a)","text":"Implements the OpenCL builtin function asin for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#asin"},{"location":"Num/#Num#asin!(a)","text":"Implements the stdlib Math method asin on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#asin!"},{"location":"Num/#Num#asin!(a)","text":"Implements the OpenCL builtin function asin for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#asin!"},{"location":"Num/#Num#asinh(a)","text":"Implements the stdlib Math method asinh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#asinh"},{"location":"Num/#Num#asinh(a)","text":"Implements the OpenCL builtin function asinh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#asinh"},{"location":"Num/#Num#asinh!(a)","text":"Implements the stdlib Math method asinh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#asinh!"},{"location":"Num/#Num#asinh!(a)","text":"Implements the OpenCL builtin function asinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#asinh!"},{"location":"Num/#Num#asinpi(a)","text":"Implements the OpenCL builtin function asinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#asinpi"},{"location":"Num/#Num#asinpi!(a)","text":"Implements the OpenCL builtin function asinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#asinpi!"},{"location":"Num/#Num#atan(a)","text":"Implements the stdlib Math method atan on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#atan"},{"location":"Num/#Num#atan(a)","text":"Implements the OpenCL builtin function atan for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#atan"},{"location":"Num/#Num#atan!(a)","text":"Implements the stdlib Math method atan on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#atan!"},{"location":"Num/#Num#atan!(a)","text":"Implements the OpenCL builtin function atan for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#atan!"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the stdlib Math method atan2 on two Tensor s, broadcasting the Tensor s together.","title":"#atan2"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the stdlib Math method atan2 on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#atan2"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the stdlib Math method atan2 on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#atan2"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the OpenCL builtin function atan2 between two Tensor s","title":"#atan2"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the OpenCL builtin function atan2 between two a Tensor and a Number","title":"#atan2"},{"location":"Num/#Num#atan2(a,b)","text":"Implements the OpenCL builtin function atan2 between two a Tensor and a Number","title":"#atan2"},{"location":"Num/#Num#atan2!(a,b)","text":"Implements the stdlib Math method atan2 on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#atan2!"},{"location":"Num/#Num#atan2!(a,b)","text":"Implements the stdlib Math method atan2 on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#atan2!"},{"location":"Num/#Num#atan2!(a,b)","text":"Implements the OpenCL builtin function atan2 between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation","title":"#atan2!"},{"location":"Num/#Num#atanh(a)","text":"Implements the stdlib Math method atanh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#atanh"},{"location":"Num/#Num#atanh(a)","text":"Implements the OpenCL builtin function atanh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#atanh"},{"location":"Num/#Num#atanh!(a)","text":"Implements the stdlib Math method atanh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#atanh!"},{"location":"Num/#Num#atanh!(a)","text":"Implements the OpenCL builtin function atanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#atanh!"},{"location":"Num/#Num#atanpi(a)","text":"Implements the OpenCL builtin function atanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#atanpi"},{"location":"Num/#Num#atanpi!(a)","text":"Implements the OpenCL builtin function atanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#atanpi!"},{"location":"Num/#Num#besselj(a,b)","text":"Implements the stdlib Math method besselj on two Tensor s, broadcasting the Tensor s together.","title":"#besselj"},{"location":"Num/#Num#besselj(a,b)","text":"Implements the stdlib Math method besselj on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#besselj"},{"location":"Num/#Num#besselj(a,b)","text":"Implements the stdlib Math method besselj on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#besselj"},{"location":"Num/#Num#besselj!(a,b)","text":"Implements the stdlib Math method besselj on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#besselj!"},{"location":"Num/#Num#besselj!(a,b)","text":"Implements the stdlib Math method besselj on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#besselj!"},{"location":"Num/#Num#besselj0(a)","text":"Implements the stdlib Math method besselj0 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#besselj0"},{"location":"Num/#Num#besselj0!(a)","text":"Implements the stdlib Math method besselj0 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#besselj0!"},{"location":"Num/#Num#besselj1(a)","text":"Implements the stdlib Math method besselj1 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#besselj1"},{"location":"Num/#Num#besselj1!(a)","text":"Implements the stdlib Math method besselj1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#besselj1!"},{"location":"Num/#Num#bessely(a,b)","text":"Implements the stdlib Math method bessely on two Tensor s, broadcasting the Tensor s together.","title":"#bessely"},{"location":"Num/#Num#bessely(a,b)","text":"Implements the stdlib Math method bessely on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#bessely"},{"location":"Num/#Num#bessely(a,b)","text":"Implements the stdlib Math method bessely on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#bessely"},{"location":"Num/#Num#bessely!(a,b)","text":"Implements the stdlib Math method bessely on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#bessely!"},{"location":"Num/#Num#bessely!(a,b)","text":"Implements the stdlib Math method bessely on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#bessely!"},{"location":"Num/#Num#bessely0(a)","text":"Implements the stdlib Math method bessely0 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#bessely0"},{"location":"Num/#Num#bessely0!(a)","text":"Implements the stdlib Math method bessely0 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#bessely0!"},{"location":"Num/#Num#bessely1(a)","text":"Implements the stdlib Math method bessely1 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#bessely1"},{"location":"Num/#Num#bessely1!(a)","text":"Implements the stdlib Math method bessely1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#bessely1!"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the :& operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the :& operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the :& operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the bitwise operator \"&\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the bitwise operator \"&\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and(a,b)","text":"Implements the bitwise operator \"&\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_and"},{"location":"Num/#Num#bitwise_and!(a,b)","text":"Implements the :& operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#bitwise_and!"},{"location":"Num/#Num#bitwise_and!(a,b)","text":"Implements the :& operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#bitwise_and!"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the :| operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the :| operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the :| operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the bitwise operator \"|\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the bitwise operator \"|\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or(a,b)","text":"Implements the bitwise operator \"|\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_or"},{"location":"Num/#Num#bitwise_or!(a,b)","text":"Implements the :| operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#bitwise_or!"},{"location":"Num/#Num#bitwise_or!(a,b)","text":"Implements the :| operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#bitwise_or!"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the :^ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the :^ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the :^ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the bitwise operator \"^\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the bitwise operator \"^\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor(a,b)","text":"Implements the bitwise operator \"^\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#bitwise_xor"},{"location":"Num/#Num#bitwise_xor!(a,b)","text":"Implements the :^ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#bitwise_xor!"},{"location":"Num/#Num#bitwise_xor!(a,b)","text":"Implements the :^ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#bitwise_xor!"},{"location":"Num/#Num#broadcast(a,b,c)","text":"Broadcasts two Tensor 's' to a new shape. This allows for elementwise operations between the two Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error.","title":"#broadcast"},{"location":"Num/#Num#broadcast_to(arr,shape)","text":"Broadcasts a Tensor to a new shape. Returns a read-only view of the original Tensor . Many elements in the Tensor will refer to the same memory location, and the result is rarely contiguous. Shapes must be broadcastable, and an error will be raised if broadcasting fails.","title":"#broadcast_to"},{"location":"Num/#Num#cbrt(a)","text":"Implements the stdlib Math method cbrt on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#cbrt"},{"location":"Num/#Num#cbrt(a)","text":"Implements the OpenCL builtin function cbrt for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#cbrt"},{"location":"Num/#Num#cbrt!(a)","text":"Implements the stdlib Math method cbrt on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#cbrt!"},{"location":"Num/#Num#cbrt!(a)","text":"Implements the OpenCL builtin function cbrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#cbrt!"},{"location":"Num/#Num#ceil(a)","text":"Implements the OpenCL builtin function ceil for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#ceil"},{"location":"Num/#Num#ceil!(a)","text":"Implements the OpenCL builtin function ceil for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#ceil!"},{"location":"Num/#Num#concatenate(arrs,axis)","text":"Join a sequence of Tensor s along an existing axis. The Tensor s must have the same shape for all axes other than the axis of concatenation","title":"#concatenate"},{"location":"Num/#Num#copysign(a,b)","text":"Implements the stdlib Math method copysign on two Tensor s, broadcasting the Tensor s together.","title":"#copysign"},{"location":"Num/#Num#copysign(a,b)","text":"Implements the stdlib Math method copysign on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#copysign"},{"location":"Num/#Num#copysign(a,b)","text":"Implements the stdlib Math method copysign on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#copysign"},{"location":"Num/#Num#copysign!(a,b)","text":"Implements the stdlib Math method copysign on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#copysign!"},{"location":"Num/#Num#copysign!(a,b)","text":"Implements the stdlib Math method copysign on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#copysign!"},{"location":"Num/#Num#cos(a)","text":"Implements the stdlib Math method cos on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#cos"},{"location":"Num/#Num#cos(a)","text":"Implements the OpenCL builtin function cos for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#cos"},{"location":"Num/#Num#cos!(a)","text":"Implements the stdlib Math method cos on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#cos!"},{"location":"Num/#Num#cos!(a)","text":"Implements the OpenCL builtin function cos for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#cos!"},{"location":"Num/#Num#cosh(a)","text":"Implements the stdlib Math method cosh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#cosh"},{"location":"Num/#Num#cosh(a)","text":"Implements the OpenCL builtin function cosh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#cosh"},{"location":"Num/#Num#cosh!(a)","text":"Implements the stdlib Math method cosh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#cosh!"},{"location":"Num/#Num#cosh!(a)","text":"Implements the OpenCL builtin function cosh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#cosh!"},{"location":"Num/#Num#cospi(a)","text":"Implements the OpenCL builtin function cospi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#cospi"},{"location":"Num/#Num#cospi!(a)","text":"Implements the OpenCL builtin function cospi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#cospi!"},{"location":"Num/#Num#cpu(arr)","text":"Converts a CPU Tensor to CPU. Returns the input array, no copy is performed.","title":"#cpu"},{"location":"Num/#Num#cpu(arr)","text":"Converts a Tensor stored on an OpenCL device to a Tensor stored on a CPU.","title":"#cpu"},{"location":"Num/#Num#diagonal(arr)","text":"Returns a view of the diagonal of a Tensor . This method only works for two-dimensional arrays. Todo Implement views for offset diagonals","title":"#diagonal"},{"location":"Num/#Num#divide(a,b)","text":"Implements the :/ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#divide"},{"location":"Num/#Num#divide(a,b)","text":"Implements the :/ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#divide"},{"location":"Num/#Num#divide(a,b)","text":"Implements the :/ operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#divide"},{"location":"Num/#Num#divide(a,b)","text":"Divide two Tensor s elementwise","title":"#divide"},{"location":"Num/#Num#divide(a,b)","text":"Divide a Tensor and a Number elementwise","title":"#divide"},{"location":"Num/#Num#divide(a,b)","text":"Divide a Number and a Tensor elementwise","title":"#divide"},{"location":"Num/#Num#divide!(a,b)","text":"Implements the :/ operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#divide!"},{"location":"Num/#Num#divide!(a,b)","text":"Implements the :/ operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#divide!"},{"location":"Num/#Num#divide!(a,b)","text":"\"Divide\" two Tensor s elementwise, storing the result in the first Tensor","title":"#divide!"},{"location":"Num/#Num#divide!(a,b)","text":"Divide a Tensor and a Number elementwise, modifying the Tensor inplace.","title":"#divide!"},{"location":"Num/#Num#dup(t,order)","text":"Deep-copies a Tensor . If an order is provided, the returned Tensor 's memory layout will respect that order. If no order is provided, the Tensor will retain it's same memory layout.","title":"#dup"},{"location":"Num/#Num#each(arr,&)","text":"Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat.","title":"#each"},{"location":"Num/#Num#each_axis(a0,axis,dims,&)","text":"Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor","title":"#each_axis"},{"location":"Num/#Num#each_axis(a0,axis,dims,&)","text":"Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor","title":"#each_axis"},{"location":"Num/#Num#each_pointer(arr,&)","text":"Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor","title":"#each_pointer"},{"location":"Num/#Num#each_pointer_with_index(arr,&)","text":"Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor","title":"#each_pointer_with_index"},{"location":"Num/#Num#each_with_index(arr,&)","text":"Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Also yields the flat index of each element.","title":"#each_with_index"},{"location":"Num/#Num#equal(a,b)","text":"Implements the :== operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#equal"},{"location":"Num/#Num#equal(a,b)","text":"Implements the :== operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#equal"},{"location":"Num/#Num#equal(a,b)","text":"Implements the :== operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#equal"},{"location":"Num/#Num#equal(a,b)","text":"Implements the comparison operator \"==\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#equal"},{"location":"Num/#Num#equal(a,b)","text":"Implements the comparison operator \"==\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#equal"},{"location":"Num/#Num#equal(a,b)","text":"Implements the comparison operator \"==\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#equal"},{"location":"Num/#Num#equal!(a,b)","text":"Implements the :== operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#equal!"},{"location":"Num/#Num#equal!(a,b)","text":"Implements the :== operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#equal!"},{"location":"Num/#Num#erf(a)","text":"Implements the stdlib Math method erf on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#erf"},{"location":"Num/#Num#erf(a)","text":"Implements the OpenCL builtin function erf for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#erf"},{"location":"Num/#Num#erf!(a)","text":"Implements the stdlib Math method erf on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#erf!"},{"location":"Num/#Num#erf!(a)","text":"Implements the OpenCL builtin function erf for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#erf!"},{"location":"Num/#Num#erfc(a)","text":"Implements the stdlib Math method erfc on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#erfc"},{"location":"Num/#Num#erfc(a)","text":"Implements the OpenCL builtin function erfc for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#erfc"},{"location":"Num/#Num#erfc!(a)","text":"Implements the stdlib Math method erfc on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#erfc!"},{"location":"Num/#Num#erfc!(a)","text":"Implements the OpenCL builtin function erfc for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#erfc!"},{"location":"Num/#Num#exp(a)","text":"Implements the stdlib Math method exp on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#exp"},{"location":"Num/#Num#exp(a)","text":"Implements the OpenCL builtin function exp for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#exp"},{"location":"Num/#Num#exp!(a)","text":"Implements the stdlib Math method exp on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#exp!"},{"location":"Num/#Num#exp!(a)","text":"Implements the OpenCL builtin function exp for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#exp!"},{"location":"Num/#Num#exp10(a)","text":"Implements the OpenCL builtin function exp10 for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#exp10"},{"location":"Num/#Num#exp10!(a)","text":"Implements the OpenCL builtin function exp10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#exp10!"},{"location":"Num/#Num#exp2(a)","text":"Implements the stdlib Math method exp2 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#exp2"},{"location":"Num/#Num#exp2(a)","text":"Implements the OpenCL builtin function exp2 for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#exp2"},{"location":"Num/#Num#exp2!(a)","text":"Implements the stdlib Math method exp2 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#exp2!"},{"location":"Num/#Num#exp2!(a)","text":"Implements the OpenCL builtin function exp2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#exp2!"},{"location":"Num/#Num#expand_dims(arr,axis)","text":"Expands a Tensor along an axis","title":"#expand_dims"},{"location":"Num/#Num#expm1(a)","text":"Implements the stdlib Math method expm1 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#expm1"},{"location":"Num/#Num#expm1(a)","text":"Implements the OpenCL builtin function expm1 for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#expm1"},{"location":"Num/#Num#expm1!(a)","text":"Implements the stdlib Math method expm1 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#expm1!"},{"location":"Num/#Num#expm1!(a)","text":"Implements the OpenCL builtin function expm1 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#expm1!"},{"location":"Num/#Num#fabs(a)","text":"Implements the OpenCL builtin function fabs for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#fabs"},{"location":"Num/#Num#fabs!(a)","text":"Implements the OpenCL builtin function fabs for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#fabs!"},{"location":"Num/#Num#flat(arr)","text":"Flattens a Tensor to a single dimension. If a view can be created, the reshape operation will not copy data.","title":"#flat"},{"location":"Num/#Num#flip(a,axis)","text":"Flips a Tensor along an axis, returning a view","title":"#flip"},{"location":"Num/#Num#floor(a)","text":"Implements the OpenCL builtin function floor for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#floor"},{"location":"Num/#Num#floor!(a)","text":"Implements the OpenCL builtin function floor for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#floor!"},{"location":"Num/#Num#floordiv(a,b)","text":"Implements the :// operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#floordiv"},{"location":"Num/#Num#floordiv(a,b)","text":"Implements the :// operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#floordiv"},{"location":"Num/#Num#floordiv(a,b)","text":"Implements the :// operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#floordiv"},{"location":"Num/#Num#floordiv!(a,b)","text":"Implements the :// operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#floordiv!"},{"location":"Num/#Num#floordiv!(a,b)","text":"Implements the :// operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#floordiv!"},{"location":"Num/#Num#gamma(a)","text":"Implements the stdlib Math method gamma on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#gamma"},{"location":"Num/#Num#gamma!(a)","text":"Implements the stdlib Math method gamma on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#gamma!"},{"location":"Num/#Num#greater(a,b)","text":"Implements the :> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#greater"},{"location":"Num/#Num#greater(a,b)","text":"Implements the :> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#greater"},{"location":"Num/#Num#greater(a,b)","text":"Implements the :> operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#greater"},{"location":"Num/#Num#greater(a,b)","text":"Implements the comparison operator \">\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#greater"},{"location":"Num/#Num#greater(a,b)","text":"Implements the comparison operator \">\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#greater"},{"location":"Num/#Num#greater(a,b)","text":"Implements the comparison operator \">\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#greater"},{"location":"Num/#Num#greater!(a,b)","text":"Implements the :> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#greater!"},{"location":"Num/#Num#greater!(a,b)","text":"Implements the :> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#greater!"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the :>= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#greater_equal"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the :>= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#greater_equal"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the :>= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#greater_equal"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the comparison operator \">=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#greater_equal"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the comparison operator \">=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#greater_equal"},{"location":"Num/#Num#greater_equal(a,b)","text":"Implements the comparison operator \">=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#greater_equal"},{"location":"Num/#Num#greater_equal!(a,b)","text":"Implements the :>= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#greater_equal!"},{"location":"Num/#Num#greater_equal!(a,b)","text":"Implements the :>= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#greater_equal!"},{"location":"Num/#Num#hstack(arrs)","text":"Stack an array of Tensor s in sequence column-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3 For one dimensional Tensor s, this will still stack along the first axis","title":"#hstack"},{"location":"Num/#Num#hypot(a,b)","text":"Implements the stdlib Math method hypot on two Tensor s, broadcasting the Tensor s together.","title":"#hypot"},{"location":"Num/#Num#hypot(a,b)","text":"Implements the stdlib Math method hypot on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#hypot"},{"location":"Num/#Num#hypot(a,b)","text":"Implements the stdlib Math method hypot on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#hypot"},{"location":"Num/#Num#hypot!(a,b)","text":"Implements the stdlib Math method hypot on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#hypot!"},{"location":"Num/#Num#hypot!(a,b)","text":"Implements the stdlib Math method hypot on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#hypot!"},{"location":"Num/#Num#ilogb(a)","text":"Implements the stdlib Math method ilogb on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#ilogb"},{"location":"Num/#Num#ilogb!(a)","text":"Implements the stdlib Math method ilogb on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#ilogb!"},{"location":"Num/#Num#ldexp(a,b)","text":"Implements the stdlib Math method ldexp on two Tensor s, broadcasting the Tensor s together.","title":"#ldexp"},{"location":"Num/#Num#ldexp(a,b)","text":"Implements the stdlib Math method ldexp on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#ldexp"},{"location":"Num/#Num#ldexp(a,b)","text":"Implements the stdlib Math method ldexp on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#ldexp"},{"location":"Num/#Num#ldexp!(a,b)","text":"Implements the stdlib Math method ldexp on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#ldexp!"},{"location":"Num/#Num#ldexp!(a,b)","text":"Implements the stdlib Math method ldexp on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#ldexp!"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the :<< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#left_shift"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the :<< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#left_shift"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the :<< operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#left_shift"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the bitwise operator \"<<\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported","title":"#left_shift"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the bitwise operator \"<<\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#left_shift"},{"location":"Num/#Num#left_shift(a,b)","text":"Implements the bitwise operator \"<<\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#left_shift"},{"location":"Num/#Num#left_shift!(a,b)","text":"Implements the :<< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#left_shift!"},{"location":"Num/#Num#left_shift!(a,b)","text":"Implements the :<< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#left_shift!"},{"location":"Num/#Num#less(a,b)","text":"Implements the :< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#less"},{"location":"Num/#Num#less(a,b)","text":"Implements the :< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#less"},{"location":"Num/#Num#less(a,b)","text":"Implements the :< operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#less"},{"location":"Num/#Num#less(a,b)","text":"Implements the comparison operator \"<\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#less"},{"location":"Num/#Num#less(a,b)","text":"Implements the comparison operator \"<\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#less"},{"location":"Num/#Num#less(a,b)","text":"Implements the comparison operator \"<\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#less"},{"location":"Num/#Num#less!(a,b)","text":"Implements the :< operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#less!"},{"location":"Num/#Num#less!(a,b)","text":"Implements the :< operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#less!"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the :<= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#less_equal"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the :<= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#less_equal"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the :<= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#less_equal"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the comparison operator \"<=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#less_equal"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the comparison operator \"<=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#less_equal"},{"location":"Num/#Num#less_equal(a,b)","text":"Implements the comparison operator \"<=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#less_equal"},{"location":"Num/#Num#less_equal!(a,b)","text":"Implements the :<= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#less_equal!"},{"location":"Num/#Num#less_equal!(a,b)","text":"Implements the :<= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#less_equal!"},{"location":"Num/#Num#lgamma(a)","text":"Implements the stdlib Math method lgamma on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#lgamma"},{"location":"Num/#Num#lgamma(a)","text":"Implements the OpenCL builtin function lgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#lgamma"},{"location":"Num/#Num#lgamma!(a)","text":"Implements the stdlib Math method lgamma on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#lgamma!"},{"location":"Num/#Num#lgamma!(a)","text":"Implements the OpenCL builtin function lgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#lgamma!"},{"location":"Num/#Num#log(a)","text":"Implements the stdlib Math method log on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#log"},{"location":"Num/#Num#log(a)","text":"Implements the OpenCL builtin function log for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#log"},{"location":"Num/#Num#log!(a)","text":"Implements the stdlib Math method log on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#log!"},{"location":"Num/#Num#log!(a)","text":"Implements the OpenCL builtin function log for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#log!"},{"location":"Num/#Num#log10(a)","text":"Implements the stdlib Math method log10 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#log10"},{"location":"Num/#Num#log10(a)","text":"Implements the OpenCL builtin function log10 for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#log10"},{"location":"Num/#Num#log10!(a)","text":"Implements the stdlib Math method log10 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#log10!"},{"location":"Num/#Num#log10!(a)","text":"Implements the OpenCL builtin function log10 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#log10!"},{"location":"Num/#Num#log1p(a)","text":"Implements the stdlib Math method log1p on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#log1p"},{"location":"Num/#Num#log1p(a)","text":"Implements the OpenCL builtin function log1p for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#log1p"},{"location":"Num/#Num#log1p!(a)","text":"Implements the stdlib Math method log1p on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#log1p!"},{"location":"Num/#Num#log1p!(a)","text":"Implements the OpenCL builtin function log1p for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#log1p!"},{"location":"Num/#Num#log2(a)","text":"Implements the stdlib Math method log2 on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#log2"},{"location":"Num/#Num#log2(a)","text":"Implements the OpenCL builtin function log2 for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#log2"},{"location":"Num/#Num#log2!(a)","text":"Implements the stdlib Math method log2 on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#log2!"},{"location":"Num/#Num#log2!(a)","text":"Implements the OpenCL builtin function log2 for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#log2!"},{"location":"Num/#Num#logb(a)","text":"Implements the stdlib Math method logb on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#logb"},{"location":"Num/#Num#logb(a)","text":"Implements the OpenCL builtin function logb for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#logb"},{"location":"Num/#Num#logb!(a)","text":"Implements the stdlib Math method logb on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#logb!"},{"location":"Num/#Num#logb!(a)","text":"Implements the OpenCL builtin function logb for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#logb!"},{"location":"Num/#Num#map(a0,a1,a2,&)","text":"Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block","title":"#map"},{"location":"Num/#Num#map!(a0,a1,a2,&)","text":"Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor 's must broadcast to the shape of self","title":"#map!"},{"location":"Num/#Num#max(a,b)","text":"Implements the stdlib Math method max on two Tensor s, broadcasting the Tensor s together.","title":"#max"},{"location":"Num/#Num#max(a,b)","text":"Implements the stdlib Math method max on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#max"},{"location":"Num/#Num#max(a,b)","text":"Implements the stdlib Math method max on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#max"},{"location":"Num/#Num#max(a,b)","text":"Implements the OpenCL builtin function fmax between two Tensor s","title":"#max"},{"location":"Num/#Num#max(a,b)","text":"Implements the OpenCL builtin function fmax between two a Tensor and a Number","title":"#max"},{"location":"Num/#Num#max(a,b)","text":"Implements the OpenCL builtin function fmax between two a Tensor and a Number","title":"#max"},{"location":"Num/#Num#max!(a,b)","text":"Implements the stdlib Math method max on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#max!"},{"location":"Num/#Num#max!(a,b)","text":"Implements the stdlib Math method max on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#max!"},{"location":"Num/#Num#max!(a,b)","text":"Implements the OpenCL builtin function fmax between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation","title":"#max!"},{"location":"Num/#Num#mean(a,axis,dims)","text":"Reduces a Tensor along an axis, finding the average of each view into the Tensor","title":"#mean"},{"location":"Num/#Num#min(a,b)","text":"Implements the stdlib Math method min on two Tensor s, broadcasting the Tensor s together.","title":"#min"},{"location":"Num/#Num#min(a,b)","text":"Implements the stdlib Math method min on a Tensor and a Number, broadcasting the method across all elements of a Tensor","title":"#min"},{"location":"Num/#Num#min(a,b)","text":"Implements the stdlib Math method min on a Number and a Tensor , broadcasting the method across all elements of a Tensor","title":"#min"},{"location":"Num/#Num#min(a,b)","text":"Implements the OpenCL builtin function fmin between two Tensor s","title":"#min"},{"location":"Num/#Num#min(a,b)","text":"Implements the OpenCL builtin function fmin between two a Tensor and a Number","title":"#min"},{"location":"Num/#Num#min(a,b)","text":"Implements the OpenCL builtin function fmin between two a Tensor and a Number","title":"#min"},{"location":"Num/#Num#min!(a,b)","text":"Implements the stdlib Math method min on a Tensor , broadcasting the Tensor s together. The second Tensor must broadcast against the shape of the first, as the first Tensor is modified inplace.","title":"#min!"},{"location":"Num/#Num#min!(a,b)","text":"Implements the stdlib Math method min on a Tensor and a Number, broadcasting the method across all elements of a Tensor . The Tensor is modified inplace","title":"#min!"},{"location":"Num/#Num#min!(a,b)","text":"Implements the OpenCL builtin function fmin between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation","title":"#min!"},{"location":"Num/#Num#modulo(a,b)","text":"Implements the :% operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#modulo"},{"location":"Num/#Num#modulo(a,b)","text":"Implements the :% operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#modulo"},{"location":"Num/#Num#modulo(a,b)","text":"Implements the :% operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#modulo"},{"location":"Num/#Num#modulo!(a,b)","text":"Implements the :% operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#modulo!"},{"location":"Num/#Num#modulo!(a,b)","text":"Implements the :% operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#modulo!"},{"location":"Num/#Num#move_axis(arr,source,destination)","text":"Move axes of a Tensor to new positions, other axes remain in their original order","title":"#move_axis"},{"location":"Num/#Num#move_axis(arr,source,destination)","text":"Move axes of a Tensor to new positions, other axes remain in their original order","title":"#move_axis"},{"location":"Num/#Num#multiply(a,b)","text":"Implements the :* operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#multiply"},{"location":"Num/#Num#multiply(a,b)","text":"Implements the :* operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#multiply"},{"location":"Num/#Num#multiply(a,b)","text":"Implements the :* operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#multiply"},{"location":"Num/#Num#multiply(a,b)","text":"Multiply two Tensor s elementwise","title":"#multiply"},{"location":"Num/#Num#multiply(a,b)","text":"Multiply a Tensor and a Number elementwise","title":"#multiply"},{"location":"Num/#Num#multiply(a,b)","text":"Multiply a Number and a Tensor elementwise","title":"#multiply"},{"location":"Num/#Num#multiply!(a,b)","text":"Implements the :* operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#multiply!"},{"location":"Num/#Num#multiply!(a,b)","text":"Implements the :* operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#multiply!"},{"location":"Num/#Num#multiply!(a,b)","text":"\"Multiply\" two Tensor s elementwise, storing the result in the first Tensor","title":"#multiply!"},{"location":"Num/#Num#multiply!(a,b)","text":"Multiply a Tensor and a Number elementwise, modifying the Tensor inplace.","title":"#multiply!"},{"location":"Num/#Num#negate(a)","text":"Implements the negation operator on a Tensor","title":"#negate"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the :!= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#not_equal"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the :!= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#not_equal"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the :!= operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#not_equal"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the comparison operator \"!=\" between two Tensor s. The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32)) .","title":"#not_equal"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the comparison operator \"!=\" between a Tensor and a Number . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#not_equal"},{"location":"Num/#Num#not_equal(a,b)","text":"Implements the comparison operator \"!=\" between a Number and a Tensor . The returned result of OpenCL relational operators will always be Tensor(Int32, OCL(Int32))`.","title":"#not_equal"},{"location":"Num/#Num#not_equal!(a,b)","text":"Implements the :!= operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#not_equal!"},{"location":"Num/#Num#not_equal!(a,b)","text":"Implements the :!= operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#not_equal!"},{"location":"Num/#Num#opencl(arr)","text":"Places a Tensor stored on a CPU onto an OpenCL Device.","title":"#opencl"},{"location":"Num/#Num#power(a,b)","text":"Implements the :** operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#power"},{"location":"Num/#Num#power(a,b)","text":"Implements the :** operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#power"},{"location":"Num/#Num#power(a,b)","text":"Implements the :** operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#power"},{"location":"Num/#Num#power(a,b)","text":"Implements the OpenCL builtin function pow between two Tensor s","title":"#power"},{"location":"Num/#Num#power(a,b)","text":"Implements the OpenCL builtin function pow between two a Tensor and a Number","title":"#power"},{"location":"Num/#Num#power(a,b)","text":"Implements the OpenCL builtin function pow between two a Tensor and a Number","title":"#power"},{"location":"Num/#Num#power!(a,b)","text":"Implements the :** operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#power!"},{"location":"Num/#Num#power!(a,b)","text":"Implements the :** operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#power!"},{"location":"Num/#Num#power!(a,b)","text":"Implements the OpenCL builtin function pow between two Tensor s, mutating the first Tensor , modifying it to store the result of the operation","title":"#power!"},{"location":"Num/#Num#prod(a,axis,dims)","text":"Reduces a Tensor along an axis, multiplying each view into the Tensor","title":"#prod"},{"location":"Num/#Num#ptp(a,axis,dims)","text":"Finds the difference between the maximum and minimum elements of a Tensor along an axis","title":"#ptp"},{"location":"Num/#Num#reduce_axis(a0,axis,dims,&)","text":"Reduces a Tensor along an axis. Returns a Tensor , with the axis of reduction either removed, or reduced to 1 if dims is True, which allows the result to broadcast against its previous shape","title":"#reduce_axis"},{"location":"Num/#Num#reduce_axis(a0,axis,dims,&)","text":"Reduces a Tensor along an axis. Returns a Tensor , with the axis of reduction either removed, or reduced to 1 if dims is True, which allows the result to broadcast against its previous shape","title":"#reduce_axis"},{"location":"Num/#Num#repeat(a,n,axis)","text":"Repeat elements of a Tensor along an axis","title":"#repeat"},{"location":"Num/#Num#reshape(arr,shape)","text":"Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same.","title":"#reshape"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the :>> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#right_shift"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the :>> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#right_shift"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the :>> operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#right_shift"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the bitwise operator \">>\" between two Tensor s. Only Int32 and UInt32 Tensor s are supported","title":"#right_shift"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the bitwise operator \">>\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#right_shift"},{"location":"Num/#Num#right_shift(a,b)","text":"Implements the bitwise operator \">>\" between a Tensor and a Number Only Int32 and UInt32 Tensor s are supported","title":"#right_shift"},{"location":"Num/#Num#right_shift!(a,b)","text":"Implements the :>> operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#right_shift!"},{"location":"Num/#Num#right_shift!(a,b)","text":"Implements the :>> operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#right_shift!"},{"location":"Num/#Num#rint(a)","text":"Implements the OpenCL builtin function rint for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#rint"},{"location":"Num/#Num#rint!(a)","text":"Implements the OpenCL builtin function rint for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#rint!"},{"location":"Num/#Num#round(a)","text":"Implements the OpenCL builtin function round for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#round"},{"location":"Num/#Num#round!(a)","text":"Implements the OpenCL builtin function round for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#round!"},{"location":"Num/#Num#rsqrt(a)","text":"Implements the OpenCL builtin function rsqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#rsqrt"},{"location":"Num/#Num#rsqrt!(a)","text":"Implements the OpenCL builtin function rsqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#rsqrt!"},{"location":"Num/#Num#set(arr,args,t)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#set"},{"location":"Num/#Num#set(arr,args,t)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#set"},{"location":"Num/#Num#set(arr,args,t)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#set"},{"location":"Num/#Num#set(arr,args,t)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#set"},{"location":"Num/#Num#sin(a)","text":"Implements the stdlib Math method sin on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#sin"},{"location":"Num/#Num#sin(a)","text":"Implements the OpenCL builtin function sin for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#sin"},{"location":"Num/#Num#sin!(a)","text":"Implements the stdlib Math method sin on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#sin!"},{"location":"Num/#Num#sin!(a)","text":"Implements the OpenCL builtin function sin for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#sin!"},{"location":"Num/#Num#sinh(a)","text":"Implements the stdlib Math method sinh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#sinh"},{"location":"Num/#Num#sinh(a)","text":"Implements the OpenCL builtin function sinh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#sinh"},{"location":"Num/#Num#sinh!(a)","text":"Implements the stdlib Math method sinh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#sinh!"},{"location":"Num/#Num#sinh!(a)","text":"Implements the OpenCL builtin function sinh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#sinh!"},{"location":"Num/#Num#sinpi(a)","text":"Implements the OpenCL builtin function sinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#sinpi"},{"location":"Num/#Num#sinpi!(a)","text":"Implements the OpenCL builtin function sinpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#sinpi!"},{"location":"Num/#Num#slice(arr,args)","text":"Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source","title":"#slice"},{"location":"Num/#Num#sort(a,axis)","text":"Sorts a Tensor along an axis.","title":"#sort"},{"location":"Num/#Num#split(a,ind,axis)","text":"Split a Tensor into multiple sub- Tensor s. The number of sections must divide the Tensor equally.","title":"#split"},{"location":"Num/#Num#split(a,ind,axis)","text":"Split a Tensor into multiple sub- Tensor s, using an explicit mapping of indices to split the Tensor","title":"#split"},{"location":"Num/#Num#sqrt(a)","text":"Implements the stdlib Math method sqrt on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#sqrt"},{"location":"Num/#Num#sqrt(a)","text":"Implements the OpenCL builtin function sqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#sqrt"},{"location":"Num/#Num#sqrt!(a)","text":"Implements the stdlib Math method sqrt on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#sqrt!"},{"location":"Num/#Num#sqrt!(a)","text":"Implements the OpenCL builtin function sqrt for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#sqrt!"},{"location":"Num/#Num#std(a,axis,dims)","text":"Reduces a Tensor along an axis, finding the std of each view into the Tensor","title":"#std"},{"location":"Num/#Num#subtract(a,b)","text":"Implements the :- operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise.","title":"#subtract"},{"location":"Num/#Num#subtract(a,b)","text":"Implements the :- operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor","title":"#subtract"},{"location":"Num/#Num#subtract(a,b)","text":"Implements the :- operator between a scalar and Tensor . The scalar is broadcasted across all elements of the Tensor","title":"#subtract"},{"location":"Num/#Num#subtract(a,b)","text":"Subtract two Tensor s elementwise","title":"#subtract"},{"location":"Num/#Num#subtract(a,b)","text":"Subtract a Tensor and a Number elementwise","title":"#subtract"},{"location":"Num/#Num#subtract(a,b)","text":"Subtract a Number and a Tensor elementwise","title":"#subtract"},{"location":"Num/#Num#subtract!(a,b)","text":"Implements the :- operator between two Tensor s. Broadcasting rules apply, the method is applied elementwise. This method applies the operation inplace, storing the result in the LHS argument. Broadcasting cannot occur for the LHS operand, so the second argument must broadcast to the first operand's shape.","title":"#subtract!"},{"location":"Num/#Num#subtract!(a,b)","text":"Implements the :- operator between a Tensor and scalar. The scalar is broadcasted across all elements of the Tensor , and the Tensor is modified inplace.","title":"#subtract!"},{"location":"Num/#Num#subtract!(a,b)","text":"\"Subtract\" two Tensor s elementwise, storing the result in the first Tensor","title":"#subtract!"},{"location":"Num/#Num#subtract!(a,b)","text":"Subtract a Tensor and a Number elementwise, modifying the Tensor inplace.","title":"#subtract!"},{"location":"Num/#Num#sum(a,axis,dims)","text":"Reduces a Tensor along an axis, summing each view into the Tensor","title":"#sum"},{"location":"Num/#Num#sum(a,axis,dims)","text":"Reduces a Tensor along an axis, summing each view into the Tensor","title":"#sum"},{"location":"Num/#Num#swap_axes(arr,a,b)","text":"Permutes two axes of a Tensor . This will always create a view of the permuted Tensor","title":"#swap_axes"},{"location":"Num/#Num#tan(a)","text":"Implements the stdlib Math method tan on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#tan"},{"location":"Num/#Num#tan(a)","text":"Implements the OpenCL builtin function tan for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#tan"},{"location":"Num/#Num#tan!(a)","text":"Implements the stdlib Math method tan on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#tan!"},{"location":"Num/#Num#tan!(a)","text":"Implements the OpenCL builtin function tan for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#tan!"},{"location":"Num/#Num#tanh(a)","text":"Implements the stdlib Math method tanh on a Tensor , broadcasting the operation across all elements of the Tensor","title":"#tanh"},{"location":"Num/#Num#tanh(a)","text":"Implements the OpenCL builtin function tanh for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#tanh"},{"location":"Num/#Num#tanh!(a)","text":"Implements the stdlib Math method tanh on a Tensor , broadcasting the operation across all elements of the Tensor . The Tensor is modified inplace to store the result","title":"#tanh!"},{"location":"Num/#Num#tanh!(a)","text":"Implements the OpenCL builtin function tanh for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#tanh!"},{"location":"Num/#Num#tanpi(a)","text":"Implements the OpenCL builtin function tanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#tanpi"},{"location":"Num/#Num#tanpi!(a)","text":"Implements the OpenCL builtin function tanpi for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#tanpi!"},{"location":"Num/#Num#tgamma(a)","text":"Implements the OpenCL builtin function tgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#tgamma"},{"location":"Num/#Num#tgamma!(a)","text":"Implements the OpenCL builtin function tgamma for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#tgamma!"},{"location":"Num/#Num#tile(a,n)","text":"Tile elements of a Tensor","title":"#tile"},{"location":"Num/#Num#tile(a,n)","text":"Tile elements of a Tensor","title":"#tile"},{"location":"Num/#Num#to_a(arr)","text":"Converts a Tensor to a standard library array. The returned array will always be one-dimensional to avoid return type ambiguity","title":"#to_a"},{"location":"Num/#Num#to_a(arr)","text":"Converts a Tensor to a standard library array. The returned array will always be one-dimensional to avoid return type ambiguity","title":"#to_a"},{"location":"Num/#Num#transpose(arr,axes)","text":"Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor .","title":"#transpose"},{"location":"Num/#Num#transpose(arr,axes)","text":"Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor .","title":"#transpose"},{"location":"Num/#Num#trunc(a)","text":"Implements the OpenCL builtin function trunc for a single Tensor . Only Float32 and Float64 Tensor s are supported.","title":"#trunc"},{"location":"Num/#Num#trunc!(a)","text":"Implements the OpenCL builtin function trunc for a single Tensor . Only Float32 and Float64 Tensor s are supported. This method mutates the original Tensor , modifying it in place.","title":"#trunc!"},{"location":"Num/#Num#view(arr,dtype)","text":"Return a shallow copy of a Tensor with a new dtype. The underlying data buffer is shared, but the Tensor owns its other attributes. The size of the new dtype must be a multiple of the current dtype","title":"#view"},{"location":"Num/#Num#vstack(arrs)","text":"Stack an array of Tensor s in sequence row-wise. While this method can take Tensor s with any number of dimensions, it makes the most sense with rank <= 3","title":"#vstack"},{"location":"Num/#Num#with_broadcast(arr,n)","text":"Expands a Tensor s dimensions n times by broadcasting the shape and strides. No data is copied, and the result is a read-only view of the original Tensor","title":"#with_broadcast"},{"location":"Num/#Num#yield_along_axis(a0,axis,&)","text":"Similar to each_axis , but instead of yielding slices of an axis, it yields slices along an axis, useful for methods that require an entire view of an axis slice for a reduction operation, such as std , rather than being able to incrementally reduce.","title":"#yield_along_axis"},{"location":"Num/#Num#zip(a,b,&)","text":"Yields the elements of two Tensor s, always in RowMajor order, as if the Tensor s were flat.","title":"#zip"},{"location":"Num/ArrayFlags/","text":"enum Num::ArrayFlags # Members # Contiguous = 1 # Contiguous really means C-style contiguious. The contiguous part means that there are no 'skipped elements'. That is, that a flat_iter over the array will touch every location in memory from the location of the first element to that of the last element. The C-style part means that the data is laid out such that the last index is the fastest varying as one scans though the array's memory. Fortran = 2 # Fortran really means Fortran-style contiguious. The contiguous part means that there are no 'skipped elements'. That is, that a flat_iter over the array will touch every location in memory from the location of the first element to that of the last element. The Fortran-style part means that the data is laid out such that the first index is the fastest varying as one scans though the array's memory. OwnData = 4 # OwnData indicates if this array is the owner of the data pointed to by its .ptr property. If not then this is a view onto some other array's data. Write = 8 # Some views into arrays are created using stride tricks and aren't safe to write to, since many locations may be sharing the same memory Methods # #contiguous? # View source #fortran? # View source #none? # View source #own_data? # View source #write? # View source","title":"ArrayFlags"},{"location":"Num/ArrayFlags/#Num::ArrayFlags","text":"","title":"ArrayFlags"},{"location":"Num/ArrayFlags/#Num::ArrayFlags-members","text":"","title":"Members"},{"location":"Num/ArrayFlags/#Num::ArrayFlags::Contiguous","text":"Contiguous really means C-style contiguious. The contiguous part means that there are no 'skipped elements'. That is, that a flat_iter over the array will touch every location in memory from the location of the first element to that of the last element. The C-style part means that the data is laid out such that the last index is the fastest varying as one scans though the array's memory.","title":"Contiguous"},{"location":"Num/ArrayFlags/#Num::ArrayFlags::Fortran","text":"Fortran really means Fortran-style contiguious. The contiguous part means that there are no 'skipped elements'. That is, that a flat_iter over the array will touch every location in memory from the location of the first element to that of the last element. The Fortran-style part means that the data is laid out such that the first index is the fastest varying as one scans though the array's memory.","title":"Fortran"},{"location":"Num/ArrayFlags/#Num::ArrayFlags::OwnData","text":"OwnData indicates if this array is the owner of the data pointed to by its .ptr property. If not then this is a view onto some other array's data.","title":"OwnData"},{"location":"Num/ArrayFlags/#Num::ArrayFlags::Write","text":"Some views into arrays are created using stride tricks and aren't safe to write to, since many locations may be sharing the same memory","title":"Write"},{"location":"Num/ArrayFlags/#Num::ArrayFlags-methods","text":"","title":"Methods"},{"location":"Num/ArrayFlags/#Num::ArrayFlags#contiguous?","text":"View source","title":"#contiguous?"},{"location":"Num/ArrayFlags/#Num::ArrayFlags#fortran?","text":"View source","title":"#fortran?"},{"location":"Num/ArrayFlags/#Num::ArrayFlags#none?","text":"View source","title":"#none?"},{"location":"Num/ArrayFlags/#Num::ArrayFlags#own_data?","text":"View source","title":"#own_data?"},{"location":"Num/ArrayFlags/#Num::ArrayFlags#write?","text":"View source","title":"#write?"},{"location":"Num/Einsum/","text":"module Num::Einsum # Extended modules Num::Einsum Methods # #einsum ( input_string : String , operands : Array ( Tensor ( U , CPU ( U )))) forall U # Evaluates the Einstein summation convention on the operands. The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. einsum provides a succinct way of representing these. A non-exhaustive list of these operations, which can be computed by einsum, is shown below: Trace of an array Return a diagonal Array axis summations Transpositions and permutations Matrix multiplication and dot product Vector inner and outer products Broadcasting , element - wise and scalar multiplication Tensor contractions The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so Num::Einsum.einsum(\"i,i\", a, b) is equivalent to an inner operation. If a label appears only once, it is not summed, so Num::Einsum.einsum(\"i\", a) produces a view of a with no changes. A further example Num::Einsum.einsum(\"ij,jk\", a, b) describes traditional matrix multiplication and is equivalent to a.matmul(b). Repeated subscript labels in one operand take the diagonal. For example, Num::Einsum.einsum(\"ii\", a) gets the trace of a matrix View source #einsum ( input_string : String , * operands : Tensor ( U , CPU ( U ))) forall U # Evaluates the Einstein summation convention on the operands. The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. einsum provides a succinct way of representing these. A non-exhaustive list of these operations, which can be computed by einsum, is shown below: Trace of an array Return a diagonal Array axis summations Transpositions and permutations Matrix multiplication and dot product Vector inner and outer products Broadcasting , element - wise and scalar multiplication Tensor contractions The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so Num::Einsum.einsum(\"i,i\", a, b) is equivalent to an inner operation. If a label appears only once, it is not summed, so Num::Einsum.einsum(\"i\", a) produces a view of a with no changes. A further example Num::Einsum.einsum(\"ij,jk\", a, b) describes traditional matrix multiplication and is equivalent to a.matmul(b). Repeated subscript labels in one operand take the diagonal. For example, Num::Einsum.einsum(\"ii\", a) gets the trace of a matrix View source","title":"Einsum"},{"location":"Num/Einsum/#Num::Einsum","text":"","title":"Einsum"},{"location":"Num/Einsum/#Num::Einsum-methods","text":"","title":"Methods"},{"location":"Num/Einsum/#Num::Einsum#einsum(input_string,operands)","text":"Evaluates the Einstein summation convention on the operands. The Einstein summation convention can be used to compute many multi-dimensional, linear algebraic array operations. einsum provides a succinct way of representing these. A non-exhaustive list of these operations, which can be computed by einsum, is shown below: Trace of an array Return a diagonal Array axis summations Transpositions and permutations Matrix multiplication and dot product Vector inner and outer products Broadcasting , element - wise and scalar multiplication Tensor contractions The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so Num::Einsum.einsum(\"i,i\", a, b) is equivalent to an inner operation. If a label appears only once, it is not summed, so Num::Einsum.einsum(\"i\", a) produces a view of a with no changes. A further example Num::Einsum.einsum(\"ij,jk\", a, b) describes traditional matrix multiplication and is equivalent to a.matmul(b). Repeated subscript labels in one operand take the diagonal. For example, Num::Einsum.einsum(\"ii\", a) gets the trace of a matrix View source","title":"#einsum"},{"location":"Num/Exceptions/","text":"module Num::Exceptions #","title":"Exceptions"},{"location":"Num/Exceptions/#Num::Exceptions","text":"","title":"Exceptions"},{"location":"Num/Exceptions/AxisError/","text":"class Num::Exceptions::AxisError inherits Exception # This is raised whenever an axis parameter is specified that is larger than the number of array dimensions.","title":"AxisError"},{"location":"Num/Exceptions/AxisError/#Num::Exceptions::AxisError","text":"This is raised whenever an axis parameter is specified that is larger than the number of array dimensions.","title":"AxisError"},{"location":"Num/Exceptions/IndexError/","text":"class Num::Exceptions::IndexError inherits Exception # Raised when a sequence subscript is out of range. (Slice indices are silently truncated to fall in the allowed range; if an index is not an integer, TypeCastError is raised.)","title":"IndexError"},{"location":"Num/Exceptions/IndexError/#Num::Exceptions::IndexError","text":"Raised when a sequence subscript is out of range. (Slice indices are silently truncated to fall in the allowed range; if an index is not an integer, TypeCastError is raised.)","title":"IndexError"},{"location":"Num/Exceptions/LinAlgError/","text":"class Num::Exceptions::LinAlgError inherits Exception # Generic exception raised by linalg functions. General purpose exception class, programmatically raised in linalg functions when a Linear Algebra-related condition would prevent further correct execution of the function.","title":"LinAlgError"},{"location":"Num/Exceptions/LinAlgError/#Num::Exceptions::LinAlgError","text":"Generic exception raised by linalg functions. General purpose exception class, programmatically raised in linalg functions when a Linear Algebra-related condition would prevent further correct execution of the function.","title":"LinAlgError"},{"location":"Num/Exceptions/ValueError/","text":"class Num::Exceptions::ValueError inherits Exception # Raised when an operation or function receives an argument that has the right type but an inappropriate value, and the situation is not described by a more precise exception such as IndexError.","title":"ValueError"},{"location":"Num/Exceptions/ValueError/#Num::Exceptions::ValueError","text":"Raised when an operation or function receives an argument that has the right type but an inappropriate value, and the situation is not described by a more precise exception such as IndexError.","title":"ValueError"},{"location":"Num/Float32AddTensorScalarInplace/","text":"class Num::Float32AddTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float32AddTensorScalarInplace"},{"location":"Num/Float32AddTensorScalarInplace/#Num::Float32AddTensorScalarInplace","text":"","title":"Float32AddTensorScalarInplace"},{"location":"Num/Float32AddTensorScalarInplace/#Num::Float32AddTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float32AddTensorScalarInplace/#Num::Float32AddTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float32AddTensorScalarInplace/#Num::Float32AddTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float32DivideTensorScalarInplace/","text":"class Num::Float32DivideTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float32DivideTensorScalarInplace"},{"location":"Num/Float32DivideTensorScalarInplace/#Num::Float32DivideTensorScalarInplace","text":"","title":"Float32DivideTensorScalarInplace"},{"location":"Num/Float32DivideTensorScalarInplace/#Num::Float32DivideTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float32DivideTensorScalarInplace/#Num::Float32DivideTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float32DivideTensorScalarInplace/#Num::Float32DivideTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float32MultiplyTensorScalarInplace/","text":"class Num::Float32MultiplyTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float32MultiplyTensorScalarInplace"},{"location":"Num/Float32MultiplyTensorScalarInplace/#Num::Float32MultiplyTensorScalarInplace","text":"","title":"Float32MultiplyTensorScalarInplace"},{"location":"Num/Float32MultiplyTensorScalarInplace/#Num::Float32MultiplyTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float32MultiplyTensorScalarInplace/#Num::Float32MultiplyTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float32MultiplyTensorScalarInplace/#Num::Float32MultiplyTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float32SubtractTensorScalarInplace/","text":"class Num::Float32SubtractTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float32SubtractTensorScalarInplace"},{"location":"Num/Float32SubtractTensorScalarInplace/#Num::Float32SubtractTensorScalarInplace","text":"","title":"Float32SubtractTensorScalarInplace"},{"location":"Num/Float32SubtractTensorScalarInplace/#Num::Float32SubtractTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float32SubtractTensorScalarInplace/#Num::Float32SubtractTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float32SubtractTensorScalarInplace/#Num::Float32SubtractTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float64AddTensorScalarInplace/","text":"class Num::Float64AddTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float64AddTensorScalarInplace"},{"location":"Num/Float64AddTensorScalarInplace/#Num::Float64AddTensorScalarInplace","text":"","title":"Float64AddTensorScalarInplace"},{"location":"Num/Float64AddTensorScalarInplace/#Num::Float64AddTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float64AddTensorScalarInplace/#Num::Float64AddTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float64AddTensorScalarInplace/#Num::Float64AddTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float64DivideTensorScalarInplace/","text":"class Num::Float64DivideTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float64DivideTensorScalarInplace"},{"location":"Num/Float64DivideTensorScalarInplace/#Num::Float64DivideTensorScalarInplace","text":"","title":"Float64DivideTensorScalarInplace"},{"location":"Num/Float64DivideTensorScalarInplace/#Num::Float64DivideTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float64DivideTensorScalarInplace/#Num::Float64DivideTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float64DivideTensorScalarInplace/#Num::Float64DivideTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float64MultiplyTensorScalarInplace/","text":"class Num::Float64MultiplyTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float64MultiplyTensorScalarInplace"},{"location":"Num/Float64MultiplyTensorScalarInplace/#Num::Float64MultiplyTensorScalarInplace","text":"","title":"Float64MultiplyTensorScalarInplace"},{"location":"Num/Float64MultiplyTensorScalarInplace/#Num::Float64MultiplyTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float64MultiplyTensorScalarInplace/#Num::Float64MultiplyTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float64MultiplyTensorScalarInplace/#Num::Float64MultiplyTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Float64SubtractTensorScalarInplace/","text":"class Num::Float64SubtractTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Float64SubtractTensorScalarInplace"},{"location":"Num/Float64SubtractTensorScalarInplace/#Num::Float64SubtractTensorScalarInplace","text":"","title":"Float64SubtractTensorScalarInplace"},{"location":"Num/Float64SubtractTensorScalarInplace/#Num::Float64SubtractTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Float64SubtractTensorScalarInplace/#Num::Float64SubtractTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Float64SubtractTensorScalarInplace/#Num::Float64SubtractTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Grad/","text":"module Num::Grad # Extended modules Num::Grad Methods # #register ( name : String , gate : Num :: Grad :: Gate ( U ), result : Num::Grad::Variable ( U ), * parents : Num::Grad::Variable ( U )) forall U # Cached a node in the computational graph. This is only required if an operation needs to be backpropogated. Arguments # name : String - Description of the operation gate : Gate - Operation gate containing a backward method and cached arguments result : Variable - The result of the operation being cached parents : Variable - The operands present in operation being cached This method should be used sparingly by Users of the application. It should only be necessary when a User is defining their own custom activation function or Layer. View source","title":"Grad"},{"location":"Num/Grad/#Num::Grad","text":"","title":"Grad"},{"location":"Num/Grad/#Num::Grad-methods","text":"","title":"Methods"},{"location":"Num/Grad/#Num::Grad#register(name,gate,result,*)","text":"Cached a node in the computational graph. This is only required if an operation needs to be backpropogated.","title":"#register"},{"location":"Num/Grad/Context/","text":"class Num::Grad::Context(T) inherits Reference # Constructors # .new # Contexts can only be initialized as empty, and a generic type must be provided View source Methods # #last : Num :: Grad :: Node ( T ) # View source #no_grad : Bool # If no_grad is set to true, operations will not be cached, and backpropogation will not be possible View source #nodes : Array ( Num :: Grad :: Node ( T )) # A list of all variables present in an operation. This list can contain duplicates View source #variable ( value : Number , requires_grad : Bool = true ) : Num::Grad::Variable ( T ) # Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T . Arguments # value : Number - Number to be monitored requires_grad : Bool - Flag to indicate if operations should be cached for this variable Examples # ctx = Context ( Tensor ( Float64 )) . new ctx . variable ( 3.0 ) View source #variable ( value : Array , requires_grad : Bool = true ) : Num::Grad::Variable ( T ) # Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T . Arguments # value : Array - Array to be monitored requires_grad : Bool - Flag to indicate if operations should be cached for this variable Examples # ctx = Context ( Tensor ( Float64 )) . new ctx . variable ( [ 1.0 , 2.0 , 3.0 ] ) View source #variable ( value : T , requires_grad : Bool = true ) : Num::Grad::Variable ( T ) # Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T . Arguments # value : Tensor - Tensor to be monitored requires_grad : Bool - Flag to indicate if operations should be cached for this variable Examples # ctx = Context ( Tensor ( Float64 )) . new ctx . variable ( [ 1.0 , 2.0 , 3.0 ]. to_tensor ) View source","title":"Context"},{"location":"Num/Grad/Context/#Num::Grad::Context","text":"","title":"Context"},{"location":"Num/Grad/Context/#Num::Grad::Context-constructors","text":"","title":"Constructors"},{"location":"Num/Grad/Context/#Num::Grad::Context.new","text":"Contexts can only be initialized as empty, and a generic type must be provided View source","title":".new"},{"location":"Num/Grad/Context/#Num::Grad::Context-methods","text":"","title":"Methods"},{"location":"Num/Grad/Context/#Num::Grad::Context#last","text":"View source","title":"#last"},{"location":"Num/Grad/Context/#Num::Grad::Context#no_grad","text":"If no_grad is set to true, operations will not be cached, and backpropogation will not be possible View source","title":"#no_grad"},{"location":"Num/Grad/Context/#Num::Grad::Context#nodes","text":"A list of all variables present in an operation. This list can contain duplicates View source","title":"#nodes"},{"location":"Num/Grad/Context/#Num::Grad::Context#variable(value,requires_grad)","text":"Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T .","title":"#variable"},{"location":"Num/Grad/Context/#Num::Grad::Context#variable(value,requires_grad)","text":"Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T .","title":"#variable"},{"location":"Num/Grad/Context/#Num::Grad::Context#variable(value,requires_grad)","text":"Creates a new variable within the Context . This variable must be able to be cast to a Tensor of type T .","title":"#variable"},{"location":"Num/Grad/Variable/","text":"class Num::Grad::Variable(T) inherits Reference # A variable is an abstraction of a Tensor that tracks the operations done to the Tensor. It also keeps track of the gradient of the operation if a Variable needs to backpropogate. This is the fundamental object used in automatic differentiation, as well as the neural network aspects of Num.cr Constructors # .new ( context : Num::Grad::Context ( T ), value : T , requires_grad : Bool = false ) # Initialization method for a Variable. This method should only be called by a context, as it creates a Variable. Context provides a helper method to add a Variable to the computational graph that handles ownership of the context and other related instance variables View source Methods # #* ( other : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Multiples a variable to another variable and stores the derivative of the operation in the computational graph. Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [ 2.0 ] ) b = ctx . variable ( [ 3.0 ] ) f = a * b # => [6.0] f . backprop View source #** ( other : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Raises a variable to another variable and stores the derivative of the operation in the computational graph. Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [ 2.0 ] ) b = ctx . variable ( [ 3.0 ] ) f = a ** b # => [8.0] f . backprop View source #+ ( other : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Adds a variable to another variable and stores the derivative of the operation in the computational graph. Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [ 2.0 ] ) b = ctx . variable ( [ 3.0 ] ) f = a + b # => [5.0] f . backprop View source #- ( other : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Subtracts a variable from another variable and stores the derivative of the operation in the computational graph. Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [ 2.0 ] ) b = ctx . variable ( [ 3.0 ] ) f = a - b # => [-1.0] f . backprop View source #/ ( other : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Divides a variable by another variable and stores the derivative of the operation in the computational graph. Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [ 2.0 ] ) b = ctx . variable ( [ 3.0 ] ) f = a / b # => [0.66667] f . backprop View source #[] ( * args ) # Slices a variable. Slices the gradient of the variable using the same arguments Arguments # args - Slicing arguments, slicing behavior is the same as it is for a standard Tensor Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [[ 2.0 ] , [ 3.0 ]] ) b = a [ 1 ] b # => [3] View source #acos : Num::Grad::Variable ( T ) # Computes the arccosine of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . acos # => [0] View source #asin : Num::Grad::Variable ( T ) # Computes the arcsine of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . asin # => [1.5708] View source #atan : Num::Grad::Variable ( T ) # Computes the arctangent of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . atan # => [0.785398] View source #backprop ( debug : Bool = false ) # Back propogates an operation along a computational graph. This operation will destroy the operational graph, populating the gradients for all variables that are predecessors of the Variable this is called on. Even if this is called on the first node in a graph, it will destroy all descendents of this variable stored by the Context View source #context : Num::Grad::Context ( T ) # The graph the variable is associated with. This is a reference, as a variable does not own its context View source #cos : Num::Grad::Variable ( T ) # Computes the cosine of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . cos # => [0.540302] View source #elu ( alpha = 0.01 ) # Exponential Linear Unit activation function Arguments # alpha : Float - Scale for the negative factor View source #exp : Num::Grad::Variable ( T ) # Computes the exp of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . exp # => [2.71828] View source #grad : T # The gradient of the Variable. This is set as a reference to the value of a Variable unless backprop has been called, in which case all related Variables will have their gradient updated correctly View source #grad= ( grad : T ) # The gradient of the Variable. This is set as a reference to the value of a Variable unless backprop has been called, in which case all related Variables will have their gradient updated correctly View source #leaky_relu # View source #matmul ( b : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Matrix multiply operator for two variables. Computes the dot product of two matrices and stores the result in the computational graph Arguments # other : Num::Grad::Variable - right hand side of the operation Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 )) . new a = ctx . variable ( [[ 2.0 ] , [ 2.0 ]] ) b = ctx . variable ( [[ 3.0 , 3.0 ]] ) f = a . matmul ( b ) # [[6, 6], # [6, 6]] f . backprop View source #relu # View source #requires_grad : Bool # If set to true, this variable will track its operations, otherwise it will act similar to a Tensor, only calculating forward operations View source #requires_grad= ( requires_grad : Bool ) # If set to true, this variable will track its operations, otherwise it will act similar to a Tensor, only calculating forward operations View source #sin : Num::Grad::Variable ( T ) # Computes the sine of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . sin # => [0.841471] View source #tan : Num::Grad::Variable ( T ) # Computes the tangent of a variable Examples # ctx = Num :: Grad :: Context ( Tensor ( Float64 , CPU ( Float64 ))) . new x = ctx . variable ( [ 1.0 ] ) x . tan # => [1.55741] View source #value : T # The value of the Variable. This should not be edited outside of Variable operations, as other edits will not be tracked and will lead to incorrect results View source","title":"Variable"},{"location":"Num/Grad/Variable/#Num::Grad::Variable","text":"A variable is an abstraction of a Tensor that tracks the operations done to the Tensor. It also keeps track of the gradient of the operation if a Variable needs to backpropogate. This is the fundamental object used in automatic differentiation, as well as the neural network aspects of Num.cr","title":"Variable"},{"location":"Num/Grad/Variable/#Num::Grad::Variable-constructors","text":"","title":"Constructors"},{"location":"Num/Grad/Variable/#Num::Grad::Variable.new(context,value,requires_grad)","text":"Initialization method for a Variable. This method should only be called by a context, as it creates a Variable. Context provides a helper method to add a Variable to the computational graph that handles ownership of the context and other related instance variables View source","title":".new"},{"location":"Num/Grad/Variable/#Num::Grad::Variable-methods","text":"","title":"Methods"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#*(other)","text":"Multiples a variable to another variable and stores the derivative of the operation in the computational graph.","title":"#*"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#**(other)","text":"Raises a variable to another variable and stores the derivative of the operation in the computational graph.","title":"#**"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#+(other)","text":"Adds a variable to another variable and stores the derivative of the operation in the computational graph.","title":"#+"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#-(other)","text":"Subtracts a variable from another variable and stores the derivative of the operation in the computational graph.","title":"#-"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#/(other)","text":"Divides a variable by another variable and stores the derivative of the operation in the computational graph.","title":"#/"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#[](*)","text":"Slices a variable. Slices the gradient of the variable using the same arguments","title":"#[]"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#acos","text":"Computes the arccosine of a variable","title":"#acos"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#asin","text":"Computes the arcsine of a variable","title":"#asin"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#atan","text":"Computes the arctangent of a variable","title":"#atan"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#backprop(debug)","text":"Back propogates an operation along a computational graph. This operation will destroy the operational graph, populating the gradients for all variables that are predecessors of the Variable this is called on. Even if this is called on the first node in a graph, it will destroy all descendents of this variable stored by the Context View source","title":"#backprop"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#context","text":"The graph the variable is associated with. This is a reference, as a variable does not own its context View source","title":"#context"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#cos","text":"Computes the cosine of a variable","title":"#cos"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#elu(alpha)","text":"Exponential Linear Unit activation function","title":"#elu"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#exp","text":"Computes the exp of a variable","title":"#exp"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#grad","text":"The gradient of the Variable. This is set as a reference to the value of a Variable unless backprop has been called, in which case all related Variables will have their gradient updated correctly View source","title":"#grad"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#grad=(grad)","text":"The gradient of the Variable. This is set as a reference to the value of a Variable unless backprop has been called, in which case all related Variables will have their gradient updated correctly View source","title":"#grad="},{"location":"Num/Grad/Variable/#Num::Grad::Variable#leaky_relu","text":"View source","title":"#leaky_relu"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#matmul(b)","text":"Matrix multiply operator for two variables. Computes the dot product of two matrices and stores the result in the computational graph","title":"#matmul"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#relu","text":"View source","title":"#relu"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#requires_grad","text":"If set to true, this variable will track its operations, otherwise it will act similar to a Tensor, only calculating forward operations View source","title":"#requires_grad"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#requires_grad=(requires_grad)","text":"If set to true, this variable will track its operations, otherwise it will act similar to a Tensor, only calculating forward operations View source","title":"#requires_grad="},{"location":"Num/Grad/Variable/#Num::Grad::Variable#sin","text":"Computes the sine of a variable","title":"#sin"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#tan","text":"Computes the tangent of a variable","title":"#tan"},{"location":"Num/Grad/Variable/#Num::Grad::Variable#value","text":"The value of the Variable. This should not be edited outside of Variable operations, as other edits will not be tracked and will lead to incorrect results View source","title":"#value"},{"location":"Num/Int32AddTensorScalarInplace/","text":"class Num::Int32AddTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Int32AddTensorScalarInplace"},{"location":"Num/Int32AddTensorScalarInplace/#Num::Int32AddTensorScalarInplace","text":"","title":"Int32AddTensorScalarInplace"},{"location":"Num/Int32AddTensorScalarInplace/#Num::Int32AddTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Int32AddTensorScalarInplace/#Num::Int32AddTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Int32AddTensorScalarInplace/#Num::Int32AddTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Int32DivideTensorScalarInplace/","text":"class Num::Int32DivideTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Int32DivideTensorScalarInplace"},{"location":"Num/Int32DivideTensorScalarInplace/#Num::Int32DivideTensorScalarInplace","text":"","title":"Int32DivideTensorScalarInplace"},{"location":"Num/Int32DivideTensorScalarInplace/#Num::Int32DivideTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Int32DivideTensorScalarInplace/#Num::Int32DivideTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Int32DivideTensorScalarInplace/#Num::Int32DivideTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Int32MultiplyTensorScalarInplace/","text":"class Num::Int32MultiplyTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Int32MultiplyTensorScalarInplace"},{"location":"Num/Int32MultiplyTensorScalarInplace/#Num::Int32MultiplyTensorScalarInplace","text":"","title":"Int32MultiplyTensorScalarInplace"},{"location":"Num/Int32MultiplyTensorScalarInplace/#Num::Int32MultiplyTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Int32MultiplyTensorScalarInplace/#Num::Int32MultiplyTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Int32MultiplyTensorScalarInplace/#Num::Int32MultiplyTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/Int32SubtractTensorScalarInplace/","text":"class Num::Int32SubtractTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"Int32SubtractTensorScalarInplace"},{"location":"Num/Int32SubtractTensorScalarInplace/#Num::Int32SubtractTensorScalarInplace","text":"","title":"Int32SubtractTensorScalarInplace"},{"location":"Num/Int32SubtractTensorScalarInplace/#Num::Int32SubtractTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/Int32SubtractTensorScalarInplace/#Num::Int32SubtractTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/Int32SubtractTensorScalarInplace/#Num::Int32SubtractTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/NN/","text":"module Num::NN # Extended modules Num::NN Methods # #compute_fans ( * shape : Int ) # View source #conv2d ( input : Tensor ( Float32 , CPU ( Float32 )), weight : Tensor ( Float32 , CPU ( Float32 )), bias : Tensor ( Float32 , CPU ( Float32 )), padding : Tuple ( Int , Int ), stride : Tuple ( Int , Int ) = { 1 , 1 }) # Computes a 2D convolution over input images. Intended to be used in 2d convolution forward pass. This applies a 2D cross-correlation, not to be confused with the mathematical convolution. Arguments # input : Tensor - 4D Tensor batch of images of the size [N,C_in,H_in,W_in] weight : Tensor - 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW] bias : Tensor - 3D Tensor bias of the size [C_out,1,1] padding : Tuple - Tuple with height and width of the padding stride : Tuple - Tuple with height and width of the stride View source #conv2d_backward ( input : Tensor ( Float32 , CPU ( Float32 )), weight : Tensor ( Float32 , CPU ( Float32 )), bias : Tensor ( Float32 , CPU ( Float32 )), grad_output : Tensor ( Float32 , CPU ( Float32 )), padding : Tuple ( Int , Int ), stride : Tuple ( Int , Int ) = { 1 , 1 }) # Computes gradients of a 2D convolution. Intended to be used after conv2d to calculate gradients in backward pass. Arguments # input : Tensor - 4D Tensor batch of images of the size [N,C_in,H_in,W_in] weight : Tensor - 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW] bias : Tensor - 3D Tensor bias of the size [C_out,1,1] grad_output : Tensor - 4D Tensor gradient of size [N, C_out, H_out, W_out] padding : Tuple - Tuple with height and width of the padding stride : Tuple - Tuple with height and width of the stride View source #dropout ( input : Tensor ( U , CPU ( U )), mask : Tensor ( U , CPU ( U )), probability : Float ) : Tensor ( U , CPU ( U )) forall U # Computes a forward dropout activation Arguments # input : Tensor - Tensor to activate mask : Tensor - Mask to dropout probability : Float - Probability of dropout View source #dropout ( input : Tensor ( U , OCL ( U )), mask : Tensor ( U , OCL ( U )), probability : Float ) : Tensor ( U , OCL ( U )) forall U # Computes a forward dropout activation Arguments # input : Tensor - Tensor to activate mask : Tensor - Mask to dropout probability : Float - Probability of dropout View source #dropout_backwards ( gradient : Tensor ( U , CPU ( U )), mask : Tensor ( U , CPU ( U )), probability : Float ) : Tensor ( U , OCL ( U )) forall U # Computes a backwards dropout derivative Arguments # gradient : Tensor - Tensor used to compute backwards pass mask : Tensor - Mask to apply to the gradient probability : Float - Probability of dropout View source #elu ( x : Tensor ( U , CPU ( U )), alpha = 0.01 ) : Tensor ( U , CPU ( U )) forall U # Exponential linear unit activation Arguments # x : Tensor - Tensor to activate View source #elu! ( x : Tensor ( U , CPU ( U )), alpha = 0.01 ) : Tensor ( U , CPU ( U )) forall U # Exponential linear unit activation Arguments # x : Tensor - Tensor to activate View source #elu_prime ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # ELU derivative Arguments # gradient : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation View source #im2colgemm_conv2d ( input : Tensor ( U , CPU ( U )), kernel : Tensor ( U , CPU ( U )), bias : Tensor ( U , CPU ( U )), padding : Tuple ( Int , Int ) = { 0 , 0 }, stride : Tuple ( Int , Int ) = { 1 , 1 }) : Tensor ( U , CPU ( U )) forall U # Computes a 2D convolution over input images. Intended to be used in 2d convolution forward pass. This applies a 2D cross-correlation, not to be confused with the mathematical convolution. Arguments # input : Tensor - 4D Tensor batch of images of the size [N,C_in,H_in,W_in] weight : Tensor - 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW] bias : Tensor - 3D Tensor bias of the size [C_out,1,1] padding : Tuple - Tuple with height and width of the padding stride : Tuple - Tuple with height and width of the stride View source #im2colgemm_conv2d_gradient ( input : Tensor ( U , CPU ( U )), kernel : Tensor ( U , CPU ( U )), bias : Tensor ( U , CPU ( U )), grad_output : Tensor ( U , CPU ( U )), padding : Tuple ( Int , Int ) = { 0 , 0 }, stride : Tuple ( Int , Int ) = { 1 , 1 }) : Tuple ( Tensor ( U , CPU ( U )), Tensor ( U , CPU ( U )), Tensor ( U , CPU ( U ))) forall U # Computes gradients of a 2D convolution. Intended to be used after conv2d to calculate gradients in backward pass. Arguments # input : Tensor - 4D Tensor batch of images of the size [N,C_in,H_in,W_in] weight : Tensor - 4D Tensor convolving kernel weights of the size [C_out,C_in,kH,kW] bias : Tensor - 3D Tensor bias of the size [C_out,1,1] grad_output : Tensor - 4D Tensor gradient of size [N, C_out, H_out, W_out] padding : Tuple - Tuple with height and width of the padding stride : Tuple - Tuple with height and width of the stride View source #kaiming_normal ( * shape : Int , dtype : Tensor ( U , V ) . class ) forall U , V # View source #kaiming_uniform ( * shape : Int , dtype : Tensor ( U , V ) . class ) forall U , V # View source #leaky_relu ( x : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Leaky ReLU activation function Arguments # x : Tensor - Argument to activate View source #leaky_relu ( x : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Leaky ReLU activation function Arguments # x : Tensor - Argument to activate View source #leaky_relu! ( x : Tensor ( U , CPU ( U ))) forall U # Leaky ReLU activation function Arguments # x : Tensor - Argument to activate View source #leaky_relu! ( x : Tensor ( U , OCL ( U ))) forall U # Leaky ReLU activation function Arguments # x : Tensor - Argument to activate View source #leaky_relu_prime ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Leaky ReLU derivative Arguments # gradient : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation View source #leaky_relu_prime ( gradient : Tensor ( U , OCL ( U )), cached : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Leaky ReLU derivative Arguments # gradient : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation View source #load_iris_dataset # Returns labels, as well as X and Y training inputs for the IRIS dataset. View source #load_mnist_dataset # Returns a struct containing features, labels, as well as test_features and test_labels for the MNIST dataset View source #maxpool ( input : Tensor ( U , CPU ( U )), kernel : Tuple ( Int , Int ), padding = { 0 , 0 }, stride = { 0 , 0 }) : Tuple ( Tensor ( Int32 , CPU ( Int32 )), Tensor ( U , CPU ( U ))) forall U # Computes the maxpooling of a Tensor Arguments # input : Tensor - Tensor to pool kernel : Tuple - Kernel height and width target : Tensor - Tensor truth values padding : Tuple - Tuple with height and width of the padding stride : Tuple - Tuple with height and width of the stride View source #maxpool_backward ( shape : Array ( Int ), max_indices : Tensor ( Int32 , CPU ( Int32 )), grad_output : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Computes the maxpooling gradient Arguments # shape : Array - Shape of gradient output max_indices : Tensor - Pooled max indices grad_output : Tensor - Output from forward pass View source #mean_relative_error ( y : Tensor ( U , CPU ( U )), y_true : Tensor ( U , CPU ( U ))) forall U # Mean relative error for Tensor, mean of the element-wise |y_true - y|/max(|y_true|, |y|) Normally the relative error is defined as |y_true - y| / |y_true|, but here max is used to make it symmetric and to prevent dividing by zero, guaranteed to return zero in the case when both values are zero. View source #mse ( input : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Mean squared error loss Arguments # input : Tensor - Predicted values target : Tensor - Truth values View source #mse_backwards ( gradient : Tensor ( U , CPU ( U )), cache : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) forall U # Computes gradients of mean squared error loss Arguments # gradient : Tensor - Tensor gradient computed from MSE forwards cache : Tensor 4D - Cached Tensor from activation target : Tensor - Tensor truth values View source #numerical_gradient ( input : Tensor ( U , CPU ( U )), f : Proc ( Tensor ( U , CPU ( U )), U ), h : U = U . new ( 1e-5 )) forall U # Compute numerical gradient for any function w.r.t. to an input Tensor, useful for gradient checking, recommend using float64 types to assure numerical precision. The gradient is calculated as: (f(x + h) - f(x - h)) / (2*h) where h is a small number, typically 1e-5 f(x) will be called for each input elements with +h and -h pertubation. Iterate over all elements calculating each partial derivative View source #numerical_gradient ( input : Float , f : Proc ( Float , Float ), h : Float = 1e-5 ) : Float # Compute numerical gradient for any function w.r.t. to an input value, useful for gradient checking, recommend using float64 types to assure numerical precision. The gradient is calculated as: (f(x + h) - f(x - h)) / (2*h) where h is a small number, typically 1e-5. View source #relu ( x : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # ReLU activation function Arguments # x : Tensor - Argument to activate View source #relu ( x : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # ReLU activation function Arguments # x : Tensor - Argument to activate View source #relu! ( x : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # ReLU activation function Arguments # x : Tensor - Argument to activate View source #relu! ( x : Tensor ( U , OCL ( U ))) forall U # ReLU activation function Arguments # x : Tensor - Argument to activate View source #relu_prime ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Derivative of the ReLU activation function Arguments # gradient : Tensor - Tensor to derive cached : Tensor Cached Tensor from activation View source #relu_prime ( gradient : Tensor ( U , OCL ( U )), cached : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Derivative of the ReLU activation function Arguments # gradient : Tensor - Tensor to derive cached : Tensor Cached Tensor from activation View source #sgd_optimize ( value : Tensor ( U , CPU ( U )), gradient : Tensor ( U , CPU ( U )), learning_rate : Float ) forall U # View source #sgd_optimize ( value : Tensor ( U , OCL ( U )), gradient : Tensor ( U , OCL ( U )), learning_rate : Float ) forall U # View source #sigmoid ( x : Tensor ( U , OCL ( U ))) forall U # Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . sigmoid ( a ) # => [0.524979, 0.584191, 0.65701 ] View source #sigmoid ( x ) # Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . sigmoid ( a ) # => [0.524979, 0.584191, 0.65701 ] View source #sigmoid! ( x : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . sigmoid ( a ) # => [0.524979, 0.584191, 0.65701 ] View source #sigmoid! ( x : Tensor ( U , OCL ( U ))) forall U # Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . sigmoid ( a ) # => [0.524979, 0.584191, 0.65701 ] View source #sigmoid_cross_entropy ( input : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) forall U # Sigmoid cross entropy loss Arguments # input : Tensor - Predicted values target : Tensor - Truth values View source #sigmoid_cross_entropy ( input : Tensor ( U , OCL ( U )), target : Tensor ( U , OCL ( U ))) forall U # Sigmoid cross entropy loss Arguments # input : Tensor - Predicted values target : Tensor - Truth values View source #sigmoid_cross_entropy_backwards ( gradient : Tensor ( U , CPU ( U )), cache : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) forall U # Computes gradients of sigmoid cross entropy loss Arguments # gradient : Tensor - Tensor gradient computed from SCE forwards cache : Tensor 4D - Cached Tensor from activation target : Tensor - Tensor truth values View source #sigmoid_cross_entropy_backwards ( gradient : Tensor ( U , OCL ( U )), cache : Tensor ( U , OCL ( U )), target : Tensor ( U , OCL ( U ))) forall U # Computes gradients of sigmoid cross entropy loss Arguments # gradient : Tensor - Tensor gradient computed from SCE forwards cache : Tensor 4D - Cached Tensor from activation target : Tensor - Tensor truth values View source #sigmoid_prime ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Derivative of the Sigmoid function Arguments # gradient : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . d_sigmoid ( a ) # => [0.249376, 0.242912, 0.225348] View source #sigmoid_prime ( gradient : Tensor ( U , OCL ( U )), cached : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Derivative of the Sigmoid function Arguments # gradient : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor puts Num :: NN . d_sigmoid ( a ) # => [0.249376, 0.242912, 0.225348] View source #softmax_cross_entropy ( input : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) forall U # Computes softmax cross entropy loss Arguments # input : Tensor - Predicted values target : Tensor - Truth values View source #softmax_cross_entropy_backward ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U )), target : Tensor ( U , CPU ( U ))) forall U # Computes gradients of SmCE loss Arguments # gradient : Tensor - Tensor gradient computed from SmCE forwards cache : Tensor 4D - Cached Tensor from activation target : Tensor - Tensor truth values View source #tanh ( x : Tensor ( U , CPU ( U ))) : Tensor ( U , CPU ( U )) forall U # Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . tanh ( a ) # => [0.099668, 0.327477, 0.57167 ] View source #tanh ( x : Tensor ( U , OCL ( U ))) : Tensor ( U , OCL ( U )) forall U # Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . tanh ( a ) # => [0.099668, 0.327477, 0.57167 ] View source #tanh! ( x : Tensor ( U , CPU ( U ))) forall U # Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . tanh ( a ) # => [0.099668, 0.327477, 0.57167 ] View source #tanh! ( x : Tensor ( U , OCL ( U ))) forall U # Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Arguments # x : Tensor - Tensor to activate Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . tanh ( a ) # => [0.099668, 0.327477, 0.57167 ] View source #tanh_prime ( gradient : Tensor ( U , CPU ( U )), cached : Tensor ( U , CPU ( U ))) forall U # Derivative of the Tanh function Arguments # x : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . d_tanh ( a ) # => [0.990066, 0.892759, 0.673193] View source #tanh_prime ( gradient : Tensor ( U , OCL ( U )), cached : Tensor ( U , OCL ( U ))) forall U # Derivative of the Tanh function Arguments # x : Tensor - Tensor to derive cached : Tensor - Cached Tensor from activation Examples # a = [ 0.1 , 0.34 , 0.65 ]. to_tensor Num :: NN . d_tanh ( a ) # => [0.990066, 0.892759, 0.673193] View source #variance_scaled ( * shape : Int , dtype : U . class , device : V . class , scale : U = U . new ( 1 ), mode : FanMode = FanMode :: FanIn , distribution : Distribution = Distribution :: Normal ) forall U , V # View source","title":"NN"},{"location":"Num/NN/#Num::NN","text":"","title":"NN"},{"location":"Num/NN/#Num::NN-methods","text":"","title":"Methods"},{"location":"Num/NN/#Num::NN#compute_fans(*)","text":"View source","title":"#compute_fans"},{"location":"Num/NN/#Num::NN#conv2d(input,weight,bias,padding,stride)","text":"Computes a 2D convolution over input images. Intended to be used in 2d convolution forward pass. This applies a 2D cross-correlation, not to be confused with the mathematical convolution.","title":"#conv2d"},{"location":"Num/NN/#Num::NN#conv2d_backward(input,weight,bias,grad_output,padding,stride)","text":"Computes gradients of a 2D convolution. Intended to be used after conv2d to calculate gradients in backward pass.","title":"#conv2d_backward"},{"location":"Num/NN/#Num::NN#dropout(input,mask,probability)","text":"Computes a forward dropout activation","title":"#dropout"},{"location":"Num/NN/#Num::NN#dropout(input,mask,probability)","text":"Computes a forward dropout activation","title":"#dropout"},{"location":"Num/NN/#Num::NN#dropout_backwards(gradient,mask,probability)","text":"Computes a backwards dropout derivative","title":"#dropout_backwards"},{"location":"Num/NN/#Num::NN#elu(x,alpha)","text":"Exponential linear unit activation","title":"#elu"},{"location":"Num/NN/#Num::NN#elu!(x,alpha)","text":"Exponential linear unit activation","title":"#elu!"},{"location":"Num/NN/#Num::NN#elu_prime(gradient,cached)","text":"ELU derivative","title":"#elu_prime"},{"location":"Num/NN/#Num::NN#im2colgemm_conv2d(input,kernel,bias,padding,stride)","text":"Computes a 2D convolution over input images. Intended to be used in 2d convolution forward pass. This applies a 2D cross-correlation, not to be confused with the mathematical convolution.","title":"#im2colgemm_conv2d"},{"location":"Num/NN/#Num::NN#im2colgemm_conv2d_gradient(input,kernel,bias,grad_output,padding,stride)","text":"Computes gradients of a 2D convolution. Intended to be used after conv2d to calculate gradients in backward pass.","title":"#im2colgemm_conv2d_gradient"},{"location":"Num/NN/#Num::NN#kaiming_normal(*,dtype)","text":"View source","title":"#kaiming_normal"},{"location":"Num/NN/#Num::NN#kaiming_uniform(*,dtype)","text":"View source","title":"#kaiming_uniform"},{"location":"Num/NN/#Num::NN#leaky_relu(x)","text":"Leaky ReLU activation function","title":"#leaky_relu"},{"location":"Num/NN/#Num::NN#leaky_relu(x)","text":"Leaky ReLU activation function","title":"#leaky_relu"},{"location":"Num/NN/#Num::NN#leaky_relu!(x)","text":"Leaky ReLU activation function","title":"#leaky_relu!"},{"location":"Num/NN/#Num::NN#leaky_relu!(x)","text":"Leaky ReLU activation function","title":"#leaky_relu!"},{"location":"Num/NN/#Num::NN#leaky_relu_prime(gradient,cached)","text":"Leaky ReLU derivative","title":"#leaky_relu_prime"},{"location":"Num/NN/#Num::NN#leaky_relu_prime(gradient,cached)","text":"Leaky ReLU derivative","title":"#leaky_relu_prime"},{"location":"Num/NN/#Num::NN#load_iris_dataset","text":"Returns labels, as well as X and Y training inputs for the IRIS dataset. View source","title":"#load_iris_dataset"},{"location":"Num/NN/#Num::NN#load_mnist_dataset","text":"Returns a struct containing features, labels, as well as test_features and test_labels for the MNIST dataset View source","title":"#load_mnist_dataset"},{"location":"Num/NN/#Num::NN#maxpool(input,kernel,padding,stride)","text":"Computes the maxpooling of a Tensor","title":"#maxpool"},{"location":"Num/NN/#Num::NN#maxpool_backward(shape,max_indices,grad_output)","text":"Computes the maxpooling gradient","title":"#maxpool_backward"},{"location":"Num/NN/#Num::NN#mean_relative_error(y,y_true)","text":"Mean relative error for Tensor, mean of the element-wise |y_true - y|/max(|y_true|, |y|) Normally the relative error is defined as |y_true - y| / |y_true|, but here max is used to make it symmetric and to prevent dividing by zero, guaranteed to return zero in the case when both values are zero. View source","title":"#mean_relative_error"},{"location":"Num/NN/#Num::NN#mse(input,target)","text":"Mean squared error loss","title":"#mse"},{"location":"Num/NN/#Num::NN#mse_backwards(gradient,cache,target)","text":"Computes gradients of mean squared error loss","title":"#mse_backwards"},{"location":"Num/NN/#Num::NN#numerical_gradient(input,f,h)","text":"Compute numerical gradient for any function w.r.t. to an input Tensor, useful for gradient checking, recommend using float64 types to assure numerical precision. The gradient is calculated as: (f(x + h) - f(x - h)) / (2*h) where h is a small number, typically 1e-5 f(x) will be called for each input elements with +h and -h pertubation. Iterate over all elements calculating each partial derivative View source","title":"#numerical_gradient"},{"location":"Num/NN/#Num::NN#numerical_gradient(input,f,h)","text":"Compute numerical gradient for any function w.r.t. to an input value, useful for gradient checking, recommend using float64 types to assure numerical precision. The gradient is calculated as: (f(x + h) - f(x - h)) / (2*h) where h is a small number, typically 1e-5. View source","title":"#numerical_gradient"},{"location":"Num/NN/#Num::NN#relu(x)","text":"ReLU activation function","title":"#relu"},{"location":"Num/NN/#Num::NN#relu(x)","text":"ReLU activation function","title":"#relu"},{"location":"Num/NN/#Num::NN#relu!(x)","text":"ReLU activation function","title":"#relu!"},{"location":"Num/NN/#Num::NN#relu!(x)","text":"ReLU activation function","title":"#relu!"},{"location":"Num/NN/#Num::NN#relu_prime(gradient,cached)","text":"Derivative of the ReLU activation function","title":"#relu_prime"},{"location":"Num/NN/#Num::NN#relu_prime(gradient,cached)","text":"Derivative of the ReLU activation function","title":"#relu_prime"},{"location":"Num/NN/#Num::NN#sgd_optimize(value,gradient,learning_rate)","text":"View source","title":"#sgd_optimize"},{"location":"Num/NN/#Num::NN#sgd_optimize(value,gradient,learning_rate)","text":"View source","title":"#sgd_optimize"},{"location":"Num/NN/#Num::NN#sigmoid(x)","text":"Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range.","title":"#sigmoid"},{"location":"Num/NN/#Num::NN#sigmoid(x)","text":"Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range.","title":"#sigmoid"},{"location":"Num/NN/#Num::NN#sigmoid!(x)","text":"Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range.","title":"#sigmoid!"},{"location":"Num/NN/#Num::NN#sigmoid!(x)","text":"Sigmoid takes a real value as input and outputs another value between 0 and 1. It\u2019s easy to work with and has all the nice properties of activation functions: it\u2019s non-linear, continuously differentiable, monotonic, and has a fixed output range.","title":"#sigmoid!"},{"location":"Num/NN/#Num::NN#sigmoid_cross_entropy(input,target)","text":"Sigmoid cross entropy loss","title":"#sigmoid_cross_entropy"},{"location":"Num/NN/#Num::NN#sigmoid_cross_entropy(input,target)","text":"Sigmoid cross entropy loss","title":"#sigmoid_cross_entropy"},{"location":"Num/NN/#Num::NN#sigmoid_cross_entropy_backwards(gradient,cache,target)","text":"Computes gradients of sigmoid cross entropy loss","title":"#sigmoid_cross_entropy_backwards"},{"location":"Num/NN/#Num::NN#sigmoid_cross_entropy_backwards(gradient,cache,target)","text":"Computes gradients of sigmoid cross entropy loss","title":"#sigmoid_cross_entropy_backwards"},{"location":"Num/NN/#Num::NN#sigmoid_prime(gradient,cached)","text":"Derivative of the Sigmoid function","title":"#sigmoid_prime"},{"location":"Num/NN/#Num::NN#sigmoid_prime(gradient,cached)","text":"Derivative of the Sigmoid function","title":"#sigmoid_prime"},{"location":"Num/NN/#Num::NN#softmax_cross_entropy(input,target)","text":"Computes softmax cross entropy loss","title":"#softmax_cross_entropy"},{"location":"Num/NN/#Num::NN#softmax_cross_entropy_backward(gradient,cached,target)","text":"Computes gradients of SmCE loss","title":"#softmax_cross_entropy_backward"},{"location":"Num/NN/#Num::NN#tanh(x)","text":"Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.","title":"#tanh"},{"location":"Num/NN/#Num::NN#tanh(x)","text":"Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.","title":"#tanh"},{"location":"Num/NN/#Num::NN#tanh!(x)","text":"Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.","title":"#tanh!"},{"location":"Num/NN/#Num::NN#tanh!(x)","text":"Tanh squashes a real-valued number to the range [-1, 1]. It\u2019s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.","title":"#tanh!"},{"location":"Num/NN/#Num::NN#tanh_prime(gradient,cached)","text":"Derivative of the Tanh function","title":"#tanh_prime"},{"location":"Num/NN/#Num::NN#tanh_prime(gradient,cached)","text":"Derivative of the Tanh function","title":"#tanh_prime"},{"location":"Num/NN/#Num::NN#variance_scaled(*,dtype,device,scale,mode,distribution)","text":"View source","title":"#variance_scaled"},{"location":"Num/NN/AdamOptimizer/","text":"class Num::NN::AdamOptimizer(T) inherits Num::NN::Optimizer # Adam (short for Adaptive Moment Estimation) is an update to the RMSProp optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used. Constructors # .new ( learning_rate : Float64 = 0.001 , beta1 : Float64 = 0.9 , beta2 : Float64 = 0.999 , epsilon : Float64 = 1e-8 ) # Initializes an Adam optimizer, disconnected from a network. In order to link this optimizer to a Num::NN::Network , calling build_params will register each variable in the computational graph with this optimizer. Arguments # learning_rate : Float - Learning rate of the optimizer beta1 : Float - The exponential decay rate for the 1st moment estimates beta2 : Float - The exponential decay rate for the 2nd moment estimates epsilon : Float - A small constant for numerical stability View source Methods # #build_params ( l : Array ( Layer ( T ))) # Adds variables from a Num::NN::Network to the optimizer, to be tracked and updated after each forward pass through a network. Arguments # l : Array(Layer(T)) - Array of Layer s in the Network View source #update # Updates all Num::Grad::Variable s registered to the optimizer based on weights present in the network and the parameters of the optimizer. Resets all gradients to 0 . View source","title":"AdamOptimizer"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer","text":"Adam (short for Adaptive Moment Estimation) is an update to the RMSProp optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used.","title":"AdamOptimizer"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer.new(learning_rate,beta1,beta2,epsilon)","text":"Initializes an Adam optimizer, disconnected from a network. In order to link this optimizer to a Num::NN::Network , calling build_params will register each variable in the computational graph with this optimizer.","title":".new"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer-methods","text":"","title":"Methods"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer#build_params(l)","text":"Adds variables from a Num::NN::Network to the optimizer, to be tracked and updated after each forward pass through a network.","title":"#build_params"},{"location":"Num/NN/AdamOptimizer/#Num::NN::AdamOptimizer#update","text":"Updates all Num::Grad::Variable s registered to the optimizer based on weights present in the network and the parameters of the optimizer. Resets all gradients to 0 . View source","title":"#update"},{"location":"Num/NN/ConvolutionalLayer/","text":"class Num::NN::ConvolutionalLayer(T) inherits Num::NN::Layer # In a CNN, the input is a Tensor with a shape: (# of inputs) x (input h) x (input w) x (input channels) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (# of inputs) x (feature map h) x (feature map w) x (feature map channels) Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs such as high resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper. For example, regardless of image size, using a 5 x 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in traditional neural networks. Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling. Constructors # .new ( context : Num::Grad::Context ( T ), in_shape : Array ( Int ), num_filters : Int , kernel_height : Int , kernel_width : Int , padding = { 0 , 0 }, stride = { 1 , 1 }) # Creates a convolutional layer in a Network Arguments # context : Num::Grad::Context(T) - Context of the network. This argument is used entirely to determine the generic type of the layer in_shape : Array(Int) - Shape of input to layer num_filters : Int - Number of filters to apply in the convolution kernel_height : Int - Height of kernel for convolution kernel_width : Int - Width of kernel for convolution padding : Int - Padding of kernel stride : Int - Stride of kernel. Note The stride argument is currently only supported for the im2colgemm_conv2d , as it is not supported by NNPACK. Using this parameter is rarely worth the large performance difference if you are able to use NNPACK View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Performs a forward pass of a variable through a ConvolutionalLayer Arguments # input : Num::Grad::Variable(T) - Variable to convolve View source #output_shape : Array ( Int32 ) # Returns the output shape of a ConvolutionalLayer . This method is primarily used to infer the input shape of following layers in a Network View source #variables : Array ( Num::Grad::Variable ( T )) # Returns all Num::Grad::Variables associated with the Layer . Used primarily to register variables with optimizers View source","title":"ConvolutionalLayer"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer","text":"In a CNN, the input is a Tensor with a shape: (# of inputs) x (input h) x (input w) x (input channels) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (# of inputs) x (feature map h) x (feature map w) x (feature map channels) Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs such as high resolution images. It would require a very high number of neurons, even in a shallow architecture, due to the large input size of images, where each pixel is a relevant input feature. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for each neuron in the second layer. Instead, convolution reduces the number of free parameters, allowing the network to be deeper. For example, regardless of image size, using a 5 x 5 tiling region, each with the same shared weights, requires only 25 learnable parameters. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in traditional neural networks. Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling.","title":"ConvolutionalLayer"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer.new(context,in_shape,num_filters,kernel_height,kernel_width,padding,stride)","text":"Creates a convolutional layer in a Network","title":".new"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer#forward(input)","text":"Performs a forward pass of a variable through a ConvolutionalLayer","title":"#forward"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer#output_shape","text":"Returns the output shape of a ConvolutionalLayer . This method is primarily used to infer the input shape of following layers in a Network View source","title":"#output_shape"},{"location":"Num/NN/ConvolutionalLayer/#Num::NN::ConvolutionalLayer#variables","text":"Returns all Num::Grad::Variables associated with the Layer . Used primarily to register variables with optimizers View source","title":"#variables"},{"location":"Num/NN/Distribution/","text":"enum Num::NN::Distribution # Type of distribution to return from the random initial values Members # Uniform = 0 # Normal = 1 # Methods # #normal? # View source #uniform? # View source","title":"Distribution"},{"location":"Num/NN/Distribution/#Num::NN::Distribution","text":"Type of distribution to return from the random initial values","title":"Distribution"},{"location":"Num/NN/Distribution/#Num::NN::Distribution-members","text":"","title":"Members"},{"location":"Num/NN/Distribution/#Num::NN::Distribution::Uniform","text":"","title":"Uniform"},{"location":"Num/NN/Distribution/#Num::NN::Distribution::Normal","text":"","title":"Normal"},{"location":"Num/NN/Distribution/#Num::NN::Distribution-methods","text":"","title":"Methods"},{"location":"Num/NN/Distribution/#Num::NN::Distribution#normal?","text":"View source","title":"#normal?"},{"location":"Num/NN/Distribution/#Num::NN::Distribution#uniform?","text":"View source","title":"#uniform?"},{"location":"Num/NN/DropoutLayer/","text":"class Num::NN::DropoutLayer(T) inherits Num::NN::Layer # Dilution (also called Dropout) is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks. The term dilution refers to the thinning of the weights. The term dropout refers to randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network. Both the thinning of weights and dropping out units trigger the same type of regularization, and often the term dropout is used when referring to the dilution of weights. Constructors # .new ( context : Num::Grad::Context ( T ), output_shape : Array ( Int32 ), prob = 0.5_f32 ) # Initialize a dropout layer in a Num::NN::Network(T) Arguments # context : Num::Grad::Context(T) - Context associated with the network, used only for determining generic type. output_shape : Array(Int32) - Cached output shape prob : Float32 - Probability of dropping out a value when performing a forward pass View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Computes the forward pass of a Num::NN::Network . This will remove a certain amount of neurons from the input variable, and scale the remaining values by the probability of removal. Arguments # input : Num::Grad::Variable(T) - Input variable to the layer View source #output_shape : Array ( Int32 ) # View source","title":"DropoutLayer"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer","text":"Dilution (also called Dropout) is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks. The term dilution refers to the thinning of the weights. The term dropout refers to randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network. Both the thinning of weights and dropping out units trigger the same type of regularization, and often the term dropout is used when referring to the dilution of weights.","title":"DropoutLayer"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer.new(context,output_shape,prob)","text":"Initialize a dropout layer in a Num::NN::Network(T)","title":".new"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer#forward(input)","text":"Computes the forward pass of a Num::NN::Network . This will remove a certain amount of neurons from the input variable, and scale the remaining values by the probability of removal.","title":"#forward"},{"location":"Num/NN/DropoutLayer/#Num::NN::DropoutLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/EluLayer/","text":"class Num::NN::EluLayer(T) inherits Num::NN::Layer # Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number. ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -\u03b1 whereas RELU sharply smoothes. Constructors # .new ( context : Num::Grad::Context ( T ), output_shape : Array ( Int32 ), alpha : Float32 | Float64 = 0.01 ) # Initializes an ELU activation layer as part of a Num::NN::Network Arguments # context : Num::Grad::Context(T) - Context of the Num::NN::Network , used only to determine generic type of the Num::NN::Layer(T) output_shape : Array(Int32) - The shape of the output of the layer alpha : Float - Scale for the negative factor View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Computes a forward pass through an ELU layer. Arguments # input : Num::Grad::Variable(T) - Variable to activate View source #output_shape : Array ( Int32 ) # View source","title":"EluLayer"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer","text":"Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number. ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -\u03b1 whereas RELU sharply smoothes.","title":"EluLayer"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer.new(context,output_shape,alpha)","text":"Initializes an ELU activation layer as part of a Num::NN::Network","title":".new"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer#forward(input)","text":"Computes a forward pass through an ELU layer.","title":"#forward"},{"location":"Num/NN/EluLayer/#Num::NN::EluLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/FanMode/","text":"enum Num::NN::FanMode # Represents the type of Fan meant to be discovered, whether it is the maximum number of inputs or outputs. Members # FanAvg = 0 # FanIn = 1 # FanOut = 2 # Methods # #fan_avg? # View source #fan_in? # View source #fan_out? # View source","title":"FanMode"},{"location":"Num/NN/FanMode/#Num::NN::FanMode","text":"Represents the type of Fan meant to be discovered, whether it is the maximum number of inputs or outputs.","title":"FanMode"},{"location":"Num/NN/FanMode/#Num::NN::FanMode-members","text":"","title":"Members"},{"location":"Num/NN/FanMode/#Num::NN::FanMode::FanAvg","text":"","title":"FanAvg"},{"location":"Num/NN/FanMode/#Num::NN::FanMode::FanIn","text":"","title":"FanIn"},{"location":"Num/NN/FanMode/#Num::NN::FanMode::FanOut","text":"","title":"FanOut"},{"location":"Num/NN/FanMode/#Num::NN::FanMode-methods","text":"","title":"Methods"},{"location":"Num/NN/FanMode/#Num::NN::FanMode#fan_avg?","text":"View source","title":"#fan_avg?"},{"location":"Num/NN/FanMode/#Num::NN::FanMode#fan_in?","text":"View source","title":"#fan_in?"},{"location":"Num/NN/FanMode/#Num::NN::FanMode#fan_out?","text":"View source","title":"#fan_out?"},{"location":"Num/NN/FlattenLayer/","text":"class Num::NN::FlattenLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), shape : Array ( Int32 )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source #shape : Array ( Int32 ) # View source","title":"FlattenLayer"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer","text":"","title":"FlattenLayer"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer.new(context,shape)","text":"View source","title":".new"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/FlattenLayer/#Num::NN::FlattenLayer#shape","text":"View source","title":"#shape"},{"location":"Num/NN/InputLayer/","text":"class Num::NN::InputLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), shape : Array ( Int )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source #shape : Array ( Int32 ) # View source","title":"InputLayer"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer","text":"","title":"InputLayer"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer.new(context,shape)","text":"View source","title":".new"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/InputLayer/#Num::NN::InputLayer#shape","text":"View source","title":"#shape"},{"location":"Num/NN/Layer/","text":"abstract class Num::NN::Layer(T) inherits Reference # Direct known subclasses Num::NN::ConvolutionalLayer(T) Num::NN::DropoutLayer(T) Num::NN::EluLayer(T) Num::NN::FlattenLayer(T) Num::NN::InputLayer(T) Num::NN::LeakyReluLayer(T) Num::NN::LinearLayer(T) Num::NN::MaxPoolLayer(T) Num::NN::ReluLayer(T) Num::NN::SigmoidLayer(T) Methods # abstract #forward ( input : Num::Grad::Variable ( T )) # View source abstract #output_shape : Array ( Int32 ) # View source #variables : Array ( Num::Grad::Variable ( T )) # View source","title":"Layer"},{"location":"Num/NN/Layer/#Num::NN::Layer","text":"","title":"Layer"},{"location":"Num/NN/Layer/#Num::NN::Layer-methods","text":"","title":"Methods"},{"location":"Num/NN/Layer/#Num::NN::Layer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/Layer/#Num::NN::Layer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/Layer/#Num::NN::Layer#variables","text":"View source","title":"#variables"},{"location":"Num/NN/LeakyReluLayer/","text":"class Num::NN::LeakyReluLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), output_shape : Array ( Int32 )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source","title":"LeakyReluLayer"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer","text":"","title":"LeakyReluLayer"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer.new(context,output_shape)","text":"View source","title":".new"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/LeakyReluLayer/#Num::NN::LeakyReluLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/LinearLayer/","text":"class Num::NN::LinearLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), inp_dim : Int , outp_dim : Int ) # View source Methods # #bias : Num::Grad::Variable ( T ) # View source #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source #variables : Array ( Num::Grad::Variable ( T )) # View source #weights : Num::Grad::Variable ( T ) # View source","title":"LinearLayer"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer","text":"","title":"LinearLayer"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer.new(context,inp_dim,outp_dim)","text":"View source","title":".new"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer#bias","text":"View source","title":"#bias"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer#variables","text":"View source","title":"#variables"},{"location":"Num/NN/LinearLayer/#Num::NN::LinearLayer#weights","text":"View source","title":"#weights"},{"location":"Num/NN/Loss/","text":"class Num::NN::Loss(T) inherits Reference # Direct known subclasses Num::NN::MSELoss(T) Num::NN::SigmoidCrossEntropyLoss(T) Num::NN::SoftmaxCrossEntropyLoss(T) Methods # #loss ( input : Num::Grad::Variable ( T ), target : T ) : Num::Grad::Variable ( T ) # View source","title":"Loss"},{"location":"Num/NN/Loss/#Num::NN::Loss","text":"","title":"Loss"},{"location":"Num/NN/Loss/#Num::NN::Loss-methods","text":"","title":"Methods"},{"location":"Num/NN/Loss/#Num::NN::Loss#loss(input,target)","text":"View source","title":"#loss"},{"location":"Num/NN/MSELoss/","text":"class Num::NN::MSELoss(T) inherits Num::NN::Loss # Methods # #loss ( input : Num::Grad::Variable ( T ), target : T ) : Num::Grad::Variable ( T ) # View source","title":"MSELoss"},{"location":"Num/NN/MSELoss/#Num::NN::MSELoss","text":"","title":"MSELoss"},{"location":"Num/NN/MSELoss/#Num::NN::MSELoss-methods","text":"","title":"Methods"},{"location":"Num/NN/MSELoss/#Num::NN::MSELoss#loss(input,target)","text":"View source","title":"#loss"},{"location":"Num/NN/MaxPoolLayer/","text":"class Num::NN::MaxPoolLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), input_shape : Array ( Int ), kernel : Tuple ( Int , Int ), padding : Tuple ( Int , Int ), stride : Tuple ( Int , Int )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #input_shape : Array ( Int32 ) # View source #kernel : Tuple ( Int32 , Int32 ) # View source #output_shape : Array ( Int32 ) # View source #padding : Tuple ( Int32 , Int32 ) # View source #stride : Tuple ( Int32 , Int32 ) # View source","title":"MaxPoolLayer"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer","text":"","title":"MaxPoolLayer"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer.new(context,input_shape,kernel,padding,stride)","text":"View source","title":".new"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#input_shape","text":"View source","title":"#input_shape"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#kernel","text":"View source","title":"#kernel"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#padding","text":"View source","title":"#padding"},{"location":"Num/NN/MaxPoolLayer/#Num::NN::MaxPoolLayer#stride","text":"View source","title":"#stride"},{"location":"Num/NN/Network/","text":"class Num::NN::Network(T) inherits Reference # A neural network can be defined as a biologically inspired computational model that consists of a network architecture composed of artificial neurons. This structure contains a set of parameters, which can be adjusted to perform specific tasks. This class is a loose wrapper that primarily provides syntactic sugar around moving data through a network -> forward, and propogating loss <- backwards Constructors # .new ( context : Num::Grad::Context # Convenience method to allow for creation of a Network with as little code as possible. Taps an instance of a LayerArray in order to allow layers to be added to the network in a block Examples # Network ( Float32 ) . new do layer ( 2 , 3 , :tanh ) layer ( 3 , 1 , :sigmoid ) end View source Methods # #forward ( train : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # Propogates an input through a network, returning the final prediction from the network Arguments # train : Num::Grad::Variable(T) - Training input data Examples # a = [[ 0.0 , 0.0 ] , [ 1.0 , 0.0 ] , [ 0.0 , 1.0 ] , [ 1.0 , 1.0 ]]. to_tensor v = ctx . variable ( a ) net . forward ( v ) View source #layers : Num::NN::NetworkInfo ( T ) # View source #loss ( output : Num::Grad::Variable ( T ), target : T ) # Uses the Network's loss function to calculate the loss based on the final output from the Network, as well as the target output Arguments # output : Num::Grad::Variable(T) - Prediction by the network target : T - Tensor containing ground truth values Examples # epochs . times do | epoch | y_pred = net . forward ( x ) loss = net . loss ( y_pred , y_actual ) end View source #optimizer # Return the Network's optimizer to allow updating the weights and biases of the network Examples # epochs . times do | epoch | y_pred = net . forward ( x ) loss = net . loss ( y_pred , y_actual ) net . optimizer . update end View source","title":"Network"},{"location":"Num/NN/Network/#Num::NN::Network","text":"A neural network can be defined as a biologically inspired computational model that consists of a network architecture composed of artificial neurons. This structure contains a set of parameters, which can be adjusted to perform specific tasks. This class is a loose wrapper that primarily provides syntactic sugar around moving data through a network -> forward, and propogating loss <- backwards","title":"Network"},{"location":"Num/NN/Network/#Num::NN::Network-constructors","text":"","title":"Constructors"},{"location":"Num/NN/Network/#Num::NN::Network.new(context,**,&)","text":"Convenience method to allow for creation of a Network with as little code as possible. Taps an instance of a LayerArray in order to allow layers to be added to the network in a block","title":".new"},{"location":"Num/NN/Network/#Num::NN::Network-methods","text":"","title":"Methods"},{"location":"Num/NN/Network/#Num::NN::Network#forward(train)","text":"Propogates an input through a network, returning the final prediction from the network","title":"#forward"},{"location":"Num/NN/Network/#Num::NN::Network#layers","text":"View source","title":"#layers"},{"location":"Num/NN/Network/#Num::NN::Network#loss(output,target)","text":"Uses the Network's loss function to calculate the loss based on the final output from the Network, as well as the target output","title":"#loss"},{"location":"Num/NN/Network/#Num::NN::Network#optimizer","text":"Return the Network's optimizer to allow updating the weights and biases of the network","title":"#optimizer"},{"location":"Num/NN/NetworkInfo/","text":"class Num::NN::NetworkInfo(T) inherits Reference # Constructors # .new ( context : Num::Grad::Context ( T )) # This should always be initialized with an empty array of layers that can be tapped and yielded by Network creation View source Methods # #adam ( * args ) # Adds an Adam optimizer to the Network View source #context : Num::Grad::Context ( T ) # View source #conv2d ( n : Int32 , kh : Int32 , kw : Int32 ) # Convolution layer for a neural network Arguments # n : Int32 - Number of filters to apply kh : Int32 - Filter height kw : Int32 - Filter width View source #dropout ( prob : Float = 0.5_f32 ) # Adds a dropout layer for a neural network Arguments # prob : Float - Probability of a neuron being dropped out View source #elu # Adds an ELU layer to the network View source #flatten # Adds a Flattening layer to a neural network View source #input ( shape : Array ( Int )) # Adds an input layer to a Network . This is simply a wrapper around the input Tensor , in order to allow layers further along in the network to infer input shapes Arguments # shape : Array(Int) - Shape of input data View source #layers : Array ( Num::NN::Layer ( T )) # View source #leaky_relu # Adds a Leaky ReLU layer to a network. View source #linear ( output_size : Int ) # Add a linear layer to the Network. Since activation functions are just treated as additional layers, this simply requires the dimensions of the transformation. Dimensions should be NUM_FEATURES x NUM_OUTPUTS , so if the data set is 100x10, with 200 neurons in the hidden layers, the dimensions of the layer would be 10, 100, the 200 will be handled by dynamically. Arguments # output_size : Int - The number of outputs in the linear layer Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 end View source #loss : Num::NN::Loss ( T ) # View source #maxpool ( kernel : Tuple ( Int , Int ), padding : Tuple ( Int , Int ), stride : Tuple ( Int , Int )) # Maxpool layer for a neural network Arguments # kernel : Tuple(Int, Int) - Kernel height and width padding : Tuple(Int, Int)` - Padding height and width stride : Tuple(Int, Int)` - Stride height and width View source #mse_loss # Uses Mean Squared Error to compute the loss for the Network Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 sigmoid linear 3 , 1 sgd 0.7 mse_loss end View source #optimizer : Num::NN::Optimizer ( T ) # View source #relu # Add a ReLU layer to the Network. Activation functions are handled the same way as other layers, but do not change the dimensions of the input Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 relu end View source #sgd ( learning_rate : Float64 = 0.01 ) # Add an SGD optimizer to the Network. Arguments # learning_rate : Float64 - Learning rate for all layers in the Network Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 sigmoid linear 3 , 1 sgd 0.7 end View source #sigmoid # Add a Sigmoid layer to the Network. Activation functions are handled the same way as other layers, but do not change the dimensions of the input Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 sigmoid end View source #sigmoid_cross_entropy_loss # Uses Sigmoid Cross Entropy to compute the loss for the Network Examples # net = Num :: NN :: Network . new ( ctx ) do linear 2 , 3 sigmoid linear 3 , 1 sgd 0.7 sigmoid_cross_entropy_loss end View source #softmax_cross_entropy_loss # Specifies Softmax Cross Entropy as the method of loss to be used with the Network View source Macros # method_missing ( call ) # View source","title":"NetworkInfo"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo","text":"","title":"NetworkInfo"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo-constructors","text":"","title":"Constructors"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo.new(context)","text":"This should always be initialized with an empty array of layers that can be tapped and yielded by Network creation View source","title":".new"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo-methods","text":"","title":"Methods"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#adam(*)","text":"Adds an Adam optimizer to the Network View source","title":"#adam"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#context","text":"View source","title":"#context"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#conv2d(n,kh,kw)","text":"Convolution layer for a neural network","title":"#conv2d"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#dropout(prob)","text":"Adds a dropout layer for a neural network","title":"#dropout"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#elu","text":"Adds an ELU layer to the network View source","title":"#elu"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#flatten","text":"Adds a Flattening layer to a neural network View source","title":"#flatten"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#input(shape)","text":"Adds an input layer to a Network . This is simply a wrapper around the input Tensor , in order to allow layers further along in the network to infer input shapes","title":"#input"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#layers","text":"View source","title":"#layers"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#leaky_relu","text":"Adds a Leaky ReLU layer to a network. View source","title":"#leaky_relu"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#linear(output_size)","text":"Add a linear layer to the Network. Since activation functions are just treated as additional layers, this simply requires the dimensions of the transformation. Dimensions should be NUM_FEATURES x NUM_OUTPUTS , so if the data set is 100x10, with 200 neurons in the hidden layers, the dimensions of the layer would be 10, 100, the 200 will be handled by dynamically.","title":"#linear"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#loss","text":"View source","title":"#loss"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#maxpool(kernel,padding,stride)","text":"Maxpool layer for a neural network","title":"#maxpool"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#mse_loss","text":"Uses Mean Squared Error to compute the loss for the Network","title":"#mse_loss"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#optimizer","text":"View source","title":"#optimizer"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#relu","text":"Add a ReLU layer to the Network. Activation functions are handled the same way as other layers, but do not change the dimensions of the input","title":"#relu"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#sgd(learning_rate)","text":"Add an SGD optimizer to the Network.","title":"#sgd"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#sigmoid","text":"Add a Sigmoid layer to the Network. Activation functions are handled the same way as other layers, but do not change the dimensions of the input","title":"#sigmoid"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#sigmoid_cross_entropy_loss","text":"Uses Sigmoid Cross Entropy to compute the loss for the Network","title":"#sigmoid_cross_entropy_loss"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo#softmax_cross_entropy_loss","text":"Specifies Softmax Cross Entropy as the method of loss to be used with the Network View source","title":"#softmax_cross_entropy_loss"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo-macros","text":"","title":"Macros"},{"location":"Num/NN/NetworkInfo/#Num::NN::NetworkInfo:method_missing(call)","text":"View source","title":"method_missing"},{"location":"Num/NN/Optimizer/","text":"class Num::NN::Optimizer(T) inherits Reference # Direct known subclasses Num::NN::AdamOptimizer(T) Num::NN::SGDMomentumOptimizer(T) Num::NN::SGDOptimizer(T) Constructors # .new ( learning_rate : Float64 = 0.01 ) # View source Methods # #build_params ( l : Array ( Layer ( T ))) # View source #learning_rate : Float64 # View source #params : Array ( Num::Grad::Variable ( T )) # View source #update # View source","title":"Optimizer"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer","text":"","title":"Optimizer"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer.new(learning_rate)","text":"View source","title":".new"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer-methods","text":"","title":"Methods"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer#build_params(l)","text":"View source","title":"#build_params"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer#learning_rate","text":"View source","title":"#learning_rate"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer#params","text":"View source","title":"#params"},{"location":"Num/NN/Optimizer/#Num::NN::Optimizer#update","text":"View source","title":"#update"},{"location":"Num/NN/ReluLayer/","text":"class Num::NN::ReluLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), output_shape : Array ( Int32 )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source","title":"ReluLayer"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer","text":"","title":"ReluLayer"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer.new(context,output_shape)","text":"View source","title":".new"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/ReluLayer/#Num::NN::ReluLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/SGDMomentumOptimizer/","text":"class Num::NN::SGDMomentumOptimizer(T) inherits Num::NN::Optimizer # Constructors # .new ( learning_rate : Float64 , momentum : Float64 = 0.0 , decay : Float64 = 0.0 , nesterov : Bool = false ) # View source Methods # #build_params ( l : Array ( Layer ( T ))) # View source #decay : Float64 # View source #learning_rate : Float64 # View source #moments : Array ( T ) # View source #momentum : Float64 # View source #nesterov : Bool # View source #params : Array ( Num::Grad::Variable ( T )) # View source #update # View source","title":"SGDMomentumOptimizer"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer","text":"","title":"SGDMomentumOptimizer"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer.new(learning_rate,momentum,decay,nesterov)","text":"View source","title":".new"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer-methods","text":"","title":"Methods"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#build_params(l)","text":"View source","title":"#build_params"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#decay","text":"View source","title":"#decay"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#learning_rate","text":"View source","title":"#learning_rate"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#moments","text":"View source","title":"#moments"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#momentum","text":"View source","title":"#momentum"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#nesterov","text":"View source","title":"#nesterov"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#params","text":"View source","title":"#params"},{"location":"Num/NN/SGDMomentumOptimizer/#Num::NN::SGDMomentumOptimizer#update","text":"View source","title":"#update"},{"location":"Num/NN/SGDOptimizer/","text":"class Num::NN::SGDOptimizer(T) inherits Num::NN::Optimizer # Constructors # .new ( learning_rate : Float64 ) # View source Methods # #build_params ( l : Array ( Layer ( T ))) # View source #learning_rate : Float64 # View source #params : Array ( Num::Grad::Variable ( T )) # View source #update # View source","title":"SGDOptimizer"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer","text":"","title":"SGDOptimizer"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer.new(learning_rate)","text":"View source","title":".new"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer-methods","text":"","title":"Methods"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer#build_params(l)","text":"View source","title":"#build_params"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer#learning_rate","text":"View source","title":"#learning_rate"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer#params","text":"View source","title":"#params"},{"location":"Num/NN/SGDOptimizer/#Num::NN::SGDOptimizer#update","text":"View source","title":"#update"},{"location":"Num/NN/SigmoidCrossEntropyLoss/","text":"class Num::NN::SigmoidCrossEntropyLoss(T) inherits Num::NN::Loss # Methods # #loss ( input : Num::Grad::Variable ( T ), target : T ) : Num::Grad::Variable ( T ) # View source","title":"SigmoidCrossEntropyLoss"},{"location":"Num/NN/SigmoidCrossEntropyLoss/#Num::NN::SigmoidCrossEntropyLoss","text":"","title":"SigmoidCrossEntropyLoss"},{"location":"Num/NN/SigmoidCrossEntropyLoss/#Num::NN::SigmoidCrossEntropyLoss-methods","text":"","title":"Methods"},{"location":"Num/NN/SigmoidCrossEntropyLoss/#Num::NN::SigmoidCrossEntropyLoss#loss(input,target)","text":"View source","title":"#loss"},{"location":"Num/NN/SigmoidLayer/","text":"class Num::NN::SigmoidLayer(T) inherits Num::NN::Layer # Constructors # .new ( context : Num::Grad::Context ( T ), output_shape : Array ( Int32 )) # View source Methods # #forward ( input : Num::Grad::Variable ( T )) : Num::Grad::Variable ( T ) # View source #output_shape : Array ( Int32 ) # View source","title":"SigmoidLayer"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer","text":"","title":"SigmoidLayer"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer-constructors","text":"","title":"Constructors"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer.new(context,output_shape)","text":"View source","title":".new"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer-methods","text":"","title":"Methods"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer#forward(input)","text":"View source","title":"#forward"},{"location":"Num/NN/SigmoidLayer/#Num::NN::SigmoidLayer#output_shape","text":"View source","title":"#output_shape"},{"location":"Num/NN/SoftmaxCrossEntropyLoss/","text":"class Num::NN::SoftmaxCrossEntropyLoss(T) inherits Num::NN::Loss # Methods # #loss ( input : Num::Grad::Variable ( T ), target : T ) : Num::Grad::Variable ( T ) # View source","title":"SoftmaxCrossEntropyLoss"},{"location":"Num/NN/SoftmaxCrossEntropyLoss/#Num::NN::SoftmaxCrossEntropyLoss","text":"","title":"SoftmaxCrossEntropyLoss"},{"location":"Num/NN/SoftmaxCrossEntropyLoss/#Num::NN::SoftmaxCrossEntropyLoss-methods","text":"","title":"Methods"},{"location":"Num/NN/SoftmaxCrossEntropyLoss/#Num::NN::SoftmaxCrossEntropyLoss#loss(input,target)","text":"View source","title":"#loss"},{"location":"Num/OrderType/","text":"enum Num::OrderType # Members # RowMajor = 0 # ColMajor = 1 # Methods # #col_major? # View source #row_major? # View source","title":"OrderType"},{"location":"Num/OrderType/#Num::OrderType","text":"","title":"OrderType"},{"location":"Num/OrderType/#Num::OrderType-members","text":"","title":"Members"},{"location":"Num/OrderType/#Num::OrderType::RowMajor","text":"","title":"RowMajor"},{"location":"Num/OrderType/#Num::OrderType::ColMajor","text":"","title":"ColMajor"},{"location":"Num/OrderType/#Num::OrderType-methods","text":"","title":"Methods"},{"location":"Num/OrderType/#Num::OrderType#col_major?","text":"View source","title":"#col_major?"},{"location":"Num/OrderType/#Num::OrderType#row_major?","text":"View source","title":"#row_major?"},{"location":"Num/Rand/","text":"class Num::Rand inherits Reference # Class methods # .generator # View source .set_seed ( seed ) # View source .stdlib_generator # View source","title":"Rand"},{"location":"Num/Rand/#Num::Rand","text":"","title":"Rand"},{"location":"Num/Rand/#Num::Rand-class-methods","text":"","title":"Class methods"},{"location":"Num/Rand/#Num::Rand.generator","text":"View source","title":".generator"},{"location":"Num/Rand/#Num::Rand.set_seed(seed)","text":"View source","title":".set_seed"},{"location":"Num/Rand/#Num::Rand.stdlib_generator","text":"View source","title":".stdlib_generator"},{"location":"Num/UInt32AddTensorScalarInplace/","text":"class Num::UInt32AddTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"UInt32AddTensorScalarInplace"},{"location":"Num/UInt32AddTensorScalarInplace/#Num::UInt32AddTensorScalarInplace","text":"","title":"UInt32AddTensorScalarInplace"},{"location":"Num/UInt32AddTensorScalarInplace/#Num::UInt32AddTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/UInt32AddTensorScalarInplace/#Num::UInt32AddTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/UInt32AddTensorScalarInplace/#Num::UInt32AddTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/UInt32DivideTensorScalarInplace/","text":"class Num::UInt32DivideTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"UInt32DivideTensorScalarInplace"},{"location":"Num/UInt32DivideTensorScalarInplace/#Num::UInt32DivideTensorScalarInplace","text":"","title":"UInt32DivideTensorScalarInplace"},{"location":"Num/UInt32DivideTensorScalarInplace/#Num::UInt32DivideTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/UInt32DivideTensorScalarInplace/#Num::UInt32DivideTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/UInt32DivideTensorScalarInplace/#Num::UInt32DivideTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/UInt32MultiplyTensorScalarInplace/","text":"class Num::UInt32MultiplyTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"UInt32MultiplyTensorScalarInplace"},{"location":"Num/UInt32MultiplyTensorScalarInplace/#Num::UInt32MultiplyTensorScalarInplace","text":"","title":"UInt32MultiplyTensorScalarInplace"},{"location":"Num/UInt32MultiplyTensorScalarInplace/#Num::UInt32MultiplyTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/UInt32MultiplyTensorScalarInplace/#Num::UInt32MultiplyTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/UInt32MultiplyTensorScalarInplace/#Num::UInt32MultiplyTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Num/UInt32SubtractTensorScalarInplace/","text":"class Num::UInt32SubtractTensorScalarInplace inherits Num::ArithmeticTensorScalarInplaceKernel # Constructors # .instance : self # .new # View source","title":"UInt32SubtractTensorScalarInplace"},{"location":"Num/UInt32SubtractTensorScalarInplace/#Num::UInt32SubtractTensorScalarInplace","text":"","title":"UInt32SubtractTensorScalarInplace"},{"location":"Num/UInt32SubtractTensorScalarInplace/#Num::UInt32SubtractTensorScalarInplace-constructors","text":"","title":"Constructors"},{"location":"Num/UInt32SubtractTensorScalarInplace/#Num::UInt32SubtractTensorScalarInplace.instance","text":"","title":".instance"},{"location":"Num/UInt32SubtractTensorScalarInplace/#Num::UInt32SubtractTensorScalarInplace.new","text":"View source","title":".new"},{"location":"Number/","text":"abstract struct Number inherits Value # The top-level number type. Included modules Comparable Steppable Methods # #* ( other : Tensor ) # Multiplies a Tensor with a number. The number is broadcasted across all elements of the Tensor . Arguments # other : Tensor - Tensor on which to perform the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor 3 * a # => [3, 6, 9] View source #** ( other : Tensor ) # Raises a number to a Tensor . The number is broadcasted across all elements of the Tensor . Arguments # other : Tensor - Tensor on which to perform the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor 2 ** a # => [2, 4, 8] View source #+ ( other : Tensor ) # Adds a Tensor to a number. The number is broadcasted across all elements of the Tensor . Arguments # other : Tensor - Tensor on which to perform the operation Examples # a = [ 1 , 2 , 3 ]. to_tensor 3 + a # => [4, 5, 6] View source #- ( other : Tensor ) # Subtracts a Tensor from a number. The number is broadcasted across all elements of the Tensor . Arguments # other : Tensor - Tensor on which to perform the operation Examples # a = [ 3 , 3 , 3 ]. to_tensor 3 - a # => [0, 0, 0] View source #/ ( other : Tensor ) # Divides a number by a Tensor . The number is broadcasted across all elements of the Tensor . Arguments # other : Tensor - Tensor on which to perform the operation Examples # a = [ 3 , 3 , 3 ]. to_tensor 3 / a # => [1, 1, 1] View source","title":"Number"},{"location":"Number/#Number","text":"The top-level number type.","title":"Number"},{"location":"Number/#Number-methods","text":"","title":"Methods"},{"location":"Number/#Number#*(other)","text":"Multiplies a Tensor with a number. The number is broadcasted across all elements of the Tensor .","title":"#*"},{"location":"Number/#Number#**(other)","text":"Raises a number to a Tensor . The number is broadcasted across all elements of the Tensor .","title":"#**"},{"location":"Number/#Number#+(other)","text":"Adds a Tensor to a number. The number is broadcasted across all elements of the Tensor .","title":"#+"},{"location":"Number/#Number#-(other)","text":"Subtracts a Tensor from a number. The number is broadcasted across all elements of the Tensor .","title":"#-"},{"location":"Number/#Number#/(other)","text":"Divides a number by a Tensor . The number is broadcasted across all elements of the Tensor .","title":"#/"},{"location":"OCL/","text":"class OCL(T) inherits Num::Backend::Storage # Constructors # .new ( shape : Array ( Int ), order : Num::OrderType , value : T ) # Initialize an OpenCL storage from an initial capacity and an initial value, which will fill the buffer Arguments # shape : Array(Int) - Shape for parent Tensor order : Num::OrderType - Memory layout for parent Tensor value : T - Value to initially populate the buffer Examples # OCL . new ( [ 10 , 10 ] , Num :: RowMajor , 3.4 ) View source .new ( shape : Array ( Int ), strides : Array ( Int ), value : T ) # Initialize an OpenCL storage from an initial capacity and an initial value, which will fill the buffer Arguments # shape : Array(Int) - Shape for parent Tensor strides : Array(Int) - Strides for parent Tensor value : T - Value to initially populate the buffer Examples # OCL . new ( [ 10 , 10 ] , [ 10 , 1 ] , 3.4 ) View source .new ( hostptr : Pointer ( T ), shape : Array ( Int ), strides : Array ( Int )) # Initialize an OpenCL storage from a standard library Crystal pointer Arguments # hostptr : Pointer(T) - Stdlib Crystal pointer containing the Tensor s data shape : Array(Int) - Shape for parent Tensor strides : Array(Int) - Strides for parent Tensor Examples # ptr = Pointer ( Int32 ) . malloc ( 9 ) OCL . new ( ptr , [ 3 , 3 ] , [ 3 , 1 ] ) View source .new ( shape : Array ( Int ), order : Num::OrderType ) # Initialize an OpenCL storage from an initial capacity. The data will be filled with zeros Arguments # shape : Array(Int) - Shape for parent Tensor order : Num::OrderType - Memory layout for parent Tensor Examples # OCL . new ( [ 100 ] , Num :: RowMajor ) View source .new ( shape : Array ( Int ), strides : Array ( Int )) # Initialize an OpenCL storage from an initial capacity. The data will be filled with zeros Arguments # shape : Array(Int) - Shape for parent Tensor strides : Array(Int) - Strides for parent Tensor Examples # OCL . new ( [ 100 ] , [ 1 ] ) View source Class methods # .base ( dtype : U . class ) : OCL ( U ) . class forall U # Return a generic class of a specific generic type, to allow for explicit return types in functions that return a different storage type than the parent Tensor a = OCL ( Float32 ) . new ( [ 10 ] ) # Cannot do # a.class.new ... a . class . base ( Float64 ) . new ( [ 10 ] ) View source Methods # #data : LibCL :: ClMem # Data buffer containing the data associated with the parent Tensor View source #finalize # Releases the underlying LibCL::ClMem buffers containing the data for a Tensor , as well as the buffers containing the shape and strides for the Tensor View source #shape : LibCL :: ClMem # Data buffer containing the shape associated with the parent Tensor View source #strides : LibCL :: ClMem # Data buffer containing the strides associated with the parent Tensor View source #to_unsafe : LibCL :: ClMem # Returns the underlying OpenCL memory buffer associated with a Tensor View source #total_size : Int32 # Total size of the underlying data buffer. Keeps track of the total size of a buffer if a Tensor has been sliced View source #update_metadata ( shape : Array ( Int32 ), strides : Array ( Int32 )) # View source","title":"OCL"},{"location":"OCL/#OCL","text":"","title":"OCL"},{"location":"OCL/#OCL-constructors","text":"","title":"Constructors"},{"location":"OCL/#OCL.new(shape,order,value)","text":"Initialize an OpenCL storage from an initial capacity and an initial value, which will fill the buffer","title":".new"},{"location":"OCL/#OCL-class-methods","text":"","title":"Class methods"},{"location":"OCL/#OCL.base(dtype)","text":"Return a generic class of a specific generic type, to allow for explicit return types in functions that return a different storage type than the parent Tensor a = OCL ( Float32 ) . new ( [ 10 ] ) # Cannot do # a.class.new ... a . class . base ( Float64 ) . new ( [ 10 ] ) View source","title":".base"},{"location":"OCL/#OCL-methods","text":"","title":"Methods"},{"location":"OCL/#OCL#data","text":"Data buffer containing the data associated with the parent Tensor View source","title":"#data"},{"location":"OCL/#OCL#finalize","text":"Releases the underlying LibCL::ClMem buffers containing the data for a Tensor , as well as the buffers containing the shape and strides for the Tensor View source","title":"#finalize"},{"location":"OCL/#OCL#shape","text":"Data buffer containing the shape associated with the parent Tensor View source","title":"#shape"},{"location":"OCL/#OCL#strides","text":"Data buffer containing the strides associated with the parent Tensor View source","title":"#strides"},{"location":"OCL/#OCL#to_unsafe","text":"Returns the underlying OpenCL memory buffer associated with a Tensor View source","title":"#to_unsafe"},{"location":"OCL/#OCL#total_size","text":"Total size of the underlying data buffer. Keeps track of the total size of a buffer if a Tensor has been sliced View source","title":"#total_size"},{"location":"OCL/#OCL#update_metadata(shape,strides)","text":"View source","title":"#update_metadata"},{"location":"Tensor/","text":"class Tensor(T, S) inherits Reference # Included modules Enumerable Constructors # .new ( data : S , shape : Array ( Int32 ), strides : Array ( Int32 ), offset : Int32 , flags : Num::ArrayFlags , dtype : T . class = T ) # Initialize a Tensor from a storage instance, a shape, strides, an offset, flags, and a data type. This should primarily be used by internal methods, since it assumes the passed shape and strides correspond to the storage provided. The dtype is required to infer T without having it explicitly provided View source .new ( data : S , shape : Array ( Int ), order : Num::OrderType = Num :: RowMajor , dtype : T . class = T ) # Initialize a Tensor from a storage instance, a shape, an order, and a data type. This should primarily be used by internal methods, since it assumes the contiguity of the storage. The dtype is required to infer T without having it explicitly provided View source .new ( data : S , shape : Array ( Int32 ), strides : Array ( Int32 ), offset : Int32 , dtype : T . class = T ) # Initialize a Tensor from a storage instance, a shape, strides, an offset, and a data type. This should primarily be used by internal methods, since it assumes the passed shape and strides correspond to the storage provided. The dtype is required to infer T without having it explicitly provided View source .new ( shape : Array ( Int ), order : Num::OrderType = Num :: RowMajor ) # Initializes a Tensor onto a device with a provided shape and memory layout. Examples # a = Tensor ( Float32 ) . new ( [ 3 , 3 , 2 ] , device : OCL ( Float32 )) # => GPU Tensor b = Tensor ( Float32 ) . new ( [ 2 , 3 , 4 ] ) # => CPU Tensor View source .new ( shape : Array ( Int ), value : T , device = CPU ( T ), order : Num::OrderType = Num :: RowMajor ) # Initializes a Tensor onto a device with a provided shape and memory layout, containing a specified value. Examples # a = Tensor . new ( [ 2 , 2 ] , 3.5 ) # => CPU Tensor filled with 3.5 View source .new ( shape : Array ( Int ), order : Num::OrderType = Num :: RowMajor , device = CPU , & block : Int32 -> T ) # Creates a Tensor from a block onto a specified device. The type of the Tensor is inferred from the return type of the block Examples # a = Tensor . new ( [ 3 , 3 , 2 ] ) { | i | i } # => Int32 Tensor stored on a CPU View source .new ( m : Int , n : Int , device = CPU , & block : Int32 , Int32 -> T ) # Creates a matrix Tensor from a block onto a specified device. The type of the Tensor is inferred from the return type of the block Examples # a = Tensor . new ( 3 , 3 ) { | i , j | i / j } # => Float64 Tensor stored on a CPU View source Class methods # .beta ( shape : Array ( Int ), a : Float , b : Float ) # Generates a Tensor containing a beta-distribution collection of values Arguments # shape : Array(Int) - Shape of output Tensor a : Float - Shape parameter of distribution b : Float - Shape parameter of distribution Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . beta ( [ 5 ] , 0.1 , 0.5 ) puts a # => [0.000463782, 0.40858 , 1.67573e-07, 0.143055, 3.08452e-08] View source .binomial ( shape : Array ( Int ), n : Int , prob : Float ) : Tensor ( T , S ) # Draw samples from a binomial distribution. Samples are drawn from a binomial distribution with specified parameters, n trials and prob probability of success where n an integer >= 0 and p is in the interval [0,1]. Arguments # shape : Array(Int) - Shape of output Tensor n : Int - Number of trials prob : Float - Probability of success on a single trial Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . binomial ( [ 5 ] , 50 , 0.5 ) puts a # => [23, 30, 22, 18, 28] View source .chisq ( shape : Array ( Int ), df : Float ) # Generates a Tensor containing chi-square distributed values Arguments # shape : Array(Int) - Shape of output Tensor df : Float - Degrees of freedom Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . chisq ( [ 5 ] , 30.0 ) puts a # => [32.2738, 27.2351, 26.0258, 22.136 , 31.9774] View source .exp ( shape : Array ( Int ), scale : Float = 1.0 ) # Generates a Tensor containing expontentially distributed values Arguments # shape : Array(Int) - Shape of output Tensor scale : Float - Scale of the distribution Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . exp ( [ 5 ] ) puts a # => [0.697832 , 0.710307 , 1.35733 , 0.0423776, 0.209743 ] View source .eye ( m : Int , n : Int? = nil , offset : Int = 0 ) # Return a two-dimensional Tensor with ones along the diagonal, and zeros elsewhere Arguments # m : Int - Number of rows in the returned Tensor n : Int? - Number of columns in the Tensor , defaults to m if nil offset : Int - Indicates which diagonal to fill with ones Examples # Tensor ( Int8 , CPU ( Int8 )) . eye ( 3 , offset : - 1 ) # [[0, 0, 0], # [1, 0, 0], # [0, 1, 0]] Tensor ( Int8 , CPU ( Int8 )) . eye ( 2 ) # [[1, 0], # [0, 1]] View source .from_array ( a : Array , device = CPU ) # Creates a Tensor from a standard library array onto a specified device. The type of Tensor is inferred from the innermost element type, and the Array's shape must be uniform along all subarrays. Examples # a = [[ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ]] Tensor . from_array ( a , device : OCL ) # => [3, 2] Tensor stored on a GPU View source .from_npy ( path : String ) : Tensor ( T , S ) # Reads a .npy file and returns a Tensor of the specified type. If the ndarray is stored in a different type inside the file, it will be converted. Arguments # path : String - Filename of npy file to load Note Only integer, unsigned integer and float ndarrays are supported at the moment. NOTE: Only little endian files can be read due to laziness by the num.cr developer View source .fsned ( shape : Array ( Int ), df1 : Float , df2 : Float ) : Tensor ( T , S ) # Generates a Tensor containing f-snedecor distributed values Arguments # shape : Array(Int) - Shape of output Tensor df1 : Float - Degrees of freedom of the underlying chi-square distribution, numerator side; usually mentioned as m. df2 : Float - Degrees of freedom of the underlying chi-square distribution, denominator side; usually mentioned as n. Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . fsned ( [ 5 ] , 30.0 , 50.0 ) puts a # => [1.15436 , 1.08983 , 0.971573, 1.75811 , 2.06518 ] View source .full ( shape : Array ( Int ), value : Number ) : Tensor ( T , S ) # Creates a Tensor of a provided shape, filled with a value. The generic type is inferred from the value Arguments # shape : Array(Int) - shape of returned Tensor Examples # t = Tensor ( Int8 , CPU ( Int8 )) . full ( [ 3 ] , 1 ) # => [1, 1, 1] View source .full_like ( t : Tensor , value : Number ) : Tensor ( T , S ) # Creates a Tensor filled with a value, sharing the shape of another provided Tensor Arguments # t : Tensor - Tensor to use for output shape Examples # t = Tensor ( Int8 , CPU ( Int8 )) &. to_f u = Tensor ( Int8 , CPU ( Int8 )) . full_like ( t , 3 ) # => [3, 3, 3] View source .gamma ( t_shape : Array ( Int ), shape : Float , scale : Float = 1.0 ) : Tensor ( T , S ) # Generate a gamma-distributed, pseudo-random Tensor Arguments # t_shape : Array(Int) - Shape of output Tensor shape : Float - shape parameter of the distribution; usually mentioned as k scale : Float - scale parameter of the distribution; usually mentioned as \u03b8 Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . gamma ( [ 5 ] , 0.5 ) puts a # => [0.169394 , 0.0336937, 0.921517 , 0.0210972, 0.0487926] View source .geometric_space ( start : T , stop : T , num : Int = 50 , endpoint : Bool = true , device = CPU ) # Return numbers spaced evenly on a log scale (a geometric progression). This is similar to logspace , but with endpoints specified directly. Each output sample is a constant multiple of the previous. Arguments # start : T - Start of interval stop : T - End of interval num : Int - Number of samples endpoint : Bool - Indicates if endpoint should be included in the results device : Num::Storage Examples # Tensor . geometric_space ( 1_f32 , 1000_f32 , 4 ) # => [1, 10, 100, 1000] View source .identity ( n : Int ) # Returns an identity Tensor with ones along the diagonal, and zeros elsewhere Arguments # n : Int - Number of rows and columns in output Examples # Tensor ( Int8 , CPU ( Int8 )) . identity ( 2 ) # [[1, 0], # [0, 1]] View source .laplace ( shape : Array ( Int ), loc : Float = 0.0 , scale : Float = 1.0 ) : Tensor ( T , S ) # Generate a laplace-distributed, pseudo-random Tensor Arguments # shape : Array(Int) - Shape of output Tensor loc : Float - Centrality parameter, or mean of the distribution; usually mentioned as \u03bc scale : Float - scale parameter of the distribution; usually mentioned as b Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . laplace ( [ 5 ] , 0.5 ) puts a # => [0.305384 , 0.601509 , 0.247952 , -3.34791 , -0.502075] View source .linear_space ( start : T , stop : T , num : Int = 50 , endpoint = true , device = CPU ) # Return evenly spaced numbers over a specified interval. Returns num evenly spaced samples, calculated over the interval [ start , stop ]. The endpoint of the interval can optionally be excluded. Arguments # start : T - Start of interval stop : T - End of interval num : Int - Number of samples endpoint : Bool - Indicates if endpoint of the interval should be included in the results device : Num::Storage - Backend for the Tensor Examples # Tensor . linspace ( 0_f32 , 1_f32 , 5 ) # => [0.0, 0.25, 0.5, 0.75, 1.0] Tensor . linspace ( 0_f32 , 1_f32 , 5 , endpoint : false ) # => [0.0, 0.2, 0.4, 0.6, 0.8] View source .logarithmic_space ( start : T , stop : T , num = 50 , endpoint = true , base : T = T . new ( 10.0 ), device = CPU ) # Return numbers spaced evenly on a log scale. In linear space, the sequence starts at base ** start ( base to the power of start ) and ends with base ** stop (see endpoint below). Arguments # start : T - Start of interval stop : T - End of interval num : Int - Number of samples endpoint : Bool - Indicates if endpoint should be included in the results device : Num::Storage - Backend for the Tensor Examples # Tensor . logarithmic_space ( 2.0 , 3.0 , num : 4 ) # [100 , 215.443, 464.159, 1000 ] View source .lognormal ( shape : Array ( Int ), loc : Float = 0.0 , sigma : Float = 1.0 ) : Tensor ( T , S ) # Generate a log-normal-distributed, pseudo-random Tensor Arguments # shape : Array(Int) - Shape of output Tensor loc : Float - centrality parameter, or mean of the underlying normal distribution; usually mentioned as \u03bc sigma : Float - scale parameter, or standard deviation of the underlying normal distribution; usually mentioned as \u03c3 Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . lognormal ( [ 5 ] , 0.5 ) puts a # => [1.41285 , 5.00594 , 0.766401, 1.61069 , 2.29073 ] View source .normal ( shape : Array ( Int ), loc = 0.0 , sigma = 1.0 ) : Tensor ( T , S ) # Generate a Tensor containing a normal-distribution collection of values Arguments # shape : Array(Int) - Shape of Tensor to create loc : Float - Centrality parameter sigma : Float - Standard deviation Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . normal ( [ 5 ] , 0.5 ) puts a # => 0.345609, 1.61063 , -0.26605, 0.476662, 0.828871] View source .ones ( shape : Array ( Int )) : Tensor ( T , S ) # Creates a Tensor of a provided shape, filled with 1. The generic type must be specified. Arguments # shape : Array(Int) - shape of returned Tensor Examples # t = Tensor ( Int8 , CPU ( Int8 )) . ones ( [ 3 ] ) # => [1, 1, 1] View source .ones_like ( t : Tensor ) : Tensor ( T , S ) # Creates a Tensor filled with 1, sharing the shape of another provided Tensor Arguments # t : Tensor - Tensor to use for output shape Examples # t = Tensor ( Int8 , CPU ( Int8 )) &. to_f u = Tensor ( Int8 , CPU ( Int8 )) . ones_like ( t ) # => [0, 0, 0] View source .poisson ( shape : Array ( Int ), lam : Float = 1.0 ) : Tensor ( T , S ) # Generate a poisson-distributed, pseudo-random Tensor(Int64) Arguments # shape : Array(Int) - Shape of output Tensor lam : Float - Separation parameter of the distribution; usually mentioned as \u03bb Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Int64 , CPU ( Int64 )) . poisson ( [ 5 ] ) puts a # => [1, 0, 1, 0, 3] View source .rand ( shape : Array ( Int )) # Generate random floating point values between 0 and 1 Arguments # shape : Array(Int) Shape of Tensor to generate Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . rand ( [ 5 ] ) puts a # => [0.411575 , 0.548264 , 0.388604 , 0.0106621, 0.183558 ] View source .random ( r : Range ( U , U ), shape : Array ( Int ), device = CPU ) forall U # Creates a Tensor sampled from a provided range, with a given shape. The generic types of the Tensor are inferred from the endpoints of the range Arguments # r : Range(U, U) - Range of values to sample between shape : Array(Int) - Shape of returned Tensor Examples # Num :: Rand . set_seed ( 0 ) t = Tensor . random ( 0 ... 10 , [ 2 , 2 ] ) t # [[8, 4], # [7, 4]] View source .range ( start : T , stop : T , step : T , device = CPU ) # Creates a flat Tensor containing a monotonically increasing or decreasing range. The generic type is inferred from the inputs to the method Arguments # start : T - Beginning value for the range stop : T - End value for the range step : T - Offset between values of the range Examples # Tensor . range ( 0 , 5 , 2 ) # => [0, 2, 4] Tensor . range ( 5 , 0 , - 1 ) # => [5, 4, 3, 2, 1] Tensor . range ( 0.0 , 3.5 , 0.7 ) # => [0 , 0.7, 1.4, 2.1, 2.8] View source .range ( start : T , stop : T , device = CPU ) # Creates a flat Tensor containing a monotonically increasing or decreasing range. The generic type is inferred from the inputs to the method Arguments # start : T - Beginning value for the range stop : T - End value for the range step : T - Offset between values of the range Examples # Tensor . range ( 0 , 5 , 2 ) # => [0, 2, 4] Tensor . range ( 5 , 0 , - 1 ) # => [5, 4, 3, 2, 1] Tensor . range ( 0.0 , 3.5 , 0.7 ) # => [0 , 0.7, 1.4, 2.1, 2.8] View source .range ( stop : T , device = CPU ) # Creates a flat Tensor containing a monotonically increasing or decreasing range. The generic type is inferred from the inputs to the method Arguments # start : T - Beginning value for the range stop : T - End value for the range step : T - Offset between values of the range Examples # Tensor . range ( 0 , 5 , 2 ) # => [0, 2, 4] Tensor . range ( 5 , 0 , - 1 ) # => [5, 4, 3, 2, 1] Tensor . range ( 0.0 , 3.5 , 0.7 ) # => [0 , 0.7, 1.4, 2.1, 2.8] View source .t_student ( shape : Array ( Int ), df : Float ) : Tensor ( T , S ) # Generate a t-student-distributed, pseudo-random Tensor Arguments # shape : Array(Int) - Shape of output Tensor df : Float - degrees of freedom of the distribution; usually mentioned as n Examples # Num :: Rand . set_seed ( 0 ) a = Tensor ( Float32 , CPU ( Float32 )) . t_student ( [ 5 ] , 30.0 ) puts a # => [-0.148853, -0.803994, 0.353089 , -1.25613 , -0.141144] View source .zeros ( shape : Array ( Int )) : Tensor ( T , S ) # Creates a Tensor of a provided shape, filled with 0. The generic type must be specified. Arguments # shape : Array(Int) - shape of returned Tensor Examples # t = Tensor ( Int8 ) . zeros ( [ 3 ] ) # => [0, 0, 0] View source .zeros_like ( t : Tensor ) : Tensor ( T , S ) # Creates a Tensor filled with 0, sharing the shape of another provided Tensor Arguments # t : Tensor - Tensor to use for output shape Examples # t = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 ] ) &. to_f u = Tensor ( Int8 , CPU ( Int8 )) . zeros_like ( t ) # => [0, 0, 0] View source Methods # #!= ( other ) # Implements the != operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a != a View source #% ( other ) # Return element-wise remainder of division for two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a % a View source #& ( other ) # Compute the bit-wise AND of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a & a View source #* ( other ) # Multiplies two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a * a View source #** ( other ) # Exponentiates two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a ** a View source #+ ( other ) # Adds two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a + a View source #- ( other ) # Subtracts two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a - a View source #/ ( other ) # Divides two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a / a View source #// ( other ) # Floor divides two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a // a View source #< ( other ) # Implements the < operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a < a View source #<< ( other ) # Shift the bits of an integer to the left. Bits are shifted to the left by appending x2 0s at the right of x1. Since the internal representation of numbers is in binary format, this operation is equivalent to multiplying x1 by 2**x2. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a << a View source #<= ( other ) # Implements the <= operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a <= a View source #== ( other ) # Implements the == operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a == a View source #> ( other ) # Implements the > operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a > a View source #>= ( other ) # Implements the >= operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a >= a View source #>> ( other ) # Shift the bits of an integer to the right. Bits are shifted to the right x2. Because the internal representation of numbers is in binary format, this operation is equivalent to dividing x1 by 2**x2. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a >> a View source #[] ( args : Array ) : Tensor ( T , S ) # Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source #[] ( * args ) : Tensor ( T , S ) # Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source #[]= ( args : Array , value ) # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # args : Array - Array of arguments. All but the last argument must be valid indexer, so a Range , Int , or Tuple(Range, Int) . value : Tensor | Number - Value to assign to the slice Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #[]= ( * args : * U ) forall U # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # args : *U - Tuple of arguments. All but the last argument must be valid indexer, so a Range , Int , or Tuple(Range, Int) . The final argument passed is used to set the values of the Tensor . It can be either a Tensor , or a scalar value. Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #^ ( other ) # Compute the bit-wise XOR of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a ^ a View source #accumulate ( & block : T , T -> T ) : Tensor ( T , S ) # Returns a Tensor containing the successive values of applying a binary operation, specified by the given block , to this Tensor's elements. For each element in the Tensor the block is passed an accumulator value and the element. The result becomes the new value for the accumulator and is also appended to the returned Tensor. The initial value for the accumulator is the first element in the Tensor. Examples # [ 2 , 3 , 4 , 5 ].. to_tensor . accumulate { | x , y | x * y } # => [2, 6, 24, 120] View source #acos # Trigonometric inverse cosine, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . acos View source #acosh # Inverse hyperbolic cosine, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . acos View source #add ( other ) # Adds two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a + a View source #all ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, asserting the truthiness of all values in each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . all ( 0 ) # => [false, true] a . all ( 1 , dims : true ) # [[false], # [ true]] View source #all : Bool # Reduces a Tensor to a boolean by asserting the truthiness of all elements Examples # a = [ 0 , 2 , 3 ]. to_tensor a . all # => false View source #all_close ( other : Tensor , epsilon : Float = 1e-6 ) : Bool # Asserts that two Tensor s are equal, allowing for small margins of errors with floating point values using an EPSILON value. Arguments # other : Tensor - Tensor to compare to self epsilon : Float - Margin of error to accept between elements Examples # a = [ 0.0 , 0.0 , 0.0000000001 ]. to_tensor b = [ 0.0 , 0.0 , 0.0 ]. to_tensor a . all_close ( b ) # => true a . all_close ( b , 1e-12 ) # => false View source #any ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, asserting the truthiness of any values in each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . any ( 0 ) # => [true, true] a . any ( 1 , dims : true ) # [[true], # [ true]] View source #any : Bool # Reduces a Tensor to a boolean by asserting the truthiness of any element Examples # a = [ 0 , 2 , 3 ]. to_tensor a . any # => true View source #argmax ( axis : Int , dims : Bool = false ) # Find the maximum index value of a Tensor along an axis Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 4 , 2 ] , [ 0 , 1 ]]. to_tensor a . argmax ( 1 ) # => [0, 1] View source #argmax : Int32 # Find the maximum index value of a Tensor Examples # a = [ 1 , 2 , 3 ]. to_tensor a . argmax # => 2 View source #argmin ( axis : Int , dims : Bool = false ) # Find the minimum index value of a Tensor along an axis Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 4 , 2 ] , [ 0 , 1 ]]. to_tensor a . argmin ( 1 ) # => [1, 0] View source #argmin : Int32 # Find the minimum index value of a Tensor Examples # a = [ 1 , 2 , 3 ]. to_tensor a . argmin # => 0 View source #as_strided ( shape : Array ( Int ), strides : Array ( Int )) : Tensor ( T , S ) # as_strided creates a view into the Tensor given the exact strides and shape. This means it manipulates the internal data structure of a Tensor and, if done incorrectly, the array elements can point to invalid memory and can corrupt results or crash your program. It is advisable to always use the original strides when calculating new strides to avoid reliance on a contiguous memory layout. Furthermore, Tensor s created with this function often contain self overlapping memory, so that two elements are identical. Vectorized write operations on such Tensor s will typically be unpredictable. They may even give different results for small, large, or transposed Tensor s. Arguments # shape : Array(Int) - Shape of the new Tensor strides : Array(Int) - Strides of the new Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . as_strided ( [ 3 , 3 ] , [ 0 , 1 ] ) # [[1, 2, 3], # [1, 2, 3], # [1, 2, 3]] View source #as_type ( dtype : U . class ) forall U # Converts a Tensor to a given dtype. No rounding is done on floating point values. Arguments # dtype : U.class - desired data type of the returned Tensor Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a . as_type ( Int32 ) # => [1, 2, 3] View source #asin # Inverse sine, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . asin View source #asinh # Inverse hyperbolic sine, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . asinh View source #atan # Inverse tangent, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . atan View source #atanh # Inverse hyperbolic tangent, element-wise. Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . atanh View source #besselj0 # Calculates besselj0, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . besselj0 View source #besselj1 # Calculates besselj1, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . besselj1 View source #bessely0 # Calculates bessely0, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . bessely0 View source #bessely1 # Calculates bessely1, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . bessely1 View source #bitwise_and ( other ) # Compute the bit-wise AND of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a & a View source #bitwise_or ( other ) # Compute the bit-wise OR of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a | a View source #bitwise_xor ( other ) # Compute the bit-wise XOR of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a ^ a View source #broadcast ( other1 : Tensor ( U , V ), other2 : Tensor ( W , X )) forall U , V , W , X # Broadcasts three Tensor 's' to a new shape. This allows for elementwise operations between the three Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error. Arguments # other1 : Tensor - Tensor to broadcast other2 : Tensor - Additional Tensor to broadcast Examples # a = Tensor . from_array [ 1 , 2 , 3 ] b = Tensor . new ( [ 3 , 3 ] ) { | i | i } c = Tensor . new ( [ 3 , 3 , 3 ] ) { | i | i } x , y , z = a . broadcast ( b , c ) x . shape # => [3, 3, 3] View source #broadcast ( other : Tensor ( U , V )) forall U , V # Broadcasts two Tensor 's' to a new shape. This allows for elementwise operations between the two Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error. Arguments # other : Tensor - RHS of the broadcast Examples # a = Tensor . from_array [ 1 , 2 , 3 ] b = Tensor . new ( [ 3 , 3 ] ) { | i | i } x , y = a . broadcast ( b ) x . shape # => [3, 3] View source #broadcast_to ( shape : Array ( Int )) : Tensor ( T , S ) # Broadcasts a Tensor to a new shape. Returns a read-only view of the original Tensor . Many elements in the Tensor will refer to the same memory location, and the result is rarely contiguous. Shapes must be broadcastable, and an error will be raised if broadcasting fails. Arguments # shape : Array(Int) - The shape of the desired output Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . broadcast_to ( [ 3 , 3 ] ) # [[1, 2, 3], # [1, 2, 3], # [1, 2, 3]] View source #cbrt # Calculates cube root, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . cbrt View source #cholesky! ( * , lower = true ) # Cholesky decomposition. Return the Cholesky decomposition, L * L.H, of the square matrix a, where L is lower-triangular and .H is the conjugate transpose operator (which is the ordinary transpose if a is real-valued). a must be Hermitian (symmetric if real-valued) and positive-definite. Only L is actually returned. Arguments # lower : Bool - Which triangular of decomposition to return Examples # t = [[ 2 , - 1 , 0 ] , [- 1 , 2 , - 1 ] , [ 0 , - 1 , 2 ]]. to_tensor . astype ( Float32 ) t . cholesky # [[ 1.414, 0.0, 0.0], # [-0.707, 1.225, 0.0], # [ 0.0, -0.816, 1.155]] View source #cos # Calculates cosine, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . cos View source #cosh # Calculates hyperbolic cosine, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . cosh View source #cpu : Tensor ( T , CPU ( T )) # Places a Tensor onto a CPU backend. No copy is done if the Tensor is already on a CPU Arguments # Examples # a = Tensor ( Float32 , OCL ( Float32 )) . ones ( [ 3 ] ) a . cpu # => [1, 1, 1] View source #data : S # View source #det # Compute the determinant of an array. Examples # t = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor . as_type ( Float32 ) puts t . det # => -2.0 View source #diagonal : Tensor ( T , S ) # Returns a view of the diagonal of a Tensor . This method only works for two-dimensional arrays. Todo Implement views for offset diagonals Examples # a = Tensor . new ( 3 , 3 ) { | i , _ | i } a . diagonal # => [0, 1, 2] View source #divide ( other ) # Divides two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a / a View source #dot ( u : Tensor ( T , S )) # DOT forms the dot product of two vectors. Uses unrolled loops for increments equal to one. Arguments # u : Tensor - Right hand side of the dot product Examples # a = [ 1 , 2 , 3 , 4 , 5 ]. to_tensor a . dot ( a ) # => 55.0 View source #dup ( order : Num::OrderType = Num :: RowMajor ) # Deep-copies a Tensor . If an order is provided, the returned Tensor 's memory layout will respect that order. Arguments # order : Num::OrderType - Memory layout to use for the returned Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . dup # => [1, 2, 3] View source #each # Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each do | el | puts el end # 0 # 1 # 2 # 3 View source #each # Yields the elements of a Tensor lazily, always in RowMajor order, as if the Tensor was flat. Examples # a = Tensor . new ( 2 , 2 ) { | i | i } iter = a . each a . size . times do puts iter . next . value end # 0 # 1 # 2 # 3 View source #each_axis ( axis : Int , dims : Bool = false # Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] ) { | i | i } a . each_axis ( 1 ) do | ax | puts ax end # [0, 3, 6] # [1, 4, 7] # [2, 5, 8] View source #each_axis ( axis : Int , dims : Bool = false ) # Returns an iterator along each element of an axis . Each element returned by the iterator is a view, not a copy Arguments # axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] ) { | i | i } a . each_axis ( 1 ) . next # => [0, 3, 6] View source #each_pointer # Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_pointer do | el | puts el . value end # 0 # 1 # 2 # 3 View source #each_pointer_with_index # Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. Also yields the flat index of a Tensor This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_pointer_with_index do | el , i | puts \" #{ el . value } _ #{ i } \" end # 0_0 # 1_1 # 2_2 # 3_3 View source #each_with_index # Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Also yields the flat index of each element. Examples # a = Tensor . new ( 2 , 2 ) { | i | i } a . each_with_index do | el , i | puts \" #{ el } _ #{ i } \" end # 0_0 # 1_1 # 2_2 # 3_3 View source #eig # Compute the eigenvalues and right eigenvectors of a square array. Examples # t = [[ 0 , 1 ] , [ 1 , 1 ]]. to_tensor . as_type ( Float32 ) w , v = t . eig puts w puts v # [-0.618034, 1.61803 ] # [[-0.850651, 0.525731 ], # [-0.525731, -0.850651]] View source #eigh # Compute the eigenvalues and right eigenvectors of a square Tensor . Examples # t = [[ 0 , 1 ] , [ 1 , 1 ]]. to_tensor . as_type ( Float32 ) w , v = t . eigh puts w puts v # [-0.618034, 1.61803 ] # [[-0.850651, 0.525731 ], # [0.525731 , 0.850651 ]] View source #eigvals # Compute the eigenvalues of a general matrix. Main difference between eigvals and eig: the eigenvectors aren\u2019t returned. Examples # t = [[ 0 , 1 ] , [ 1 , 1 ]]. to_tensor . as_type ( Float32 ) puts t . eigvals # [-0.618034, 1.61803 ] View source #eigvalsh # Compute the eigenvalues of a symmetric matrix. Main difference between eigvals and eig: the eigenvectors aren\u2019t returned. Examples # t = [[ 0 , 1 ] , [ 1 , 1 ]]. to_tensor . as_type ( Float32 ) puts t . eigvalsh # [-0.618034, 1.61803 ] View source #equal ( other ) # Implements the == operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a == a View source #erf # Calculates erf, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . erf View source #erfc # Calculates erfc, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . erfc View source #exp # Calculates exp, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . exp View source #exp2 # Calculates exp2, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . exp2 View source #expand_dims ( axis : Int ) : Tensor ( T , S ) # Expands the dimensions of a Tensor , along a single axis Arguments # axis : Int - Axis on which to expand dimensions Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . expand_dims ( 1 ) # [[[0, 1]], # # [[2, 3]]] View source #expm1 # Calculates expm1, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . expm1 View source #flags : Num::ArrayFlags # Returns the flags of a Tensor, describing its memory and read status Examples # a = Tensor ( Float32 , CPU ( Float32 )) . new ( [ 2 , 3 , 4 ] ) b = a [... , 1 ] a . flags # => CONTIGUOUS | OWNDATA | WRITE b . flags # => WRITE View source #flat : Tensor ( T , S ) # Flattens a Tensor to a single dimension. If a view can be created, the reshape operation will not copy data. Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . flat # => [0, 1, 2, 3] View source #flip ( axis : Int ) : Tensor ( T , S ) # Flips a Tensor along an axis, returning a view Arguments # axis : Int - Axis to flip Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] a . flip ( 1 ) # [[3, 2, 1], # [6, 5, 4]] View source #flip : Tensor ( T , S ) # Flips a Tensor along all axes, returning a view Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] a . flip # [[6, 5, 4], # [3, 2, 1]] View source #floordiv ( other ) # Floor divides two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a // a View source #gamma # Calculates gamma function, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . gamma View source #greater ( other ) # Implements the > operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a > a View source #greater_equal ( other ) # Implements the >= operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a >= a View source #hessenberg # Compute Hessenberg form of a matrix. The Hessenberg decomposition is: A = Q H Q ^ H where Q is unitary/orthogonal and H has only zero elements below the first sub-diagonal. Examples # a = [[ 2 , 5 , 8 , 7 ] , [ 5 , 2 , 2 , 8 ] , [ 7 , 5 , 6 , 6 ] , [ 5 , 4 , 4 , 8 ]]. to_tensor . as_type ( Float64 ) puts a . hessenberg # [[2 , -11.6584, 1.42005 , 0.253491], # [-9.94987, 14.5354 , -5.31022, 2.43082 ], # [0 , -1.83299, 0.3897 , -0.51527], # [0 , 0 , -3.8319 , 1.07495 ]] View source #ilogb # Calculates ilogb, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . ilogb View source #inv # Compute the (multiplicative) inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0]) Examples # t = [[ 1 , 2 ] , [ 3 , 4 ]]. to_tensor . as_type ( Float32 ) puts t . inv # [[-2 , 1 ], # [1.5 , -0.5]] View source #left_shift ( other ) # Shift the bits of an integer to the left. Bits are shifted to the left by appending x2 0s at the right of x1. Since the internal representation of numbers is in binary format, this operation is equivalent to multiplying x1 by 2**x2. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a << a View source #less ( other ) # Implements the < operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a < a View source #less_equal ( other ) # Implements the <= operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a <= a View source #lgamma # Calculates logarithmic gamma, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . lgamma View source #log # Calculates log, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . log View source #log10 # Calculates log10, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . log10 View source #log1p # Calculates log1p, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . log1p View source #log2 # Calculates log2, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . log2 View source #logb # Calculates logb, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . logb View source #map ( d1 : Tensor , d2 : Tensor # Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block Arguments # t : Tensor - The second Tensor for iteration. Must be broadcastable against the shape of self and v v : Tensor - The third Tensor for iteration. Must be broadcastable against the shape of self and t block : Proc(T, U, V, W) - The Proc to map across all Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } c = Tensor . new ( [ 3 ] ) { | i | i } a . map ( b , c ) { | i , j , k | i + j + k } # => [0, 3, 6] View source #map ( other : Tensor # Maps a block across two Tensors . This is more efficient than zipping iterators since it iterates both Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block Arguments # t : Tensor - The second Tensor for iteration. Must be broadcastable against the shape of self block : Proc(T, U, V) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } a . map ( b ) { | i , j | i + j } # => [0, 2, 4] View source #map ( & block : T -> U ) forall U # Maps a block across a Tensor . The Tensor is treated as flat during iteration, and iteration is always done in RowMajor order The generic type of the returned Tensor is inferred from the block Arguments # block : Proc(T, U) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . map { | e | e + 5 } # => [5, 6, 7] View source #map! ( d1 : Tensor , d2 : Tensor # Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor 's must broadcast to the shape of self Arguments # t : Tensor - The second Tensor for iteration. Must be broadcastable against the shape of self and v v : Tensor - The third Tensor for iteration. Must be broadcastable against the shape of self and t block : Proc(T, U, V, T) - The Proc to map across all Tensor s Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } c = Tensor . new ( [ 3 ] ) { | i | i } a . map! ( b , c ) { | i , j , k | i + j + k } a # => [0, 3, 6] View source #map! ( other : Tensor # Maps a block across two Tensors . This is more efficient than zipping iterators since it iterates both Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor must broadcast to the shape of self Arguments # t : Tensor - The second Tensor for iteration. Must be broadcastable against the shape of self block : Proc(T, U, T) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } b = Tensor . new ( [ 3 ] ) { | i | i } a . map! ( b ) { | i , j | i + j } a # => [0, 2, 4] View source #map! # Maps a block across a Tensor in place. The Tensor is treated as flat during iteration, and iteration is always done in RowMajor order Arguments # block : Proc(T, U) - Proc to map across the Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . map! { | e | e + 5 } a # => [5, 6, 7] View source #matmul ( other : Tensor ( T , S ), output : Tensor ( T , S )? = nil ) # Computes a matrix multiplication between two Tensors . The Tensor s must be two dimensional with compatible shapes. Currently only Float and Complex Tensor s are supported, as BLAS is used for this operation Arguments # other : Tensor - The right hand side of the operation Examples # Num :: Rand . set_seed ( 0 ) a = Tensor . random ( 0.0 ... 10.0 , [ 3 , 3 ] ) a . matmul ( a ) # [[28.2001, 87.4285, 30.5423], # [12.4381, 30.9552, 26.2495], # [34.0873, 73.5366, 40.5504]] View source #max ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, finding the max of each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . max ( 0 ) # => [2, 3] a . max ( 1 , dims : true ) # [[1], # [3]] View source #max : T # Reduces a Tensor to a scalar by finding the maximum value Examples # a = [ 1 , 2 , 3 ] a . max # => 3 View source #mean ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, finding the average of each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . mean ( 0 ) # => [1, 2] a . mean ( 1 , dims : true ) # [[0], # [2]] View source #mean # Reduces a Tensor to a scalar by finding the average Examples # a = [ 1 , 2 , 3 ] a . mean # => 2.0 View source #min ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, finding the min of each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . min ( 0 ) # => [0, 1] a . min ( 1 , dims : true ) # [[0], # [2]] View source #min : T # Reduces a Tensor to a scalar by finding the minimum value Examples # a = [ 1 , 2 , 3 ] Num . min ( a ) # => 3 View source #modulo ( other ) # Return element-wise remainder of division for two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a % a View source #move_axis ( source : Array ( Int ), destination : Array ( Int )) : Tensor ( T , S ) # Move axes of a Tensor to new positions, other axes remain in their original order Arguments # source : Array(Int) - Original positions of axes to move destination : Array(Int) - Destination positions to permute axes Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 4 , 5 ] ) a . moveaxis ( [ 0 ] , [- 1 ] ) . shape # => 4, 5, 3 View source #move_axis ( source : Int , destination : Int ) : Tensor ( T , S ) # Move axes of a Tensor to new positions, other axes remain in their original order Arguments # source : Int - Original position of axis to move destination : Int - Destination position of axis Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 4 , 5 ] ) a . moveaxis ( 0 , - 1 ) . shape # => 4, 5, 3 View source #multiply ( other ) # Multiplies two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a * a View source #norm ( order = 'F' ) # Matrix norm This function is able to return one of eight different matrix norms Arguments # order : String - Type of norm Examples # t = [[ 0 , 1 ] , [ 1 , 1 ] , [ 1 , 1 ] , [ 2 , 1 ]]. to_tensor . as_type ( Float32 ) t . norm # => 3.6055512 View source #not_equal ( other ) # Implements the != operator for two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a != a View source #offset : Int32 # Returns the offset of a Tensor's data Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 2 , 3 , 4 ] ) a . offset # => 0 View source #opencl : Tensor ( T , OCL ( T )) # Places a Tensor onto an OpenCL backend. No copy is done if the Tensor is already on a CPU Examples # a = Tensor ( Float32 , CPU ( Float32 )) . ones ( [ 3 ] ) a . opencl # => <[3] on OpenCL Backend> View source #power ( other ) # Exponentiates two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a ** a View source #prod ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, multiplying each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . prod ( 0 ) # => [0, 3] a . prod ( 1 , dims : true ) # [[0], # [6]] View source #prod : T # Reduces a Tensor to a scalar by multiplying all of its elements Examples # a = [ 1 , 2 , 3 ] a . prod # => 6 View source #ptp ( axis : Int , dims : Bool = false ) # Finds the difference between the maximum and minimum elements of a Tensor along an axis Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = [[ 3 , 4 ] , [ 1 , 2 ] , [ 6 , 2 ]]. to_tensor a . ptp ( 1 ) # [1, 1, 4] View source #ptp : T # Finds the difference between the maximum and minimum elements of a Tensor Examples # a = [ 1 , 2 , 3 ]. to_tensor a . ptp # => 2 View source #qr # Compute the qr factorization of a matrix. Factor the matrix a as qr, where q is orthonormal and r is upper-triangular Examples # t = [[ 0 , 1 ] , [ 1 , 1 ] , [ 1 , 1 ] , [ 2 , 1 ]]. to_tensor . as_type ( Float32 ) q , r = t . qr puts q puts r # [[ 0.0, 0.866], # [-0.408, 0.289], # [-0.408, 0.289], # [-0.816, -0.289]] # [[-2.449, -1.633], # [ 0.0, 1.155], # [ 0.0, 0.0], # [ 0.0, 0.0]] View source #rank : Int32 # Returns the number of dimensions in a Tensor Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 3 , 3 , 3 ] ) a . rank # => 4 View source #reduce # Just like the other variant, but you can set the initial value of the accumulator. Arguments # memo - Value to start off the accumulator Examples # [ 1 , 2 , 3 , 4 , 5 ]. to_tensor . reduce ( 10 ) { | acc , i | acc + i } # => 25 View source #reduce # Combines all elements in the Tensor by applying a binary operation, specified by a block, so as to reduce them to a single value. For each element in the Tensor the block is passed an accumulator value ( memo ) and the element. The result becomes the new value for memo . At the end of the iteration, the final value of memo is the return value for the method. The initial value for the accumulator is the first element in the Tensor. Raises Enumerable::EmptyError if the Tensor is empty. Examples # [ 1 , 2 , 3 , 4 , 5 ]. to_tensor . reduce { | acc , i | acc + i } # => 15 View source #reduce_axis ( axis : Int , dims : Bool = false # Equivalent of calling reduce on each slice into an axis . Used primarily for reductions like Num.sum , Num.prod , in their axis versions. Arguments # axis : Int - Axis of reduction dims : Bool - Indicates if the axis of reduction should be removed from the result Examples # a = Tensor . new ( [ 3 , 3 ] ) { | i | i } a . reduce_axis ( 0 ) { | i , j | i + j } # => [ 9, 12, 15] View source #repeat ( n : Int , axis : Int ) : Tensor ( T , S ) # Repeat elements of a Tensor along an axis Arguments # n : Int - Number of times to repeat axis : Int - Axis along which to repeat Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] a . repeat ( 2 , 1 ) # [[1, 1, 2, 2, 3, 3], # [4, 4, 5, 5, 6, 6]] View source #repeat ( n : Int ) : Tensor ( T , S ) # Repeat elements of a Tensor , treating the Tensor as flat Arguments # n : Int - Number of times to repeat Examples # a = [ 1 , 2 , 3 ] a . repeat ( 2 ) # => [1, 1, 2, 2, 3, 3] View source #reshape ( shape : Array ( Int )) : Tensor ( T , S ) # Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same. Arguments # result_shape : Array(Int) - Result shape for the Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 , 4 ] a . reshape ( [ 2 , 2 ] ) # [[1, 2], # [3, 4]] View source #reshape ( * shape : Int ) : Tensor ( T , S ) # Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same. Arguments # result_shape : Array(Int) - Result shape for the Tensor Examples # a = Tensor . from_array [ 1 , 2 , 3 , 4 ] a . reshape ( [ 2 , 2 ] ) # [[1, 2], # [3, 4]] View source #right_shift ( other ) # Shift the bits of an integer to the right. Bits are shifted to the right x2. Because the internal representation of numbers is in binary format, this operation is equivalent to dividing x1 by 2**x2. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a >> a View source #set ( * args , value ) # The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice. Arguments # args : Tuple - Tuple of arguments. All must be valid indexers, so a Range , Int , or Tuple(Range, Int) . value : Tensor | Number - Value to assign to the slice Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 1 .. , 1 ..] = 99 a # [[ 0, 1], # [ 2, 99]] View source #shape : Array ( Int32 ) # Returns the size of a Tensor along each dimension Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 2 , 3 , 4 ] ) a . shape # => [2, 3, 4] View source #sin # Calculates sine, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . sin View source #sinh # Calculates hyperbolic sine, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . sinh View source #size : Int32 # Returns the size of a Tensor along each dimension a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 2 , 3 , 4 ] ) a . shape # => [2, 3, 4] View source #slice ( * args ) : Tensor ( T , S ) # Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source #solve ( x : Tensor ( T , S )) # Solve a linear matrix equation, or system of linear scalar equations. Computes the \u201cexact\u201d solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b. Arguments # x : Tensor - Argument with which to solve Examples # a = [[ 3 , 1 ] , [ 1 , 2 ]]. to_tensor . as_type ( Float32 ) b = [ 9 , 8 ]. to_tensor . as_type ( Float32 ) puts a . solve ( b ) # [2, 3] View source #sort ( axis : Int ) : Tensor ( T , S ) # Sorts a Tensor along an axis. Arguments # axis : Int - Axis of reduction Examples # t = Tensor . random ( 0 ... 10 , [ 3 , 3 , 2 ] ) puts t . sort ( axis : 1 ) # [[[1, 1], # [4, 5], # [5, 7]], # # [[0, 0], # [2, 3], # [8, 4]], # # [[2, 5], # [5, 7], # [5, 7]]] View source #sort : Tensor ( T , S ) # Sorts a Tensor , treating it's elements like the Tensor is flat. Examples # a = [ 3 , 2 , 1 ]. to_tensor a . sort # => [1, 2, 3] View source #sort ( axis : Int , & block : T , T -> _ ) # Sorts a Tensor along an axis. Arguments # axis : Int - Axis of reduction Examples # t = Tensor . random ( 0 ... 10 , [ 3 , 3 , 2 ] ) puts t . sort ( axis : 1 ) { | i , j | i <=> j } # [[[5, 3], # [6, 9], # [7, 9]], # # [[0, 1], # [3, 2], # [8, 5]], # # [[3, 1], # [4, 7], # [7, 8]]] View source #sort ( & block : T , T -> _ ) # Sorts a Tensor , treating it's elements like the Tensor is flat. Sorts using criteria specified by a passed block Arguments # block : Proc(T, T, _) - Function used to sort Examples # a = [ 3 , 2 , 1 ]. to_tensor a . sort { | i , j | j - i } # => [3, 2, 1] View source #sqrt # Calculates square root, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . sqrt View source #std ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, finding the std of each view into the Tensor Arguments # axis : Int - Axis of reduction dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . std ( 0 ) # => [1, 1] a . std ( 1 , dims : true ) # [[0.707107], # [0.707107]] View source #std : Float64 # Reduces a Tensor to a scalar by finding the standard deviation Examples # a = [ 1 , 2 , 3 ]. to_tensor a . std # => 0.816496580927726 View source #strides : Array ( Int32 ) # Returns the step of a Tensor along each dimension Examples # a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 3 , 3 , 2 ] ) a . shape # => [4, 2, 1] View source #subtract ( other ) # Subtracts two Tensor s elementwise Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1.5 , 2.2 , 3.2 ] a - a View source #sum ( axis : Int , dims : Bool = false ) # Reduces a Tensor along an axis, summing each view into the Tensor Arguments # axis : Int - Axis of summation dims : Bool - Indicate if the axis of reduction should remain in the result Examples # a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a . sum ( 0 ) # => [2, 4] a . sum ( 1 , dims : true ) # [[1], # [5]] View source #sum : T # Reduces a Tensor to a scalar by summing all of its elements Examples # a = [ 1 , 2 , 3 ] a . sum # => 6 View source #svd # Singular Value Decomposition. When a is a 2D array, it is factorized as u @ np.diag(s) @ vh = (u * s) @ vh, where u and vh are 2D unitary arrays and s is a 1D array of a\u2019s singular values. Examples # t = [[ 0 , 1 ] , [ 1 , 1 ] , [ 1 , 1 ] , [ 2 , 1 ]]. to_tensor . as_type ( Float32 ) a , b , c = t . svd puts a puts b puts c # [[-0.203749, 0.841716 , -0.330613, 0.375094 ], # [-0.464705, 0.184524 , -0.19985 , -0.842651], # [-0.464705, 0.184524 , 0.861075 , 0.092463 ], # [-0.725662, -0.472668, -0.330613, 0.375094 ]] # [3.02045 , 0.936426] # [[-0.788205, -0.615412], # [-0.615412, 0.788205 ]] View source #swap_axes ( a : Int , b : Int ) : Tensor ( T , S ) # Permutes two axes of a Tensor . This will always create a view of the permuted Tensor Arguments # a : Int - First axis of permutation b : Int - Second axis of permutation Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . swap_axes ( 2 , 0 ) # [[[ 0, 6, 12, 18] # [ 2, 8, 14, 20] # [ 4, 10, 16, 22]] # # [[ 1, 7, 13, 19] # [ 3, 9, 15, 21] # [ 5, 11, 17, 23]]] View source #tan # Calculates tangent, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . tan View source #tanh # Calculates hyperbolic tangent, elementwise Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a . tanh View source #tile ( n : Int ) : Tensor ( T , S ) # Tile elements of a Tensor Arguments # n : Int - Number of times to tile Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts a . tile ( 2 ) # [[1, 2, 3, 1, 2, 3], # [4, 5, 6, 4, 5, 6]] View source #tile ( n : Array ( Int )) : Tensor ( T , S ) # Tile elements of a Tensor Arguments # n : Int - Number of times to tile Examples # a = [[ 1 , 2 , 3 ] , [ 4 , 5 , 6 ]] puts Num . tile ( a , 2 ) # [[1, 2, 3, 1, 2, 3], # [4, 5, 6, 4, 5, 6]] View source #to_a : Array ( T ) # Converts a Tensor to an Array. To avoid return type ambiguity this will always return a 1D Array Arguments # Examples # a = Tensor . from_array [[ 1 , 2 ] , [ 3 , 4 ]] a . to_a # => [1, 2, 3, 4] View source #to_npy ( path : String ) # Export a Tensor to the Numpy format Arguments # path : String - filename of output .npy file. Only integer, unsigned integer and float ndarrays are supported at the moment. View source #transpose ( axes : Array ( Int ) = [] of Int32 ) : Tensor ( T , S ) # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # axes : Array(Int) - New ordering of axes for the permuted Tensor . If empty, a full transpose will occur Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #transpose ( * axes : Int ) : Tensor ( T , S ) # Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor . Arguments # axes : Array(Int) - New ordering of axes for the permuted Tensor . If empty, a full transpose will occur Examples # a = Tensor . new ( [ 4 , 3 , 2 ] ) { | i | i } a . transpose ( [ 2 , 0 , 1 ] ) # [[[ 0, 2, 4], # [ 6, 8, 10], # [12, 14, 16], # [18, 20, 22]], # # [[ 1, 3, 5], # [ 7, 9, 11], # [13, 15, 17], # [19, 21, 23]]] View source #tril ( k : Int = 0 ) # Computes the lower triangle of a Tensor . Zeros out values above the k th diagonal Arguments # k : Int - Diagonal Examples # a = Tensor ( Int32 ) . ones ( [ 3 , 3 ] ) a . tril! a # [[1, 0, 0], # [1, 1, 0], # [1, 1, 1]] View source #tril! ( k : Int = 0 ) # Computes the lower triangle of a Tensor . Zeros out values above the k th diagonal Arguments # k : Int - Diagonal Examples # a = Tensor ( Int32 ) . ones ( [ 3 , 3 ] ) a . tril! a # [[1, 0, 0], # [1, 1, 0], # [1, 1, 1]] View source #triu ( k : Int = 0 ) # Computes the upper triangle of a Tensor . Zeros out values below the k th diagonal Arguments # k : Int - Diagonal Examples # a = Tensor ( Int32 ) . ones ( [ 3 , 3 ] ) a . triu! a # [[1, 1, 1], # [0, 1, 1], # [0, 0, 1]] View source #triu! ( k : Int = 0 ) # Computes the upper triangle of a Tensor . Zeros out values below the k th diagonal Arguments # k : Int - Diagonal Examples # a = Tensor ( Int32 ) . ones ( [ 3 , 3 ] ) a . triu! a # [[1, 1, 1], # [0, 1, 1], # [0, 0, 1]] View source #view ( u : U . class ) forall U # Return a shallow copy of a Tensor with a new dtype. The underlying data buffer is shared, but the Tensor owns its other attributes. The size of the new dtype must be a multiple of the current dtype Arguments # u : U.class - The data type used to reintepret the underlying data buffer of a Tensor Examples # a = Tensor . new ( [ 3 ] ) { | i | i } a . view ( Int8 ) # => [0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0] View source #view : Tensor ( T , S ) # Return a shallow copy of a Tensor . The underlying data buffer is shared, but the Tensor owns its other attributes. Changes to a view of a Tensor will be reflected in the original Tensor Examples # a = Tensor ( Int32 , CPU ( Int32 )) . new ( [ 3 , 3 ] ) b = a . view b [...] = 99 a # [[99, 99, 99], # [99, 99, 99], # [99, 99, 99]] View source #with_broadcast ( n : Int ) : Tensor ( T , S ) # Expands a Tensor s dimensions n times by broadcasting the shape and strides. No data is copied, and the result is a read-only view of the original Tensor Arguments # n : Int - Number of dimensions to broadcast Examples # a = [ 1 , 2 , 3 ]. to_tensor a . with_broadcast ( 2 ) # [[[1]], # # [[2]], # # [[3]]] View source #yield_along_axis # Similar to each_axis , but instead of yielding slices of an axis, it yields slices along an axis, useful for methods that require an entire view of an axis slice for a reduction operation, such as std , rather than being able to incrementally reduce. Arguments # axis : Int - Axis of reduction Examples # a = Tensor . new ( [ 3 , 3 , 3 ] ) { | i | i } a . yield_along_axis ( 0 ) do | ax | puts ax end # [ 0, 9, 18] # [ 1, 10, 19] # [ 2, 11, 20] # [ 3, 12, 21] # [ 4, 13, 22] # [ 5, 14, 23] # [ 6, 15, 24] # [ 7, 16, 25] # [ 8, 17, 26] View source #zip ( b : Tensor ( U , CPU ( U )), & block : T , U -> _ ) forall U , V # Yields the elements of two Tensor s, always in RowMajor order, as if the Tensor s were flat. Arguments # b : Tensor - The other Tensor with which to iterate Examples # a = Tensor . new ( 2 , 2 ) { | i | i } b = Tensor . new ( 2 , 2 ) { | i | i + 2 } a . zip ( b ) do | el | puts el end # { 0, 2} # { 1, 3} # { 2, 4} # { 3, 5} View source #| ( other ) # Compute the bit-wise OR of two Tensor s element-wise. Arguments # other : Tensor | Number - RHS argument Examples # a = Tensor . from_array [ 1 , 2 , 3 ] a | a View source","title":"Tensor"},{"location":"Tensor/#Tensor","text":"","title":"Tensor"},{"location":"Tensor/#Tensor-constructors","text":"","title":"Constructors"},{"location":"Tensor/#Tensor.new(data,shape,strides,offset,flags,dtype)","text":"Initialize a Tensor from a storage instance, a shape, strides, an offset, flags, and a data type. This should primarily be used by internal methods, since it assumes the passed shape and strides correspond to the storage provided. The dtype is required to infer T without having it explicitly provided View source","title":".new"},{"location":"Tensor/#Tensor-class-methods","text":"","title":"Class methods"},{"location":"Tensor/#Tensor.beta(shape,a,b)","text":"Generates a Tensor containing a beta-distribution collection of values","title":".beta"},{"location":"Tensor/#Tensor.binomial(shape,n,prob)","text":"Draw samples from a binomial distribution. Samples are drawn from a binomial distribution with specified parameters, n trials and prob probability of success where n an integer >= 0 and p is in the interval [0,1].","title":".binomial"},{"location":"Tensor/#Tensor.chisq(shape,df)","text":"Generates a Tensor containing chi-square distributed values","title":".chisq"},{"location":"Tensor/#Tensor.exp(shape,scale)","text":"Generates a Tensor containing expontentially distributed values","title":".exp"},{"location":"Tensor/#Tensor.eye(m,n,offset)","text":"Return a two-dimensional Tensor with ones along the diagonal, and zeros elsewhere","title":".eye"},{"location":"Tensor/#Tensor.from_array(a,device)","text":"Creates a Tensor from a standard library array onto a specified device. The type of Tensor is inferred from the innermost element type, and the Array's shape must be uniform along all subarrays.","title":".from_array"},{"location":"Tensor/#Tensor.from_npy(path)","text":"Reads a .npy file and returns a Tensor of the specified type. If the ndarray is stored in a different type inside the file, it will be converted.","title":".from_npy"},{"location":"Tensor/#Tensor.fsned(shape,df1,df2)","text":"Generates a Tensor containing f-snedecor distributed values","title":".fsned"},{"location":"Tensor/#Tensor.full(shape,value)","text":"Creates a Tensor of a provided shape, filled with a value. The generic type is inferred from the value","title":".full"},{"location":"Tensor/#Tensor.full_like(t,value)","text":"Creates a Tensor filled with a value, sharing the shape of another provided Tensor","title":".full_like"},{"location":"Tensor/#Tensor.gamma(t_shape,shape,scale)","text":"Generate a gamma-distributed, pseudo-random Tensor","title":".gamma"},{"location":"Tensor/#Tensor.geometric_space(start,stop,num,endpoint,device)","text":"Return numbers spaced evenly on a log scale (a geometric progression). This is similar to logspace , but with endpoints specified directly. Each output sample is a constant multiple of the previous.","title":".geometric_space"},{"location":"Tensor/#Tensor.identity(n)","text":"Returns an identity Tensor with ones along the diagonal, and zeros elsewhere","title":".identity"},{"location":"Tensor/#Tensor.laplace(shape,loc,scale)","text":"Generate a laplace-distributed, pseudo-random Tensor","title":".laplace"},{"location":"Tensor/#Tensor.linear_space(start,stop,num,endpoint,device)","text":"Return evenly spaced numbers over a specified interval. Returns num evenly spaced samples, calculated over the interval [ start , stop ]. The endpoint of the interval can optionally be excluded.","title":".linear_space"},{"location":"Tensor/#Tensor.logarithmic_space(start,stop,num,endpoint,base,device)","text":"Return numbers spaced evenly on a log scale. In linear space, the sequence starts at base ** start ( base to the power of start ) and ends with base ** stop (see endpoint below).","title":".logarithmic_space"},{"location":"Tensor/#Tensor.lognormal(shape,loc,sigma)","text":"Generate a log-normal-distributed, pseudo-random Tensor","title":".lognormal"},{"location":"Tensor/#Tensor.normal(shape,loc,sigma)","text":"Generate a Tensor containing a normal-distribution collection of values","title":".normal"},{"location":"Tensor/#Tensor.ones(shape)","text":"Creates a Tensor of a provided shape, filled with 1. The generic type must be specified.","title":".ones"},{"location":"Tensor/#Tensor.ones_like(t)","text":"Creates a Tensor filled with 1, sharing the shape of another provided Tensor","title":".ones_like"},{"location":"Tensor/#Tensor.poisson(shape,lam)","text":"Generate a poisson-distributed, pseudo-random Tensor(Int64)","title":".poisson"},{"location":"Tensor/#Tensor.rand(shape)","text":"Generate random floating point values between 0 and 1","title":".rand"},{"location":"Tensor/#Tensor.random(r,shape,device)","text":"Creates a Tensor sampled from a provided range, with a given shape. The generic types of the Tensor are inferred from the endpoints of the range","title":".random"},{"location":"Tensor/#Tensor.range(start,stop,step,device)","text":"Creates a flat Tensor containing a monotonically increasing or decreasing range. The generic type is inferred from the inputs to the method","title":".range"},{"location":"Tensor/#Tensor.t_student(shape,df)","text":"Generate a t-student-distributed, pseudo-random Tensor","title":".t_student"},{"location":"Tensor/#Tensor.zeros(shape)","text":"Creates a Tensor of a provided shape, filled with 0. The generic type must be specified.","title":".zeros"},{"location":"Tensor/#Tensor.zeros_like(t)","text":"Creates a Tensor filled with 0, sharing the shape of another provided Tensor","title":".zeros_like"},{"location":"Tensor/#Tensor-methods","text":"","title":"Methods"},{"location":"Tensor/#Tensor#!=(other)","text":"Implements the != operator for two Tensor s element-wise.","title":"#!="},{"location":"Tensor/#Tensor#%(other)","text":"Return element-wise remainder of division for two Tensor s elementwise","title":"#%"},{"location":"Tensor/#Tensor#&(other)","text":"Compute the bit-wise AND of two Tensor s element-wise.","title":"#&amp;"},{"location":"Tensor/#Tensor#*(other)","text":"Multiplies two Tensor s elementwise","title":"#*"},{"location":"Tensor/#Tensor#**(other)","text":"Exponentiates two Tensor s elementwise","title":"#**"},{"location":"Tensor/#Tensor#+(other)","text":"Adds two Tensor s elementwise","title":"#+"},{"location":"Tensor/#Tensor#-(other)","text":"Subtracts two Tensor s elementwise","title":"#-"},{"location":"Tensor/#Tensor#/(other)","text":"Divides two Tensor s elementwise","title":"#/"},{"location":"Tensor/#Tensor#//(other)","text":"Floor divides two Tensor s elementwise","title":"#//"},{"location":"Tensor/#Tensor#<(other)","text":"Implements the < operator for two Tensor s element-wise.","title":"#&lt;"},{"location":"Tensor/#Tensor#<<(other)","text":"Shift the bits of an integer to the left. Bits are shifted to the left by appending x2 0s at the right of x1. Since the internal representation of numbers is in binary format, this operation is equivalent to multiplying x1 by 2**x2.","title":"#&lt;&lt;"},{"location":"Tensor/#Tensor#<=(other)","text":"Implements the <= operator for two Tensor s element-wise.","title":"#&lt;="},{"location":"Tensor/#Tensor#==(other)","text":"Implements the == operator for two Tensor s element-wise.","title":"#=="},{"location":"Tensor/#Tensor#>(other)","text":"Implements the > operator for two Tensor s element-wise.","title":"#&gt;"},{"location":"Tensor/#Tensor#>=(other)","text":"Implements the >= operator for two Tensor s element-wise.","title":"#&gt;="},{"location":"Tensor/#Tensor#>>(other)","text":"Shift the bits of an integer to the right. Bits are shifted to the right x2. Because the internal representation of numbers is in binary format, this operation is equivalent to dividing x1 by 2**x2.","title":"#&gt;&gt;"},{"location":"Tensor/#Tensor#[](args)","text":"Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source","title":"#[]"},{"location":"Tensor/#Tensor#[]=(args,value)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#[]="},{"location":"Tensor/#Tensor#^(other)","text":"Compute the bit-wise XOR of two Tensor s element-wise.","title":"#^"},{"location":"Tensor/#Tensor#accumulate(&)","text":"Returns a Tensor containing the successive values of applying a binary operation, specified by the given block , to this Tensor's elements. For each element in the Tensor the block is passed an accumulator value and the element. The result becomes the new value for the accumulator and is also appended to the returned Tensor. The initial value for the accumulator is the first element in the Tensor.","title":"#accumulate"},{"location":"Tensor/#Tensor#acos","text":"Trigonometric inverse cosine, element-wise.","title":"#acos"},{"location":"Tensor/#Tensor#acosh","text":"Inverse hyperbolic cosine, element-wise.","title":"#acosh"},{"location":"Tensor/#Tensor#add(other)","text":"Adds two Tensor s elementwise","title":"#add"},{"location":"Tensor/#Tensor#all(axis,dims)","text":"Reduces a Tensor along an axis, asserting the truthiness of all values in each view into the Tensor","title":"#all"},{"location":"Tensor/#Tensor#all_close(other,epsilon)","text":"Asserts that two Tensor s are equal, allowing for small margins of errors with floating point values using an EPSILON value.","title":"#all_close"},{"location":"Tensor/#Tensor#any(axis,dims)","text":"Reduces a Tensor along an axis, asserting the truthiness of any values in each view into the Tensor","title":"#any"},{"location":"Tensor/#Tensor#argmax(axis,dims)","text":"Find the maximum index value of a Tensor along an axis","title":"#argmax"},{"location":"Tensor/#Tensor#argmin(axis,dims)","text":"Find the minimum index value of a Tensor along an axis","title":"#argmin"},{"location":"Tensor/#Tensor#as_strided(shape,strides)","text":"as_strided creates a view into the Tensor given the exact strides and shape. This means it manipulates the internal data structure of a Tensor and, if done incorrectly, the array elements can point to invalid memory and can corrupt results or crash your program. It is advisable to always use the original strides when calculating new strides to avoid reliance on a contiguous memory layout. Furthermore, Tensor s created with this function often contain self overlapping memory, so that two elements are identical. Vectorized write operations on such Tensor s will typically be unpredictable. They may even give different results for small, large, or transposed Tensor s.","title":"#as_strided"},{"location":"Tensor/#Tensor#as_type(dtype)","text":"Converts a Tensor to a given dtype. No rounding is done on floating point values.","title":"#as_type"},{"location":"Tensor/#Tensor#asin","text":"Inverse sine, element-wise.","title":"#asin"},{"location":"Tensor/#Tensor#asinh","text":"Inverse hyperbolic sine, element-wise.","title":"#asinh"},{"location":"Tensor/#Tensor#atan","text":"Inverse tangent, element-wise.","title":"#atan"},{"location":"Tensor/#Tensor#atanh","text":"Inverse hyperbolic tangent, element-wise.","title":"#atanh"},{"location":"Tensor/#Tensor#besselj0","text":"Calculates besselj0, elementwise","title":"#besselj0"},{"location":"Tensor/#Tensor#besselj1","text":"Calculates besselj1, elementwise","title":"#besselj1"},{"location":"Tensor/#Tensor#bessely0","text":"Calculates bessely0, elementwise","title":"#bessely0"},{"location":"Tensor/#Tensor#bessely1","text":"Calculates bessely1, elementwise","title":"#bessely1"},{"location":"Tensor/#Tensor#bitwise_and(other)","text":"Compute the bit-wise AND of two Tensor s element-wise.","title":"#bitwise_and"},{"location":"Tensor/#Tensor#bitwise_or(other)","text":"Compute the bit-wise OR of two Tensor s element-wise.","title":"#bitwise_or"},{"location":"Tensor/#Tensor#bitwise_xor(other)","text":"Compute the bit-wise XOR of two Tensor s element-wise.","title":"#bitwise_xor"},{"location":"Tensor/#Tensor#broadcast(other1,other2)","text":"Broadcasts three Tensor 's' to a new shape. This allows for elementwise operations between the three Tensors with the new shape. Broadcasting rules apply, and imcompatible shapes will raise an error.","title":"#broadcast"},{"location":"Tensor/#Tensor#broadcast_to(shape)","text":"Broadcasts a Tensor to a new shape. Returns a read-only view of the original Tensor . Many elements in the Tensor will refer to the same memory location, and the result is rarely contiguous. Shapes must be broadcastable, and an error will be raised if broadcasting fails.","title":"#broadcast_to"},{"location":"Tensor/#Tensor#cbrt","text":"Calculates cube root, elementwise","title":"#cbrt"},{"location":"Tensor/#Tensor#cholesky!(*,lower)","text":"Cholesky decomposition. Return the Cholesky decomposition, L * L.H, of the square matrix a, where L is lower-triangular and .H is the conjugate transpose operator (which is the ordinary transpose if a is real-valued). a must be Hermitian (symmetric if real-valued) and positive-definite. Only L is actually returned.","title":"#cholesky!"},{"location":"Tensor/#Tensor#cos","text":"Calculates cosine, elementwise","title":"#cos"},{"location":"Tensor/#Tensor#cosh","text":"Calculates hyperbolic cosine, elementwise","title":"#cosh"},{"location":"Tensor/#Tensor#cpu","text":"Places a Tensor onto a CPU backend. No copy is done if the Tensor is already on a CPU","title":"#cpu"},{"location":"Tensor/#Tensor#data","text":"View source","title":"#data"},{"location":"Tensor/#Tensor#det","text":"Compute the determinant of an array.","title":"#det"},{"location":"Tensor/#Tensor#diagonal","text":"Returns a view of the diagonal of a Tensor . This method only works for two-dimensional arrays. Todo Implement views for offset diagonals","title":"#diagonal"},{"location":"Tensor/#Tensor#divide(other)","text":"Divides two Tensor s elementwise","title":"#divide"},{"location":"Tensor/#Tensor#dot(u)","text":"DOT forms the dot product of two vectors. Uses unrolled loops for increments equal to one.","title":"#dot"},{"location":"Tensor/#Tensor#dup(order)","text":"Deep-copies a Tensor . If an order is provided, the returned Tensor 's memory layout will respect that order.","title":"#dup"},{"location":"Tensor/#Tensor#each(&)","text":"Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat.","title":"#each"},{"location":"Tensor/#Tensor#each_axis(axis,dims,&)","text":"Yields a view of each lane of an axis . Changes made in the passed block will be reflected in the original Tensor","title":"#each_axis"},{"location":"Tensor/#Tensor#each_pointer(&)","text":"Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor","title":"#each_pointer"},{"location":"Tensor/#Tensor#each_pointer_with_index(&)","text":"Yields the memory locations of each element of a Tensor , always in RowMajor oder, as if the Tensor was flat. Also yields the flat index of a Tensor This should primarily be used by internal methods. Methods such as map! provided more convenient access to editing the values of a Tensor","title":"#each_pointer_with_index"},{"location":"Tensor/#Tensor#each_with_index(&)","text":"Yields the elements of a Tensor , always in RowMajor order, as if the Tensor was flat. Also yields the flat index of each element.","title":"#each_with_index"},{"location":"Tensor/#Tensor#eig","text":"Compute the eigenvalues and right eigenvectors of a square array.","title":"#eig"},{"location":"Tensor/#Tensor#eigh","text":"Compute the eigenvalues and right eigenvectors of a square Tensor .","title":"#eigh"},{"location":"Tensor/#Tensor#eigvals","text":"Compute the eigenvalues of a general matrix. Main difference between eigvals and eig: the eigenvectors aren\u2019t returned.","title":"#eigvals"},{"location":"Tensor/#Tensor#eigvalsh","text":"Compute the eigenvalues of a symmetric matrix. Main difference between eigvals and eig: the eigenvectors aren\u2019t returned.","title":"#eigvalsh"},{"location":"Tensor/#Tensor#equal(other)","text":"Implements the == operator for two Tensor s element-wise.","title":"#equal"},{"location":"Tensor/#Tensor#erf","text":"Calculates erf, elementwise","title":"#erf"},{"location":"Tensor/#Tensor#erfc","text":"Calculates erfc, elementwise","title":"#erfc"},{"location":"Tensor/#Tensor#exp","text":"Calculates exp, elementwise","title":"#exp"},{"location":"Tensor/#Tensor#exp2","text":"Calculates exp2, elementwise","title":"#exp2"},{"location":"Tensor/#Tensor#expand_dims(axis)","text":"Expands the dimensions of a Tensor , along a single axis","title":"#expand_dims"},{"location":"Tensor/#Tensor#expm1","text":"Calculates expm1, elementwise","title":"#expm1"},{"location":"Tensor/#Tensor#flags","text":"Returns the flags of a Tensor, describing its memory and read status","title":"#flags"},{"location":"Tensor/#Tensor#flat","text":"Flattens a Tensor to a single dimension. If a view can be created, the reshape operation will not copy data.","title":"#flat"},{"location":"Tensor/#Tensor#flip(axis)","text":"Flips a Tensor along an axis, returning a view","title":"#flip"},{"location":"Tensor/#Tensor#floordiv(other)","text":"Floor divides two Tensor s elementwise","title":"#floordiv"},{"location":"Tensor/#Tensor#gamma","text":"Calculates gamma function, elementwise","title":"#gamma"},{"location":"Tensor/#Tensor#greater(other)","text":"Implements the > operator for two Tensor s element-wise.","title":"#greater"},{"location":"Tensor/#Tensor#greater_equal(other)","text":"Implements the >= operator for two Tensor s element-wise.","title":"#greater_equal"},{"location":"Tensor/#Tensor#hessenberg","text":"Compute Hessenberg form of a matrix. The Hessenberg decomposition is: A = Q H Q ^ H where Q is unitary/orthogonal and H has only zero elements below the first sub-diagonal.","title":"#hessenberg"},{"location":"Tensor/#Tensor#ilogb","text":"Calculates ilogb, elementwise","title":"#ilogb"},{"location":"Tensor/#Tensor#inv","text":"Compute the (multiplicative) inverse of a matrix. Given a square matrix a, return the matrix ainv satisfying dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])","title":"#inv"},{"location":"Tensor/#Tensor#left_shift(other)","text":"Shift the bits of an integer to the left. Bits are shifted to the left by appending x2 0s at the right of x1. Since the internal representation of numbers is in binary format, this operation is equivalent to multiplying x1 by 2**x2.","title":"#left_shift"},{"location":"Tensor/#Tensor#less(other)","text":"Implements the < operator for two Tensor s element-wise.","title":"#less"},{"location":"Tensor/#Tensor#less_equal(other)","text":"Implements the <= operator for two Tensor s element-wise.","title":"#less_equal"},{"location":"Tensor/#Tensor#lgamma","text":"Calculates logarithmic gamma, elementwise","title":"#lgamma"},{"location":"Tensor/#Tensor#log","text":"Calculates log, elementwise","title":"#log"},{"location":"Tensor/#Tensor#log10","text":"Calculates log10, elementwise","title":"#log10"},{"location":"Tensor/#Tensor#log1p","text":"Calculates log1p, elementwise","title":"#log1p"},{"location":"Tensor/#Tensor#log2","text":"Calculates log2, elementwise","title":"#log2"},{"location":"Tensor/#Tensor#logb","text":"Calculates logb, elementwise","title":"#logb"},{"location":"Tensor/#Tensor#map(d1,d2,&)","text":"Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The generic type of the returned Tensor is inferred from a block","title":"#map"},{"location":"Tensor/#Tensor#map!(d1,d2,&)","text":"Maps a block across three Tensors . This is more efficient than zipping iterators since it iterates all Tensor 's in a single call, avoiding overhead from tracking multiple iterators. The result of the block is stored in self . Broadcasting rules still apply, but since this is an in place operation, the other Tensor 's must broadcast to the shape of self","title":"#map!"},{"location":"Tensor/#Tensor#matmul(other,output)","text":"Computes a matrix multiplication between two Tensors . The Tensor s must be two dimensional with compatible shapes. Currently only Float and Complex Tensor s are supported, as BLAS is used for this operation","title":"#matmul"},{"location":"Tensor/#Tensor#max(axis,dims)","text":"Reduces a Tensor along an axis, finding the max of each view into the Tensor","title":"#max"},{"location":"Tensor/#Tensor#mean(axis,dims)","text":"Reduces a Tensor along an axis, finding the average of each view into the Tensor","title":"#mean"},{"location":"Tensor/#Tensor#min(axis,dims)","text":"Reduces a Tensor along an axis, finding the min of each view into the Tensor","title":"#min"},{"location":"Tensor/#Tensor#modulo(other)","text":"Return element-wise remainder of division for two Tensor s elementwise","title":"#modulo"},{"location":"Tensor/#Tensor#move_axis(source,destination)","text":"Move axes of a Tensor to new positions, other axes remain in their original order","title":"#move_axis"},{"location":"Tensor/#Tensor#move_axis(source,destination)","text":"Move axes of a Tensor to new positions, other axes remain in their original order","title":"#move_axis"},{"location":"Tensor/#Tensor#multiply(other)","text":"Multiplies two Tensor s elementwise","title":"#multiply"},{"location":"Tensor/#Tensor#norm(order)","text":"Matrix norm This function is able to return one of eight different matrix norms","title":"#norm"},{"location":"Tensor/#Tensor#not_equal(other)","text":"Implements the != operator for two Tensor s element-wise.","title":"#not_equal"},{"location":"Tensor/#Tensor#offset","text":"Returns the offset of a Tensor's data","title":"#offset"},{"location":"Tensor/#Tensor#opencl","text":"Places a Tensor onto an OpenCL backend. No copy is done if the Tensor is already on a CPU","title":"#opencl"},{"location":"Tensor/#Tensor#power(other)","text":"Exponentiates two Tensor s elementwise","title":"#power"},{"location":"Tensor/#Tensor#prod(axis,dims)","text":"Reduces a Tensor along an axis, multiplying each view into the Tensor","title":"#prod"},{"location":"Tensor/#Tensor#ptp(axis,dims)","text":"Finds the difference between the maximum and minimum elements of a Tensor along an axis","title":"#ptp"},{"location":"Tensor/#Tensor#qr","text":"Compute the qr factorization of a matrix. Factor the matrix a as qr, where q is orthonormal and r is upper-triangular","title":"#qr"},{"location":"Tensor/#Tensor#rank","text":"Returns the number of dimensions in a Tensor","title":"#rank"},{"location":"Tensor/#Tensor#reduce(memo,&)","text":"Just like the other variant, but you can set the initial value of the accumulator.","title":"#reduce"},{"location":"Tensor/#Tensor#reduce_axis(axis,dims,&)","text":"Equivalent of calling reduce on each slice into an axis . Used primarily for reductions like Num.sum , Num.prod , in their axis versions.","title":"#reduce_axis"},{"location":"Tensor/#Tensor#repeat(n,axis)","text":"Repeat elements of a Tensor along an axis","title":"#repeat"},{"location":"Tensor/#Tensor#reshape(shape)","text":"Transform's a Tensor 's shape. If a view can be created, the reshape will not copy data. The number of elements in the Tensor must remain the same.","title":"#reshape"},{"location":"Tensor/#Tensor#right_shift(other)","text":"Shift the bits of an integer to the right. Bits are shifted to the right x2. Because the internal representation of numbers is in binary format, this operation is equivalent to dividing x1 by 2**x2.","title":"#right_shift"},{"location":"Tensor/#Tensor#set(*,value)","text":"The primary method of setting Tensor values. The slicing behavior for this method is identical to the [] method. If a Tensor is passed as the value to set, it will be broadcast to the shape of the slice if possible. If a scalar is passed, it will be tiled across the slice.","title":"#set"},{"location":"Tensor/#Tensor#shape","text":"Returns the size of a Tensor along each dimension","title":"#shape"},{"location":"Tensor/#Tensor#sin","text":"Calculates sine, elementwise","title":"#sin"},{"location":"Tensor/#Tensor#sinh","text":"Calculates hyperbolic sine, elementwise","title":"#sinh"},{"location":"Tensor/#Tensor#size","text":"Returns the size of a Tensor along each dimension a = Tensor ( Int8 , CPU ( Int8 )) . new ( [ 2 , 3 , 4 ] ) a . shape # => [2, 3, 4] View source","title":"#size"},{"location":"Tensor/#Tensor#slice(*)","text":"Returns a view of a Tensor from any valid indexers. This view must be able to be represented as valid strided/shaped view, slicing as a copy is not supported. When an Integer argument is passed, an axis will be removed from the Tensor , and a view at that index will be returned. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ 0 ] # => [0, 1] When a Range argument is passed, an axis will be sliced based on the endpoints of the range. a = Tensor . new ( [ 2 , 2 , 2 ] ) { | i | i } a [ 1 ...] # [[[4, 5], # [6, 7]]] When a Tuple containing a Range and an Integer step is passed, an axis is sliced based on the endpoints of the range, and the strides of the axis are updated to reflect the step. Negative steps will reflect the array along an axis. a = Tensor . new ( [ 2 , 2 ] ) { | i | i } a [ { ... , - 1 } ] # [[2, 3], # [0, 1]] View source","title":"#slice"},{"location":"Tensor/#Tensor#solve(x)","text":"Solve a linear matrix equation, or system of linear scalar equations. Computes the \u201cexact\u201d solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.","title":"#solve"},{"location":"Tensor/#Tensor#sort(axis)","text":"Sorts a Tensor along an axis.","title":"#sort"},{"location":"Tensor/#Tensor#sqrt","text":"Calculates square root, elementwise","title":"#sqrt"},{"location":"Tensor/#Tensor#std(axis,dims)","text":"Reduces a Tensor along an axis, finding the std of each view into the Tensor","title":"#std"},{"location":"Tensor/#Tensor#strides","text":"Returns the step of a Tensor along each dimension","title":"#strides"},{"location":"Tensor/#Tensor#subtract(other)","text":"Subtracts two Tensor s elementwise","title":"#subtract"},{"location":"Tensor/#Tensor#sum(axis,dims)","text":"Reduces a Tensor along an axis, summing each view into the Tensor","title":"#sum"},{"location":"Tensor/#Tensor#svd","text":"Singular Value Decomposition. When a is a 2D array, it is factorized as u @ np.diag(s) @ vh = (u * s) @ vh, where u and vh are 2D unitary arrays and s is a 1D array of a\u2019s singular values.","title":"#svd"},{"location":"Tensor/#Tensor#swap_axes(a,b)","text":"Permutes two axes of a Tensor . This will always create a view of the permuted Tensor","title":"#swap_axes"},{"location":"Tensor/#Tensor#tan","text":"Calculates tangent, elementwise","title":"#tan"},{"location":"Tensor/#Tensor#tanh","text":"Calculates hyperbolic tangent, elementwise","title":"#tanh"},{"location":"Tensor/#Tensor#tile(n)","text":"Tile elements of a Tensor","title":"#tile"},{"location":"Tensor/#Tensor#tile(n)","text":"Tile elements of a Tensor","title":"#tile"},{"location":"Tensor/#Tensor#to_a","text":"Converts a Tensor to an Array. To avoid return type ambiguity this will always return a 1D Array","title":"#to_a"},{"location":"Tensor/#Tensor#to_npy(path)","text":"Export a Tensor to the Numpy format","title":"#to_npy"},{"location":"Tensor/#Tensor#transpose(axes)","text":"Permutes a Tensor 's axes to a different order. This will always create a view of the permuted Tensor .","title":"#transpose"},{"location":"Tensor/#Tensor#tril(k)","text":"Computes the lower triangle of a Tensor . Zeros out values above the k th diagonal","title":"#tril"},{"location":"Tensor/#Tensor#tril!(k)","text":"Computes the lower triangle of a Tensor . Zeros out values above the k th diagonal","title":"#tril!"},{"location":"Tensor/#Tensor#triu(k)","text":"Computes the upper triangle of a Tensor . Zeros out values below the k th diagonal","title":"#triu"},{"location":"Tensor/#Tensor#triu!(k)","text":"Computes the upper triangle of a Tensor . Zeros out values below the k th diagonal","title":"#triu!"},{"location":"Tensor/#Tensor#view(u)","text":"Return a shallow copy of a Tensor with a new dtype. The underlying data buffer is shared, but the Tensor owns its other attributes. The size of the new dtype must be a multiple of the current dtype","title":"#view"},{"location":"Tensor/#Tensor#with_broadcast(n)","text":"Expands a Tensor s dimensions n times by broadcasting the shape and strides. No data is copied, and the result is a read-only view of the original Tensor","title":"#with_broadcast"},{"location":"Tensor/#Tensor#yield_along_axis(axis,&)","text":"Similar to each_axis , but instead of yielding slices of an axis, it yields slices along an axis, useful for methods that require an entire view of an axis slice for a reduction operation, such as std , rather than being able to incrementally reduce.","title":"#yield_along_axis"},{"location":"Tensor/#Tensor#zip(b,&)","text":"Yields the elements of two Tensor s, always in RowMajor order, as if the Tensor s were flat.","title":"#zip"},{"location":"Tensor/#Tensor#|(other)","text":"Compute the bit-wise OR of two Tensor s element-wise.","title":"#|"}]}